{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPoleDQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Megacity1/CartpoleDQN/blob/main/Week%203%20Deep%20RL%202/CartPoleDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Deep Q Network (DQN) for CartPole Using Boltzmann Q Policy\n",
        "This exercise implements a DQN for CartPole using a Boltzmann Q policy for selecting the actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGsC7cJ5jNcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f50ec379-f434-46e9-c2d3-fa59af2eabce"
      },
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-rl2\n",
            "  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████▎                         | 10 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 20 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 30 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 40 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 51 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 52 kB 631 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (13.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.10.0.2)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 8.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.24.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.44.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.5)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n",
            "Installing collected packages: tf-estimator-nightly, keras-rl2\n",
            "Successfully installed keras-rl2-1.0.5 tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMIHLgQ3Z-lF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb7d19ac-1b3d-4778-8f13-0bbbaaf16141"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0AMLzq08ap0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42c18a54-fc1c-4406-d70e-382fb29fe9ac"
      },
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "!pip install Adam\n",
        "#from keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Adam\n",
            "  Downloading adam-0.0.0.dev0-py2.py3-none-any.whl (2.6 kB)\n",
            "Installing collected packages: Adam\n",
            "Successfully installed Adam-0.0.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll6bNdUm54WS"
      },
      "source": [
        "Implementation of DQN for CartPole, applying policy BoltzmannQPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCrPKNy40PC"
      },
      "source": [
        "##Implement DQN with BoltzmannGumbelQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efM9jkXr5A3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2d405668-e346-47bb-ee40-c4fbc5816bad"
      },
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import BoltzmannGumbelQPolicy\n",
        "from rl.policy import LinearAnnealedPolicy\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n",
        "# setup experience replay buffer\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
        "                               attr='eps',            \n",
        "                               value_max=1.,\n",
        "                               value_min=.1, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=200)\n",
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "# add extra layers here\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=30,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy) \n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=8000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 4)                 0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                80        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 8000 steps ...\n",
            "   18/8000: episode: 1, duration: 0.097s, episode steps:  18, steps per second: 186, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   68/8000: episode: 2, duration: 0.901s, episode steps:  50, steps per second:  56, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 0.442023, mae: 0.503041, mean_q: 0.049033, mean_eps: 0.779500\n",
            "  150/8000: episode: 3, duration: 0.793s, episode steps:  82, steps per second: 103, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 0.311973, mae: 0.524846, mean_q: 0.423807, mean_eps: 0.511750\n",
            "  178/8000: episode: 4, duration: 0.249s, episode steps:  28, steps per second: 112, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 0.227188, mae: 0.589285, mean_q: 0.709056, mean_eps: 0.264250\n",
            "  229/8000: episode: 5, duration: 0.395s, episode steps:  51, steps per second: 129, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 0.179869, mae: 0.675129, mean_q: 0.962456, mean_eps: 0.122324\n",
            "  274/8000: episode: 6, duration: 0.364s, episode steps:  45, steps per second: 123, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 0.118089, mae: 0.779261, mean_q: 1.279103, mean_eps: 0.100000\n",
            "  302/8000: episode: 7, duration: 0.245s, episode steps:  28, steps per second: 114, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.103771, mae: 0.898862, mean_q: 1.550620, mean_eps: 0.100000\n",
            "  330/8000: episode: 8, duration: 0.227s, episode steps:  28, steps per second: 123, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.081224, mae: 0.972557, mean_q: 1.755690, mean_eps: 0.100000\n",
            "  347/8000: episode: 9, duration: 0.258s, episode steps:  17, steps per second:  66, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.089202, mae: 1.063977, mean_q: 1.924057, mean_eps: 0.100000\n",
            "  371/8000: episode: 10, duration: 0.362s, episode steps:  24, steps per second:  66, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.086823, mae: 1.136413, mean_q: 2.094234, mean_eps: 0.100000\n",
            "  390/8000: episode: 11, duration: 0.317s, episode steps:  19, steps per second:  60, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 0.100794, mae: 1.227625, mean_q: 2.272595, mean_eps: 0.100000\n",
            "  403/8000: episode: 12, duration: 0.231s, episode steps:  13, steps per second:  56, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.097874, mae: 1.288142, mean_q: 2.406970, mean_eps: 0.100000\n",
            "  419/8000: episode: 13, duration: 0.308s, episode steps:  16, steps per second:  52, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.107270, mae: 1.336151, mean_q: 2.506146, mean_eps: 0.100000\n",
            "  434/8000: episode: 14, duration: 0.294s, episode steps:  15, steps per second:  51, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.117611, mae: 1.411189, mean_q: 2.624900, mean_eps: 0.100000\n",
            "  449/8000: episode: 15, duration: 0.318s, episode steps:  15, steps per second:  47, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.135803, mae: 1.468625, mean_q: 2.747563, mean_eps: 0.100000\n",
            "  461/8000: episode: 16, duration: 0.267s, episode steps:  12, steps per second:  45, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.112457, mae: 1.508682, mean_q: 2.868675, mean_eps: 0.100000\n",
            "  477/8000: episode: 17, duration: 0.237s, episode steps:  16, steps per second:  67, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.155123, mae: 1.588710, mean_q: 2.992544, mean_eps: 0.100000\n",
            "  490/8000: episode: 18, duration: 0.217s, episode steps:  13, steps per second:  60, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.088021, mae: 1.597846, mean_q: 3.100489, mean_eps: 0.100000\n",
            "  502/8000: episode: 19, duration: 0.173s, episode steps:  12, steps per second:  70, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.138852, mae: 1.688155, mean_q: 3.266939, mean_eps: 0.100000\n",
            "  514/8000: episode: 20, duration: 0.187s, episode steps:  12, steps per second:  64, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.169961, mae: 1.751087, mean_q: 3.398484, mean_eps: 0.100000\n",
            "  523/8000: episode: 21, duration: 0.138s, episode steps:   9, steps per second:  65, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.229132, mae: 1.829010, mean_q: 3.500742, mean_eps: 0.100000\n",
            "  535/8000: episode: 22, duration: 0.197s, episode steps:  12, steps per second:  61, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.206457, mae: 1.871693, mean_q: 3.534288, mean_eps: 0.100000\n",
            "  546/8000: episode: 23, duration: 0.195s, episode steps:  11, steps per second:  56, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.225501, mae: 1.893389, mean_q: 3.605616, mean_eps: 0.100000\n",
            "  564/8000: episode: 24, duration: 0.299s, episode steps:  18, steps per second:  60, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 0.290632, mae: 1.981464, mean_q: 3.734312, mean_eps: 0.100000\n",
            "  579/8000: episode: 25, duration: 0.270s, episode steps:  15, steps per second:  56, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.246159, mae: 2.023764, mean_q: 3.815873, mean_eps: 0.100000\n",
            "  590/8000: episode: 26, duration: 0.165s, episode steps:  11, steps per second:  67, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.280665, mae: 2.095684, mean_q: 3.936624, mean_eps: 0.100000\n",
            "  602/8000: episode: 27, duration: 0.170s, episode steps:  12, steps per second:  70, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.212834, mae: 2.124451, mean_q: 4.049161, mean_eps: 0.100000\n",
            "  614/8000: episode: 28, duration: 0.187s, episode steps:  12, steps per second:  64, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.324154, mae: 2.195825, mean_q: 4.164804, mean_eps: 0.100000\n",
            "  627/8000: episode: 29, duration: 0.204s, episode steps:  13, steps per second:  64, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.352053, mae: 2.254662, mean_q: 4.248611, mean_eps: 0.100000\n",
            "  644/8000: episode: 30, duration: 0.269s, episode steps:  17, steps per second:  63, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.394969, mae: 2.302716, mean_q: 4.347263, mean_eps: 0.100000\n",
            "  654/8000: episode: 31, duration: 0.221s, episode steps:  10, steps per second:  45, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.357647, mae: 2.364059, mean_q: 4.432296, mean_eps: 0.100000\n",
            "  664/8000: episode: 32, duration: 0.170s, episode steps:  10, steps per second:  59, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.454255, mae: 2.438457, mean_q: 4.558170, mean_eps: 0.100000\n",
            "  674/8000: episode: 33, duration: 0.214s, episode steps:  10, steps per second:  47, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.576800, mae: 2.493967, mean_q: 4.556759, mean_eps: 0.100000\n",
            "  689/8000: episode: 34, duration: 0.337s, episode steps:  15, steps per second:  45, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.361661, mae: 2.461998, mean_q: 4.632446, mean_eps: 0.100000\n",
            "  704/8000: episode: 35, duration: 0.445s, episode steps:  15, steps per second:  34, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.391654, mae: 2.540319, mean_q: 4.840071, mean_eps: 0.100000\n",
            "  717/8000: episode: 36, duration: 0.200s, episode steps:  13, steps per second:  65, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.579114, mae: 2.640165, mean_q: 4.931166, mean_eps: 0.100000\n",
            "  731/8000: episode: 37, duration: 0.254s, episode steps:  14, steps per second:  55, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.530720, mae: 2.651574, mean_q: 4.996925, mean_eps: 0.100000\n",
            "  742/8000: episode: 38, duration: 0.157s, episode steps:  11, steps per second:  70, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.574058, mae: 2.741526, mean_q: 5.114630, mean_eps: 0.100000\n",
            "  753/8000: episode: 39, duration: 0.156s, episode steps:  11, steps per second:  71, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.600758, mae: 2.759672, mean_q: 5.182036, mean_eps: 0.100000\n",
            "  764/8000: episode: 40, duration: 0.182s, episode steps:  11, steps per second:  60, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.686895, mae: 2.823726, mean_q: 5.222521, mean_eps: 0.100000\n",
            "  779/8000: episode: 41, duration: 0.374s, episode steps:  15, steps per second:  40, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.732884, mae: 2.859726, mean_q: 5.270335, mean_eps: 0.100000\n",
            "  789/8000: episode: 42, duration: 0.151s, episode steps:  10, steps per second:  66, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.676874, mae: 2.889801, mean_q: 5.317069, mean_eps: 0.100000\n",
            "  801/8000: episode: 43, duration: 0.209s, episode steps:  12, steps per second:  57, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.636012, mae: 2.926736, mean_q: 5.429195, mean_eps: 0.100000\n",
            "  814/8000: episode: 44, duration: 0.279s, episode steps:  13, steps per second:  47, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.539287, mae: 2.957662, mean_q: 5.566761, mean_eps: 0.100000\n",
            "  824/8000: episode: 45, duration: 0.158s, episode steps:  10, steps per second:  63, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.494118, mae: 2.985127, mean_q: 5.661868, mean_eps: 0.100000\n",
            "  833/8000: episode: 46, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.814544, mae: 3.072578, mean_q: 5.743278, mean_eps: 0.100000\n",
            "  842/8000: episode: 47, duration: 0.176s, episode steps:   9, steps per second:  51, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.463038, mae: 3.061087, mean_q: 5.798068, mean_eps: 0.100000\n",
            "  852/8000: episode: 48, duration: 0.204s, episode steps:  10, steps per second:  49, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.478720, mae: 3.070153, mean_q: 5.890232, mean_eps: 0.100000\n",
            "  862/8000: episode: 49, duration: 0.220s, episode steps:  10, steps per second:  45, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.884402, mae: 3.206170, mean_q: 5.967366, mean_eps: 0.100000\n",
            "  874/8000: episode: 50, duration: 0.191s, episode steps:  12, steps per second:  63, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.673919, mae: 3.180837, mean_q: 5.916563, mean_eps: 0.100000\n",
            "  886/8000: episode: 51, duration: 0.182s, episode steps:  12, steps per second:  66, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.832611, mae: 3.264103, mean_q: 6.004249, mean_eps: 0.100000\n",
            "  900/8000: episode: 52, duration: 0.279s, episode steps:  14, steps per second:  50, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.695851, mae: 3.275200, mean_q: 6.068785, mean_eps: 0.100000\n",
            "  911/8000: episode: 53, duration: 0.178s, episode steps:  11, steps per second:  62, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.742957, mae: 3.309656, mean_q: 6.163078, mean_eps: 0.100000\n",
            "  925/8000: episode: 54, duration: 0.225s, episode steps:  14, steps per second:  62, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 1.014120, mae: 3.399581, mean_q: 6.210587, mean_eps: 0.100000\n",
            "  935/8000: episode: 55, duration: 0.254s, episode steps:  10, steps per second:  39, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.978429, mae: 3.411633, mean_q: 6.248543, mean_eps: 0.100000\n",
            "  947/8000: episode: 56, duration: 0.277s, episode steps:  12, steps per second:  43, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.754923, mae: 3.394206, mean_q: 6.273976, mean_eps: 0.100000\n",
            "  958/8000: episode: 57, duration: 0.223s, episode steps:  11, steps per second:  49, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 1.024687, mae: 3.487168, mean_q: 6.421121, mean_eps: 0.100000\n",
            "  969/8000: episode: 58, duration: 0.185s, episode steps:  11, steps per second:  59, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.976625, mae: 3.492562, mean_q: 6.378428, mean_eps: 0.100000\n",
            "  981/8000: episode: 59, duration: 0.175s, episode steps:  12, steps per second:  69, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.162412, mae: 3.556928, mean_q: 6.473002, mean_eps: 0.100000\n",
            "  992/8000: episode: 60, duration: 0.192s, episode steps:  11, steps per second:  57, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.697693, mae: 3.500527, mean_q: 6.553281, mean_eps: 0.100000\n",
            " 1002/8000: episode: 61, duration: 0.213s, episode steps:  10, steps per second:  47, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 1.066212, mae: 3.607453, mean_q: 6.642582, mean_eps: 0.100000\n",
            " 1012/8000: episode: 62, duration: 0.148s, episode steps:  10, steps per second:  68, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 1.035335, mae: 3.609563, mean_q: 6.640002, mean_eps: 0.100000\n",
            " 1024/8000: episode: 63, duration: 0.198s, episode steps:  12, steps per second:  61, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.931275, mae: 3.645010, mean_q: 6.665415, mean_eps: 0.100000\n",
            " 1038/8000: episode: 64, duration: 0.241s, episode steps:  14, steps per second:  58, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.918501, mae: 3.661283, mean_q: 6.769800, mean_eps: 0.100000\n",
            " 1049/8000: episode: 65, duration: 0.227s, episode steps:  11, steps per second:  48, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.981111, mae: 3.709904, mean_q: 6.822343, mean_eps: 0.100000\n",
            " 1061/8000: episode: 66, duration: 0.239s, episode steps:  12, steps per second:  50, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 1.029954, mae: 3.727379, mean_q: 6.825546, mean_eps: 0.100000\n",
            " 1074/8000: episode: 67, duration: 0.231s, episode steps:  13, steps per second:  56, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 1.048152, mae: 3.759640, mean_q: 6.908014, mean_eps: 0.100000\n",
            " 1087/8000: episode: 68, duration: 0.238s, episode steps:  13, steps per second:  55, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 1.465218, mae: 3.816832, mean_q: 6.868912, mean_eps: 0.100000\n",
            " 1096/8000: episode: 69, duration: 0.218s, episode steps:   9, steps per second:  41, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.826472, mae: 3.765532, mean_q: 6.915096, mean_eps: 0.100000\n",
            " 1108/8000: episode: 70, duration: 0.225s, episode steps:  12, steps per second:  53, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.915529, mae: 3.812832, mean_q: 7.103259, mean_eps: 0.100000\n",
            " 1118/8000: episode: 71, duration: 0.158s, episode steps:  10, steps per second:  63, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.867167, mae: 3.824630, mean_q: 7.137365, mean_eps: 0.100000\n",
            " 1132/8000: episode: 72, duration: 0.257s, episode steps:  14, steps per second:  55, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 1.212387, mae: 3.936739, mean_q: 7.245254, mean_eps: 0.100000\n",
            " 1148/8000: episode: 73, duration: 0.260s, episode steps:  16, steps per second:  61, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.993490, mae: 3.907850, mean_q: 7.158764, mean_eps: 0.100000\n",
            " 1162/8000: episode: 74, duration: 0.116s, episode steps:  14, steps per second: 121, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.869134, mae: 3.915978, mean_q: 7.282868, mean_eps: 0.100000\n",
            " 1175/8000: episode: 75, duration: 0.112s, episode steps:  13, steps per second: 116, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.963503, mae: 3.960072, mean_q: 7.394245, mean_eps: 0.100000\n",
            " 1187/8000: episode: 76, duration: 0.100s, episode steps:  12, steps per second: 119, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 1.142427, mae: 4.004394, mean_q: 7.341909, mean_eps: 0.100000\n",
            " 1199/8000: episode: 77, duration: 0.110s, episode steps:  12, steps per second: 109, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.473998, mae: 4.095252, mean_q: 7.388175, mean_eps: 0.100000\n",
            " 1213/8000: episode: 78, duration: 0.114s, episode steps:  14, steps per second: 123, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.355841, mae: 4.088771, mean_q: 7.382575, mean_eps: 0.100000\n",
            " 1227/8000: episode: 79, duration: 0.119s, episode steps:  14, steps per second: 118, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.089969, mae: 4.070551, mean_q: 7.347595, mean_eps: 0.100000\n",
            " 1243/8000: episode: 80, duration: 0.133s, episode steps:  16, steps per second: 121, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 1.208151, mae: 4.086863, mean_q: 7.465925, mean_eps: 0.100000\n",
            " 1255/8000: episode: 81, duration: 0.109s, episode steps:  12, steps per second: 111, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.901325, mae: 4.124142, mean_q: 7.672477, mean_eps: 0.100000\n",
            " 1353/8000: episode: 82, duration: 0.758s, episode steps:  98, steps per second: 129, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 1.112646, mae: 4.253782, mean_q: 7.826368, mean_eps: 0.100000\n",
            " 1437/8000: episode: 83, duration: 0.651s, episode steps:  84, steps per second: 129, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.405 [0.000, 1.000],  loss: 1.137882, mae: 4.562564, mean_q: 8.508858, mean_eps: 0.100000\n",
            " 1486/8000: episode: 84, duration: 0.401s, episode steps:  49, steps per second: 122, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 1.143340, mae: 4.745647, mean_q: 8.890024, mean_eps: 0.100000\n",
            " 1528/8000: episode: 85, duration: 0.335s, episode steps:  42, steps per second: 125, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 1.261447, mae: 4.849349, mean_q: 9.077016, mean_eps: 0.100000\n",
            " 1549/8000: episode: 86, duration: 0.182s, episode steps:  21, steps per second: 115, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 1.648942, mae: 4.943133, mean_q: 9.278862, mean_eps: 0.100000\n",
            " 1581/8000: episode: 87, duration: 0.265s, episode steps:  32, steps per second: 121, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.281 [0.000, 1.000],  loss: 1.606722, mae: 5.055308, mean_q: 9.419678, mean_eps: 0.100000\n",
            " 1597/8000: episode: 88, duration: 0.126s, episode steps:  16, steps per second: 127, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.110121, mae: 5.041367, mean_q: 9.506153, mean_eps: 0.100000\n",
            " 1614/8000: episode: 89, duration: 0.144s, episode steps:  17, steps per second: 118, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 1.354736, mae: 5.096190, mean_q: 9.629143, mean_eps: 0.100000\n",
            " 1628/8000: episode: 90, duration: 0.108s, episode steps:  14, steps per second: 129, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 1.545304, mae: 5.196193, mean_q: 9.794849, mean_eps: 0.100000\n",
            " 1639/8000: episode: 91, duration: 0.086s, episode steps:  11, steps per second: 128, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 1.309109, mae: 5.243335, mean_q: 9.978744, mean_eps: 0.100000\n",
            " 1651/8000: episode: 92, duration: 0.099s, episode steps:  12, steps per second: 121, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 1.333969, mae: 5.263461, mean_q: 10.119830, mean_eps: 0.100000\n",
            " 1661/8000: episode: 93, duration: 0.085s, episode steps:  10, steps per second: 117, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 1.604587, mae: 5.397888, mean_q: 10.374701, mean_eps: 0.100000\n",
            " 1672/8000: episode: 94, duration: 0.091s, episode steps:  11, steps per second: 121, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 1.361316, mae: 5.403277, mean_q: 10.390081, mean_eps: 0.100000\n",
            " 1681/8000: episode: 95, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 1.537142, mae: 5.311998, mean_q: 10.136074, mean_eps: 0.100000\n",
            " 1691/8000: episode: 96, duration: 0.095s, episode steps:  10, steps per second: 105, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 1.644774, mae: 5.489180, mean_q: 10.480484, mean_eps: 0.100000\n",
            " 1700/8000: episode: 97, duration: 0.076s, episode steps:   9, steps per second: 119, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 2.204057, mae: 5.578819, mean_q: 10.509307, mean_eps: 0.100000\n",
            " 1709/8000: episode: 98, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.836327, mae: 5.430165, mean_q: 10.467287, mean_eps: 0.100000\n",
            " 1717/8000: episode: 99, duration: 0.067s, episode steps:   8, steps per second: 119, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.875 [0.000, 1.000],  loss: 1.902564, mae: 5.595257, mean_q: 10.529221, mean_eps: 0.100000\n",
            " 1728/8000: episode: 100, duration: 0.086s, episode steps:  11, steps per second: 128, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 1.442798, mae: 5.508566, mean_q: 10.427188, mean_eps: 0.100000\n",
            " 1737/8000: episode: 101, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 1.006927, mae: 5.427783, mean_q: 10.418150, mean_eps: 0.100000\n",
            " 1747/8000: episode: 102, duration: 0.083s, episode steps:  10, steps per second: 120, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 1.930676, mae: 5.569956, mean_q: 10.677534, mean_eps: 0.100000\n",
            " 1757/8000: episode: 103, duration: 0.080s, episode steps:  10, steps per second: 126, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 1.939171, mae: 5.683956, mean_q: 10.762524, mean_eps: 0.100000\n",
            " 1767/8000: episode: 104, duration: 0.090s, episode steps:  10, steps per second: 111, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 2.389793, mae: 5.619316, mean_q: 10.543263, mean_eps: 0.100000\n",
            " 1777/8000: episode: 105, duration: 0.082s, episode steps:  10, steps per second: 122, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 2.877400, mae: 5.852978, mean_q: 10.855957, mean_eps: 0.100000\n",
            " 1787/8000: episode: 106, duration: 0.083s, episode steps:  10, steps per second: 121, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 2.286690, mae: 5.828304, mean_q: 11.008438, mean_eps: 0.100000\n",
            " 1797/8000: episode: 107, duration: 0.089s, episode steps:  10, steps per second: 112, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 2.695654, mae: 5.932931, mean_q: 11.179414, mean_eps: 0.100000\n",
            " 1807/8000: episode: 108, duration: 0.087s, episode steps:  10, steps per second: 115, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 3.207489, mae: 5.971814, mean_q: 11.124772, mean_eps: 0.100000\n",
            " 1818/8000: episode: 109, duration: 0.095s, episode steps:  11, steps per second: 116, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 2.733914, mae: 5.948424, mean_q: 11.054875, mean_eps: 0.100000\n",
            " 1832/8000: episode: 110, duration: 0.124s, episode steps:  14, steps per second: 113, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.857 [0.000, 1.000],  loss: 2.647652, mae: 6.048482, mean_q: 11.223030, mean_eps: 0.100000\n",
            " 1841/8000: episode: 111, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 2.905802, mae: 6.039470, mean_q: 11.265721, mean_eps: 0.100000\n",
            " 1850/8000: episode: 112, duration: 0.075s, episode steps:   9, steps per second: 119, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 2.780333, mae: 6.093666, mean_q: 11.458595, mean_eps: 0.100000\n",
            " 1860/8000: episode: 113, duration: 0.083s, episode steps:  10, steps per second: 120, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 2.501968, mae: 5.988203, mean_q: 11.230691, mean_eps: 0.100000\n",
            " 1869/8000: episode: 114, duration: 0.082s, episode steps:   9, steps per second: 109, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 2.541585, mae: 6.173069, mean_q: 11.592804, mean_eps: 0.100000\n",
            " 1881/8000: episode: 115, duration: 0.100s, episode steps:  12, steps per second: 120, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 1.842100, mae: 6.138992, mean_q: 11.581456, mean_eps: 0.100000\n",
            " 1889/8000: episode: 116, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.875 [0.000, 1.000],  loss: 3.515145, mae: 6.268825, mean_q: 11.793104, mean_eps: 0.100000\n",
            " 1898/8000: episode: 117, duration: 0.073s, episode steps:   9, steps per second: 123, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 3.349777, mae: 6.203124, mean_q: 11.605294, mean_eps: 0.100000\n",
            " 1907/8000: episode: 118, duration: 0.072s, episode steps:   9, steps per second: 124, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 4.614445, mae: 6.304559, mean_q: 11.506873, mean_eps: 0.100000\n",
            " 1917/8000: episode: 119, duration: 0.099s, episode steps:  10, steps per second: 101, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 2.530510, mae: 6.261339, mean_q: 11.688885, mean_eps: 0.100000\n",
            " 1928/8000: episode: 120, duration: 0.100s, episode steps:  11, steps per second: 110, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 4.183012, mae: 6.390962, mean_q: 11.656140, mean_eps: 0.100000\n",
            " 1939/8000: episode: 121, duration: 0.089s, episode steps:  11, steps per second: 124, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 3.897238, mae: 6.456580, mean_q: 11.829695, mean_eps: 0.100000\n",
            " 1950/8000: episode: 122, duration: 0.101s, episode steps:  11, steps per second: 109, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 2.720438, mae: 6.309987, mean_q: 11.568910, mean_eps: 0.100000\n",
            " 1960/8000: episode: 123, duration: 0.079s, episode steps:  10, steps per second: 126, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 2.693853, mae: 6.320304, mean_q: 11.746185, mean_eps: 0.100000\n",
            " 1973/8000: episode: 124, duration: 0.112s, episode steps:  13, steps per second: 116, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.846 [0.000, 1.000],  loss: 3.181069, mae: 6.334190, mean_q: 11.782135, mean_eps: 0.100000\n",
            " 1982/8000: episode: 125, duration: 0.079s, episode steps:   9, steps per second: 113, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 3.053808, mae: 6.491276, mean_q: 12.058634, mean_eps: 0.100000\n",
            " 1990/8000: episode: 126, duration: 0.063s, episode steps:   8, steps per second: 126, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.875 [0.000, 1.000],  loss: 2.748607, mae: 6.404701, mean_q: 11.927938, mean_eps: 0.100000\n",
            " 1999/8000: episode: 127, duration: 0.078s, episode steps:   9, steps per second: 116, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 4.328419, mae: 6.469154, mean_q: 11.811453, mean_eps: 0.100000\n",
            " 2015/8000: episode: 128, duration: 0.128s, episode steps:  16, steps per second: 125, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 3.812750, mae: 6.442238, mean_q: 11.735662, mean_eps: 0.100000\n",
            " 2026/8000: episode: 129, duration: 0.086s, episode steps:  11, steps per second: 127, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 3.030410, mae: 6.448827, mean_q: 11.867743, mean_eps: 0.100000\n",
            " 2036/8000: episode: 130, duration: 0.122s, episode steps:  10, steps per second:  82, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 3.446132, mae: 6.537579, mean_q: 11.984815, mean_eps: 0.100000\n",
            " 2044/8000: episode: 131, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 3.363350, mae: 6.610317, mean_q: 12.142205, mean_eps: 0.100000\n",
            " 2053/8000: episode: 132, duration: 0.080s, episode steps:   9, steps per second: 112, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 3.434044, mae: 6.521572, mean_q: 11.966784, mean_eps: 0.100000\n",
            " 2063/8000: episode: 133, duration: 0.083s, episode steps:  10, steps per second: 120, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 3.734883, mae: 6.437141, mean_q: 11.794263, mean_eps: 0.100000\n",
            " 2072/8000: episode: 134, duration: 0.073s, episode steps:   9, steps per second: 123, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 3.378669, mae: 6.427721, mean_q: 11.862426, mean_eps: 0.100000\n",
            " 2082/8000: episode: 135, duration: 0.087s, episode steps:  10, steps per second: 115, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 4.612234, mae: 6.817429, mean_q: 12.456053, mean_eps: 0.100000\n",
            " 2094/8000: episode: 136, duration: 0.102s, episode steps:  12, steps per second: 117, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 3.392626, mae: 6.536033, mean_q: 12.045774, mean_eps: 0.100000\n",
            " 2103/8000: episode: 137, duration: 0.077s, episode steps:   9, steps per second: 116, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 3.575453, mae: 6.609137, mean_q: 11.995036, mean_eps: 0.100000\n",
            " 2112/8000: episode: 138, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 4.589503, mae: 6.636777, mean_q: 12.124338, mean_eps: 0.100000\n",
            " 2122/8000: episode: 139, duration: 0.090s, episode steps:  10, steps per second: 111, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 2.630372, mae: 6.496278, mean_q: 12.065816, mean_eps: 0.100000\n",
            " 2131/8000: episode: 140, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 3.955145, mae: 6.649754, mean_q: 12.263938, mean_eps: 0.100000\n",
            " 2140/8000: episode: 141, duration: 0.076s, episode steps:   9, steps per second: 118, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 2.898860, mae: 6.587840, mean_q: 12.270610, mean_eps: 0.100000\n",
            " 2150/8000: episode: 142, duration: 0.090s, episode steps:  10, steps per second: 112, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 2.461339, mae: 6.520989, mean_q: 12.226939, mean_eps: 0.100000\n",
            " 2160/8000: episode: 143, duration: 0.090s, episode steps:  10, steps per second: 111, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 3.293826, mae: 6.675138, mean_q: 12.450958, mean_eps: 0.100000\n",
            " 2173/8000: episode: 144, duration: 0.108s, episode steps:  13, steps per second: 120, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.846 [0.000, 1.000],  loss: 3.734110, mae: 6.649559, mean_q: 12.209692, mean_eps: 0.100000\n",
            " 2182/8000: episode: 145, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 3.720125, mae: 6.725421, mean_q: 12.285881, mean_eps: 0.100000\n",
            " 2193/8000: episode: 146, duration: 0.091s, episode steps:  11, steps per second: 121, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 3.172886, mae: 6.609427, mean_q: 12.145429, mean_eps: 0.100000\n",
            " 2201/8000: episode: 147, duration: 0.071s, episode steps:   8, steps per second: 112, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.875 [0.000, 1.000],  loss: 4.292989, mae: 6.688755, mean_q: 12.186540, mean_eps: 0.100000\n",
            " 2210/8000: episode: 148, duration: 0.075s, episode steps:   9, steps per second: 120, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 4.906895, mae: 6.813851, mean_q: 12.148993, mean_eps: 0.100000\n",
            " 2221/8000: episode: 149, duration: 0.088s, episode steps:  11, steps per second: 125, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 3.059122, mae: 6.735246, mean_q: 12.264722, mean_eps: 0.100000\n",
            " 2231/8000: episode: 150, duration: 0.088s, episode steps:  10, steps per second: 114, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 3.237442, mae: 6.689586, mean_q: 12.196383, mean_eps: 0.100000\n",
            " 2241/8000: episode: 151, duration: 0.083s, episode steps:  10, steps per second: 120, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 3.709481, mae: 6.621964, mean_q: 12.025996, mean_eps: 0.100000\n",
            " 2250/8000: episode: 152, duration: 0.073s, episode steps:   9, steps per second: 124, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 4.777496, mae: 6.804825, mean_q: 12.219160, mean_eps: 0.100000\n",
            " 2260/8000: episode: 153, duration: 0.092s, episode steps:  10, steps per second: 108, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 2.817265, mae: 6.584086, mean_q: 12.064957, mean_eps: 0.100000\n",
            " 2271/8000: episode: 154, duration: 0.092s, episode steps:  11, steps per second: 120, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 3.431094, mae: 6.771512, mean_q: 12.415652, mean_eps: 0.100000\n",
            " 2282/8000: episode: 155, duration: 0.098s, episode steps:  11, steps per second: 112, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 4.035268, mae: 6.867921, mean_q: 12.540693, mean_eps: 0.100000\n",
            " 2294/8000: episode: 156, duration: 0.103s, episode steps:  12, steps per second: 117, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 2.717495, mae: 6.650494, mean_q: 12.234288, mean_eps: 0.100000\n",
            " 2308/8000: episode: 157, duration: 0.120s, episode steps:  14, steps per second: 117, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.163246, mae: 6.694295, mean_q: 12.240648, mean_eps: 0.100000\n",
            " 2321/8000: episode: 158, duration: 0.107s, episode steps:  13, steps per second: 122, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 3.014239, mae: 6.755338, mean_q: 12.558511, mean_eps: 0.100000\n",
            " 2332/8000: episode: 159, duration: 0.088s, episode steps:  11, steps per second: 125, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 3.357572, mae: 6.692179, mean_q: 12.429051, mean_eps: 0.100000\n",
            " 2343/8000: episode: 160, duration: 0.084s, episode steps:  11, steps per second: 131, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 3.293468, mae: 6.691063, mean_q: 12.356970, mean_eps: 0.100000\n",
            " 2359/8000: episode: 161, duration: 0.134s, episode steps:  16, steps per second: 119, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 3.130821, mae: 6.732539, mean_q: 12.439521, mean_eps: 0.100000\n",
            " 2374/8000: episode: 162, duration: 0.119s, episode steps:  15, steps per second: 126, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 3.216922, mae: 6.764735, mean_q: 12.465731, mean_eps: 0.100000\n",
            " 2386/8000: episode: 163, duration: 0.104s, episode steps:  12, steps per second: 115, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 3.652099, mae: 6.749144, mean_q: 12.398746, mean_eps: 0.100000\n",
            " 2398/8000: episode: 164, duration: 0.115s, episode steps:  12, steps per second: 104, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 4.548944, mae: 6.778184, mean_q: 12.272992, mean_eps: 0.100000\n",
            " 2410/8000: episode: 165, duration: 0.095s, episode steps:  12, steps per second: 127, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 3.342307, mae: 6.731577, mean_q: 12.280529, mean_eps: 0.100000\n",
            " 2424/8000: episode: 166, duration: 0.104s, episode steps:  14, steps per second: 134, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 3.869384, mae: 6.745432, mean_q: 12.239829, mean_eps: 0.100000\n",
            " 2443/8000: episode: 167, duration: 0.150s, episode steps:  19, steps per second: 126, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 4.097139, mae: 6.809401, mean_q: 12.445812, mean_eps: 0.100000\n",
            " 2455/8000: episode: 168, duration: 0.094s, episode steps:  12, steps per second: 128, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 4.508972, mae: 6.848389, mean_q: 12.303010, mean_eps: 0.100000\n",
            " 2472/8000: episode: 169, duration: 0.131s, episode steps:  17, steps per second: 130, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 3.952646, mae: 6.819202, mean_q: 12.389796, mean_eps: 0.100000\n",
            " 2485/8000: episode: 170, duration: 0.098s, episode steps:  13, steps per second: 133, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 2.798534, mae: 6.595910, mean_q: 12.165301, mean_eps: 0.100000\n",
            " 2501/8000: episode: 171, duration: 0.131s, episode steps:  16, steps per second: 122, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 2.462179, mae: 6.638467, mean_q: 12.321980, mean_eps: 0.100000\n",
            " 2516/8000: episode: 172, duration: 0.120s, episode steps:  15, steps per second: 125, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 2.752656, mae: 6.658621, mean_q: 12.378237, mean_eps: 0.100000\n",
            " 2533/8000: episode: 173, duration: 0.140s, episode steps:  17, steps per second: 121, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.889261, mae: 6.667274, mean_q: 12.373834, mean_eps: 0.100000\n",
            " 2549/8000: episode: 174, duration: 0.128s, episode steps:  16, steps per second: 125, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.520778, mae: 6.654535, mean_q: 12.099852, mean_eps: 0.100000\n",
            " 2593/8000: episode: 175, duration: 0.341s, episode steps:  44, steps per second: 129, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.699131, mae: 6.692433, mean_q: 12.319930, mean_eps: 0.100000\n",
            " 2673/8000: episode: 176, duration: 0.635s, episode steps:  80, steps per second: 126, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 3.180500, mae: 6.802519, mean_q: 12.477193, mean_eps: 0.100000\n",
            " 2801/8000: episode: 177, duration: 1.001s, episode steps: 128, steps per second: 128, episode reward: 128.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 2.861607, mae: 6.824359, mean_q: 12.698110, mean_eps: 0.100000\n",
            " 2971/8000: episode: 178, duration: 1.346s, episode steps: 170, steps per second: 126, episode reward: 170.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 2.520933, mae: 7.027642, mean_q: 13.178932, mean_eps: 0.100000\n",
            " 3071/8000: episode: 179, duration: 0.806s, episode steps: 100, steps per second: 124, episode reward: 100.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.580 [0.000, 1.000],  loss: 2.597969, mae: 7.320722, mean_q: 13.785424, mean_eps: 0.100000\n",
            " 3193/8000: episode: 180, duration: 0.979s, episode steps: 122, steps per second: 125, episode reward: 122.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.549 [0.000, 1.000],  loss: 2.995368, mae: 7.462550, mean_q: 14.018220, mean_eps: 0.100000\n",
            " 3298/8000: episode: 181, duration: 0.804s, episode steps: 105, steps per second: 131, episode reward: 105.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 2.842113, mae: 7.739134, mean_q: 14.698270, mean_eps: 0.100000\n",
            " 3402/8000: episode: 182, duration: 0.836s, episode steps: 104, steps per second: 124, episode reward: 104.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 2.972515, mae: 7.895813, mean_q: 15.059612, mean_eps: 0.100000\n",
            " 3468/8000: episode: 183, duration: 0.519s, episode steps:  66, steps per second: 127, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 3.320283, mae: 8.156108, mean_q: 15.486759, mean_eps: 0.100000\n",
            " 3536/8000: episode: 184, duration: 0.528s, episode steps:  68, steps per second: 129, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  loss: 3.131210, mae: 8.167571, mean_q: 15.539264, mean_eps: 0.100000\n",
            " 3713/8000: episode: 185, duration: 1.383s, episode steps: 177, steps per second: 128, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 3.149207, mae: 8.484547, mean_q: 16.251395, mean_eps: 0.100000\n",
            " 3792/8000: episode: 186, duration: 0.622s, episode steps:  79, steps per second: 127, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 3.590181, mae: 8.657752, mean_q: 16.584046, mean_eps: 0.100000\n",
            " 3922/8000: episode: 187, duration: 1.018s, episode steps: 130, steps per second: 128, episode reward: 130.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.983110, mae: 8.860725, mean_q: 16.906528, mean_eps: 0.100000\n",
            " 4075/8000: episode: 188, duration: 1.171s, episode steps: 153, steps per second: 131, episode reward: 153.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 3.457432, mae: 9.111497, mean_q: 17.449051, mean_eps: 0.100000\n",
            " 4142/8000: episode: 189, duration: 0.532s, episode steps:  67, steps per second: 126, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 2.736457, mae: 9.324732, mean_q: 17.975039, mean_eps: 0.100000\n",
            " 4281/8000: episode: 190, duration: 1.089s, episode steps: 139, steps per second: 128, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 3.580308, mae: 9.490140, mean_q: 18.235059, mean_eps: 0.100000\n",
            " 4382/8000: episode: 191, duration: 0.820s, episode steps: 101, steps per second: 123, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 3.439531, mae: 9.750623, mean_q: 18.761379, mean_eps: 0.100000\n",
            " 4457/8000: episode: 192, duration: 0.587s, episode steps:  75, steps per second: 128, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 3.715161, mae: 9.975282, mean_q: 19.236691, mean_eps: 0.100000\n",
            " 4567/8000: episode: 193, duration: 0.862s, episode steps: 110, steps per second: 128, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 3.800888, mae: 10.062704, mean_q: 19.429725, mean_eps: 0.100000\n",
            " 4651/8000: episode: 194, duration: 0.727s, episode steps:  84, steps per second: 116, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 4.395903, mae: 10.286932, mean_q: 19.839004, mean_eps: 0.100000\n",
            " 4724/8000: episode: 195, duration: 0.586s, episode steps:  73, steps per second: 125, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 3.066495, mae: 10.419547, mean_q: 20.269910, mean_eps: 0.100000\n",
            " 4817/8000: episode: 196, duration: 0.728s, episode steps:  93, steps per second: 128, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 4.686925, mae: 10.612826, mean_q: 20.503384, mean_eps: 0.100000\n",
            " 4866/8000: episode: 197, duration: 0.403s, episode steps:  49, steps per second: 122, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.551 [0.000, 1.000],  loss: 3.949057, mae: 10.689195, mean_q: 20.663992, mean_eps: 0.100000\n",
            " 5066/8000: episode: 198, duration: 1.500s, episode steps: 200, steps per second: 133, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.457823, mae: 10.911078, mean_q: 21.056064, mean_eps: 0.100000\n",
            " 5178/8000: episode: 199, duration: 0.852s, episode steps: 112, steps per second: 131, episode reward: 112.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 3.771471, mae: 11.203552, mean_q: 21.782316, mean_eps: 0.100000\n",
            " 5258/8000: episode: 200, duration: 0.623s, episode steps:  80, steps per second: 128, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 4.493097, mae: 11.259201, mean_q: 21.824437, mean_eps: 0.100000\n",
            " 5458/8000: episode: 201, duration: 1.552s, episode steps: 200, steps per second: 129, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.612273, mae: 11.501419, mean_q: 22.295371, mean_eps: 0.100000\n",
            " 5564/8000: episode: 202, duration: 0.828s, episode steps: 106, steps per second: 128, episode reward: 106.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 4.653141, mae: 11.850387, mean_q: 23.030497, mean_eps: 0.100000\n",
            " 5679/8000: episode: 203, duration: 0.897s, episode steps: 115, steps per second: 128, episode reward: 115.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 4.736999, mae: 12.046192, mean_q: 23.453166, mean_eps: 0.100000\n",
            " 5879/8000: episode: 204, duration: 1.525s, episode steps: 200, steps per second: 131, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.866281, mae: 12.249336, mean_q: 23.896239, mean_eps: 0.100000\n",
            " 5948/8000: episode: 205, duration: 0.532s, episode steps:  69, steps per second: 130, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 4.091653, mae: 12.454383, mean_q: 24.410034, mean_eps: 0.100000\n",
            " 6053/8000: episode: 206, duration: 0.842s, episode steps: 105, steps per second: 125, episode reward: 105.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 5.236636, mae: 12.670790, mean_q: 24.760086, mean_eps: 0.100000\n",
            " 6202/8000: episode: 207, duration: 1.154s, episode steps: 149, steps per second: 129, episode reward: 149.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 5.078464, mae: 12.889737, mean_q: 25.158335, mean_eps: 0.100000\n",
            " 6362/8000: episode: 208, duration: 1.204s, episode steps: 160, steps per second: 133, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 4.556741, mae: 13.076560, mean_q: 25.638654, mean_eps: 0.100000\n",
            " 6487/8000: episode: 209, duration: 0.964s, episode steps: 125, steps per second: 130, episode reward: 125.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 5.011875, mae: 13.334800, mean_q: 26.176273, mean_eps: 0.100000\n",
            " 6687/8000: episode: 210, duration: 1.558s, episode steps: 200, steps per second: 128, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.256515, mae: 13.660783, mean_q: 26.757005, mean_eps: 0.100000\n",
            " 6887/8000: episode: 211, duration: 1.562s, episode steps: 200, steps per second: 128, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 5.621298, mae: 13.949742, mean_q: 27.363014, mean_eps: 0.100000\n",
            " 7087/8000: episode: 212, duration: 1.542s, episode steps: 200, steps per second: 130, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 5.182605, mae: 14.209339, mean_q: 27.897171, mean_eps: 0.100000\n",
            " 7287/8000: episode: 213, duration: 1.566s, episode steps: 200, steps per second: 128, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 5.763424, mae: 14.602880, mean_q: 28.725963, mean_eps: 0.100000\n",
            " 7392/8000: episode: 214, duration: 0.825s, episode steps: 105, steps per second: 127, episode reward: 105.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 4.943317, mae: 14.847133, mean_q: 29.281390, mean_eps: 0.100000\n",
            " 7523/8000: episode: 215, duration: 1.013s, episode steps: 131, steps per second: 129, episode reward: 131.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 6.024961, mae: 14.984029, mean_q: 29.508798, mean_eps: 0.100000\n",
            " 7723/8000: episode: 216, duration: 1.524s, episode steps: 200, steps per second: 131, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.220954, mae: 15.217242, mean_q: 30.071359, mean_eps: 0.100000\n",
            " 7923/8000: episode: 217, duration: 1.589s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 5.054312, mae: 15.563573, mean_q: 30.767098, mean_eps: 0.100000\n",
            "done, took 72.554 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 158.000, steps: 158\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 147.000, steps: 147\n",
            "Episode 6: reward: 200.000, steps: 200\n",
            "Episode 7: reward: 200.000, steps: 200\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 186.000, steps: 186\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 180.000, steps: 180\n",
            "Episode 13: reward: 172.000, steps: 172\n",
            "Episode 14: reward: 159.000, steps: 159\n",
            "Episode 15: reward: 200.000, steps: 200\n",
            "Episode 16: reward: 153.000, steps: 153\n",
            "Episode 17: reward: 200.000, steps: 200\n",
            "Episode 18: reward: 200.000, steps: 200\n",
            "Episode 19: reward: 200.000, steps: 200\n",
            "Episode 20: reward: 200.000, steps: 200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5b9ea91250>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}