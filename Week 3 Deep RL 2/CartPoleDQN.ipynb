{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPoleDQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Megacity1/CartpoleDQN/blob/main/Week%203%20Deep%20RL%202/CartPoleDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Deep Q Network (DQN) for CartPole Using Boltzmann Q Policy\n",
        "This exercise implements a DQN for CartPole using a Boltzmann Q policy for selecting the actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGsC7cJ5jNcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda73cbf-3020-40f4-e45e-7b87053448f1"
      },
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.24.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (13.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.44.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.10.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMIHLgQ3Z-lF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5a8608-456f-4127-ce8a-f551e5f4e4f3"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0AMLzq08ap0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b380e1-6c3d-4bce-8233-a8ac01deaf26"
      },
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "!pip install Adam\n",
        "#from keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Adam in /usr/local/lib/python3.7/dist-packages (0.0.0.dev0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll6bNdUm54WS"
      },
      "source": [
        "Implementation of DQN for CartPole, applying policy BoltzmannQPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCrPKNy40PC"
      },
      "source": [
        "##Implement DQN with BoltzmannGumbelQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efM9jkXr5A3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d8b8de7f-34b8-415e-8fd8-a224afc60f99"
      },
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import BoltzmannGumbelQPolicy\n",
        "from rl.policy import LinearAnnealedPolicy\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n",
        "# setup experience replay buffer\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
        "                               attr='eps',            \n",
        "                               value_max=5,\n",
        "                               value_min=5, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=20)\n",
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "# add extra layers here\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=20,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy) \n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=8000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_52\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_50 (Flatten)        (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_101 (Dense)           (None, 16)                80        \n",
            "                                                                 \n",
            " dense_102 (Dense)           (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 8000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   17/8000: episode: 1, duration: 3.039s, episode steps:  17, steps per second:   6, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   31/8000: episode: 2, duration: 11.211s, episode steps:  14, steps per second:   1, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.581445, mae: 0.627457, mean_q: 0.220835, mean_eps: 5.000000\n",
            "   80/8000: episode: 3, duration: 0.771s, episode steps:  49, steps per second:  64, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 0.415388, mae: 0.596301, mean_q: 0.308752, mean_eps: 5.000000\n",
            "   95/8000: episode: 4, duration: 0.284s, episode steps:  15, steps per second:  53, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.301264, mae: 0.619944, mean_q: 0.497124, mean_eps: 5.000000\n",
            "  125/8000: episode: 5, duration: 0.492s, episode steps:  30, steps per second:  61, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.247506, mae: 0.656974, mean_q: 0.658020, mean_eps: 5.000000\n",
            "  140/8000: episode: 6, duration: 0.246s, episode steps:  15, steps per second:  61, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.220461, mae: 0.712683, mean_q: 0.820258, mean_eps: 5.000000\n",
            "  155/8000: episode: 7, duration: 0.234s, episode steps:  15, steps per second:  64, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.202562, mae: 0.758844, mean_q: 0.954663, mean_eps: 5.000000\n",
            "  176/8000: episode: 8, duration: 0.369s, episode steps:  21, steps per second:  57, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.219228, mae: 0.813433, mean_q: 1.076370, mean_eps: 5.000000\n",
            "  196/8000: episode: 9, duration: 0.331s, episode steps:  20, steps per second:  60, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.172628, mae: 0.838590, mean_q: 1.233765, mean_eps: 5.000000\n",
            "  210/8000: episode: 10, duration: 0.253s, episode steps:  14, steps per second:  55, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.202303, mae: 0.872216, mean_q: 1.275337, mean_eps: 5.000000\n",
            "  268/8000: episode: 11, duration: 1.249s, episode steps:  58, steps per second:  46, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 0.192266, mae: 0.950518, mean_q: 1.486290, mean_eps: 5.000000\n",
            "  308/8000: episode: 12, duration: 1.123s, episode steps:  40, steps per second:  36, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.172609, mae: 1.098724, mean_q: 1.890693, mean_eps: 5.000000\n",
            "  334/8000: episode: 13, duration: 0.484s, episode steps:  26, steps per second:  54, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.182503, mae: 1.215853, mean_q: 2.121996, mean_eps: 5.000000\n",
            "  357/8000: episode: 14, duration: 0.375s, episode steps:  23, steps per second:  61, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  loss: 0.160422, mae: 1.268867, mean_q: 2.318830, mean_eps: 5.000000\n",
            "  406/8000: episode: 15, duration: 0.783s, episode steps:  49, steps per second:  63, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  loss: 0.209983, mae: 1.416618, mean_q: 2.651167, mean_eps: 5.000000\n",
            "  425/8000: episode: 16, duration: 0.330s, episode steps:  19, steps per second:  58, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.207530, mae: 1.515572, mean_q: 2.859101, mean_eps: 5.000000\n",
            "  444/8000: episode: 17, duration: 0.336s, episode steps:  19, steps per second:  57, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.316 [0.000, 1.000],  loss: 0.198946, mae: 1.568109, mean_q: 2.977842, mean_eps: 5.000000\n",
            "  478/8000: episode: 18, duration: 0.588s, episode steps:  34, steps per second:  58, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.559 [0.000, 1.000],  loss: 0.225119, mae: 1.731537, mean_q: 3.345921, mean_eps: 5.000000\n",
            "  494/8000: episode: 19, duration: 0.291s, episode steps:  16, steps per second:  55, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.355995, mae: 1.885908, mean_q: 3.625652, mean_eps: 5.000000\n",
            "  506/8000: episode: 20, duration: 0.264s, episode steps:  12, steps per second:  45, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.302159, mae: 1.892145, mean_q: 3.643458, mean_eps: 5.000000\n",
            "  540/8000: episode: 21, duration: 0.599s, episode steps:  34, steps per second:  57, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.559 [0.000, 1.000],  loss: 0.246138, mae: 2.016215, mean_q: 3.960027, mean_eps: 5.000000\n",
            "  556/8000: episode: 22, duration: 0.269s, episode steps:  16, steps per second:  59, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.289188, mae: 2.098630, mean_q: 4.154483, mean_eps: 5.000000\n",
            "  573/8000: episode: 23, duration: 0.286s, episode steps:  17, steps per second:  59, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: 0.271323, mae: 2.176282, mean_q: 4.277854, mean_eps: 5.000000\n",
            "  582/8000: episode: 24, duration: 0.169s, episode steps:   9, steps per second:  53, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.267633, mae: 2.201421, mean_q: 4.362271, mean_eps: 5.000000\n",
            "  602/8000: episode: 25, duration: 0.339s, episode steps:  20, steps per second:  59, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.369810, mae: 2.341752, mean_q: 4.628300, mean_eps: 5.000000\n",
            "  621/8000: episode: 26, duration: 0.315s, episode steps:  19, steps per second:  60, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.400419, mae: 2.464648, mean_q: 4.814162, mean_eps: 5.000000\n",
            "  639/8000: episode: 27, duration: 0.354s, episode steps:  18, steps per second:  51, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 0.312432, mae: 2.522744, mean_q: 5.033776, mean_eps: 5.000000\n",
            "  661/8000: episode: 28, duration: 0.398s, episode steps:  22, steps per second:  55, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.425136, mae: 2.635368, mean_q: 5.221859, mean_eps: 5.000000\n",
            "  690/8000: episode: 29, duration: 0.495s, episode steps:  29, steps per second:  59, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 0.366225, mae: 2.729607, mean_q: 5.435826, mean_eps: 5.000000\n",
            "  706/8000: episode: 30, duration: 0.280s, episode steps:  16, steps per second:  57, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.662687, mae: 2.916370, mean_q: 5.725851, mean_eps: 5.000000\n",
            "  723/8000: episode: 31, duration: 0.311s, episode steps:  17, steps per second:  55, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.508754, mae: 2.935544, mean_q: 5.697147, mean_eps: 5.000000\n",
            "  747/8000: episode: 32, duration: 0.441s, episode steps:  24, steps per second:  54, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.665320, mae: 3.080330, mean_q: 5.963232, mean_eps: 5.000000\n",
            "  780/8000: episode: 33, duration: 0.540s, episode steps:  33, steps per second:  61, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 0.683696, mae: 3.217208, mean_q: 6.242671, mean_eps: 5.000000\n",
            "  789/8000: episode: 34, duration: 0.168s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.602728, mae: 3.309907, mean_q: 6.560472, mean_eps: 5.000000\n",
            "  801/8000: episode: 35, duration: 0.205s, episode steps:  12, steps per second:  59, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.465098, mae: 3.342825, mean_q: 6.657729, mean_eps: 5.000000\n",
            "  814/8000: episode: 36, duration: 0.219s, episode steps:  13, steps per second:  59, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.778688, mae: 3.434566, mean_q: 6.756467, mean_eps: 5.000000\n",
            "  833/8000: episode: 37, duration: 0.304s, episode steps:  19, steps per second:  62, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 0.696551, mae: 3.514071, mean_q: 6.848643, mean_eps: 5.000000\n",
            "  844/8000: episode: 38, duration: 0.185s, episode steps:  11, steps per second:  59, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 1.241605, mae: 3.645976, mean_q: 6.898715, mean_eps: 5.000000\n",
            "  856/8000: episode: 39, duration: 0.215s, episode steps:  12, steps per second:  56, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.881667, mae: 3.659602, mean_q: 7.007663, mean_eps: 5.000000\n",
            "  868/8000: episode: 40, duration: 0.199s, episode steps:  12, steps per second:  60, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.765443, mae: 3.684709, mean_q: 7.107083, mean_eps: 5.000000\n",
            "  887/8000: episode: 41, duration: 0.357s, episode steps:  19, steps per second:  53, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 0.924863, mae: 3.794744, mean_q: 7.377389, mean_eps: 5.000000\n",
            "  896/8000: episode: 42, duration: 0.153s, episode steps:   9, steps per second:  59, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 1.080789, mae: 3.848495, mean_q: 7.478500, mean_eps: 5.000000\n",
            "  907/8000: episode: 43, duration: 0.194s, episode steps:  11, steps per second:  57, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1.028871, mae: 3.879760, mean_q: 7.525415, mean_eps: 5.000000\n",
            "  923/8000: episode: 44, duration: 0.268s, episode steps:  16, steps per second:  60, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 1.010230, mae: 3.976006, mean_q: 7.710602, mean_eps: 5.000000\n",
            "  933/8000: episode: 45, duration: 0.169s, episode steps:  10, steps per second:  59, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.006105, mae: 4.041011, mean_q: 7.823677, mean_eps: 5.000000\n",
            "  945/8000: episode: 46, duration: 0.195s, episode steps:  12, steps per second:  62, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.977842, mae: 4.129980, mean_q: 8.021337, mean_eps: 5.000000\n",
            "  958/8000: episode: 47, duration: 0.196s, episode steps:  13, steps per second:  66, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 1.072589, mae: 4.191163, mean_q: 8.167457, mean_eps: 5.000000\n",
            "  984/8000: episode: 48, duration: 0.637s, episode steps:  26, steps per second:  41, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 1.178872, mae: 4.258762, mean_q: 8.205161, mean_eps: 5.000000\n",
            "  994/8000: episode: 49, duration: 0.283s, episode steps:  10, steps per second:  35, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.042823, mae: 4.238383, mean_q: 8.167398, mean_eps: 5.000000\n",
            " 1017/8000: episode: 50, duration: 0.478s, episode steps:  23, steps per second:  48, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.652 [0.000, 1.000],  loss: 0.942285, mae: 4.395036, mean_q: 8.589587, mean_eps: 5.000000\n",
            " 1063/8000: episode: 51, duration: 0.693s, episode steps:  46, steps per second:  66, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.294590, mae: 4.540537, mean_q: 8.761608, mean_eps: 5.000000\n",
            " 1079/8000: episode: 52, duration: 0.394s, episode steps:  16, steps per second:  41, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 1.958807, mae: 4.750052, mean_q: 8.921199, mean_eps: 5.000000\n",
            " 1094/8000: episode: 53, duration: 0.392s, episode steps:  15, steps per second:  38, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.201365, mae: 4.714426, mean_q: 9.087093, mean_eps: 5.000000\n",
            " 1115/8000: episode: 54, duration: 0.479s, episode steps:  21, steps per second:  44, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 1.374102, mae: 4.819812, mean_q: 9.320096, mean_eps: 5.000000\n",
            " 1140/8000: episode: 55, duration: 0.397s, episode steps:  25, steps per second:  63, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 1.356626, mae: 4.912593, mean_q: 9.536685, mean_eps: 5.000000\n",
            " 1166/8000: episode: 56, duration: 0.417s, episode steps:  26, steps per second:  62, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1.537661, mae: 5.039900, mean_q: 9.744695, mean_eps: 5.000000\n",
            " 1197/8000: episode: 57, duration: 0.551s, episode steps:  31, steps per second:  56, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.419 [0.000, 1.000],  loss: 1.698746, mae: 5.190555, mean_q: 9.942822, mean_eps: 5.000000\n",
            " 1208/8000: episode: 58, duration: 0.172s, episode steps:  11, steps per second:  64, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 1.251563, mae: 5.203956, mean_q: 10.091078, mean_eps: 5.000000\n",
            " 1228/8000: episode: 59, duration: 0.318s, episode steps:  20, steps per second:  63, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 1.096139, mae: 5.255934, mean_q: 10.297610, mean_eps: 5.000000\n",
            " 1261/8000: episode: 60, duration: 0.537s, episode steps:  33, steps per second:  61, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 1.582566, mae: 5.411951, mean_q: 10.553874, mean_eps: 5.000000\n",
            " 1296/8000: episode: 61, duration: 0.609s, episode steps:  35, steps per second:  57, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.645331, mae: 5.561640, mean_q: 10.703442, mean_eps: 5.000000\n",
            " 1314/8000: episode: 62, duration: 0.300s, episode steps:  18, steps per second:  60, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 1.984980, mae: 5.696335, mean_q: 11.019813, mean_eps: 5.000000\n",
            " 1333/8000: episode: 63, duration: 0.312s, episode steps:  19, steps per second:  61, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.316 [0.000, 1.000],  loss: 1.886798, mae: 5.758666, mean_q: 11.103614, mean_eps: 5.000000\n",
            " 1345/8000: episode: 64, duration: 0.253s, episode steps:  12, steps per second:  47, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 2.193354, mae: 5.750142, mean_q: 11.043072, mean_eps: 5.000000\n",
            " 1357/8000: episode: 65, duration: 0.221s, episode steps:  12, steps per second:  54, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.800811, mae: 5.822258, mean_q: 11.182631, mean_eps: 5.000000\n",
            " 1374/8000: episode: 66, duration: 0.289s, episode steps:  17, steps per second:  59, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1.231842, mae: 5.839924, mean_q: 11.292705, mean_eps: 5.000000\n",
            " 1395/8000: episode: 67, duration: 0.385s, episode steps:  21, steps per second:  55, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 2.571040, mae: 6.011236, mean_q: 11.389309, mean_eps: 5.000000\n",
            " 1432/8000: episode: 68, duration: 0.612s, episode steps:  37, steps per second:  60, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.432 [0.000, 1.000],  loss: 2.301744, mae: 6.033371, mean_q: 11.305234, mean_eps: 5.000000\n",
            " 1445/8000: episode: 69, duration: 0.209s, episode steps:  13, steps per second:  62, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 2.448810, mae: 6.083219, mean_q: 11.380609, mean_eps: 5.000000\n",
            " 1476/8000: episode: 70, duration: 0.518s, episode steps:  31, steps per second:  60, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 1.930483, mae: 6.152368, mean_q: 11.640875, mean_eps: 5.000000\n",
            " 1507/8000: episode: 71, duration: 0.562s, episode steps:  31, steps per second:  55, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 2.089005, mae: 6.225092, mean_q: 11.854927, mean_eps: 5.000000\n",
            " 1531/8000: episode: 72, duration: 0.432s, episode steps:  24, steps per second:  56, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 1.778844, mae: 6.282177, mean_q: 12.158227, mean_eps: 5.000000\n",
            " 1542/8000: episode: 73, duration: 0.179s, episode steps:  11, steps per second:  61, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 1.391589, mae: 6.344920, mean_q: 12.324194, mean_eps: 5.000000\n",
            " 1568/8000: episode: 74, duration: 0.418s, episode steps:  26, steps per second:  62, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 2.122855, mae: 6.445887, mean_q: 12.371340, mean_eps: 5.000000\n",
            " 1579/8000: episode: 75, duration: 0.198s, episode steps:  11, steps per second:  56, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 1.612857, mae: 6.464551, mean_q: 12.400682, mean_eps: 5.000000\n",
            " 1609/8000: episode: 76, duration: 0.504s, episode steps:  30, steps per second:  60, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.294360, mae: 6.543275, mean_q: 12.407940, mean_eps: 5.000000\n",
            " 1635/8000: episode: 77, duration: 0.409s, episode steps:  26, steps per second:  64, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.346 [0.000, 1.000],  loss: 2.292298, mae: 6.628749, mean_q: 12.544998, mean_eps: 5.000000\n",
            " 1665/8000: episode: 78, duration: 0.508s, episode steps:  30, steps per second:  59, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 2.123884, mae: 6.682744, mean_q: 12.707309, mean_eps: 5.000000\n",
            " 1711/8000: episode: 79, duration: 0.712s, episode steps:  46, steps per second:  65, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 2.036445, mae: 6.758501, mean_q: 12.855122, mean_eps: 5.000000\n",
            " 1743/8000: episode: 80, duration: 0.495s, episode steps:  32, steps per second:  65, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.406 [0.000, 1.000],  loss: 2.423860, mae: 6.851456, mean_q: 12.971805, mean_eps: 5.000000\n",
            " 1754/8000: episode: 81, duration: 0.189s, episode steps:  11, steps per second:  58, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 1.649907, mae: 6.807074, mean_q: 12.970643, mean_eps: 5.000000\n",
            " 1767/8000: episode: 82, duration: 0.241s, episode steps:  13, steps per second:  54, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 2.112672, mae: 6.904364, mean_q: 13.105565, mean_eps: 5.000000\n",
            " 1799/8000: episode: 83, duration: 0.525s, episode steps:  32, steps per second:  61, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.964339, mae: 6.941372, mean_q: 13.210182, mean_eps: 5.000000\n",
            " 1866/8000: episode: 84, duration: 1.044s, episode steps:  67, steps per second:  64, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 2.541099, mae: 7.048387, mean_q: 13.320598, mean_eps: 5.000000\n",
            " 1888/8000: episode: 85, duration: 0.348s, episode steps:  22, steps per second:  63, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.137277, mae: 7.128234, mean_q: 13.555498, mean_eps: 5.000000\n",
            " 1927/8000: episode: 86, duration: 0.653s, episode steps:  39, steps per second:  60, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 2.030448, mae: 7.213707, mean_q: 13.822584, mean_eps: 5.000000\n",
            " 1941/8000: episode: 87, duration: 0.239s, episode steps:  14, steps per second:  58, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 2.368123, mae: 7.273299, mean_q: 13.848599, mean_eps: 5.000000\n",
            " 1961/8000: episode: 88, duration: 0.368s, episode steps:  20, steps per second:  54, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 2.742979, mae: 7.333929, mean_q: 13.998386, mean_eps: 5.000000\n",
            " 1976/8000: episode: 89, duration: 0.271s, episode steps:  15, steps per second:  55, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 2.107282, mae: 7.297811, mean_q: 13.935812, mean_eps: 5.000000\n",
            " 1989/8000: episode: 90, duration: 0.222s, episode steps:  13, steps per second:  59, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 2.817393, mae: 7.378256, mean_q: 13.998115, mean_eps: 5.000000\n",
            " 2009/8000: episode: 91, duration: 0.350s, episode steps:  20, steps per second:  57, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 2.578616, mae: 7.417938, mean_q: 14.154651, mean_eps: 5.000000\n",
            " 2026/8000: episode: 92, duration: 0.384s, episode steps:  17, steps per second:  44, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 2.627009, mae: 7.368923, mean_q: 14.017612, mean_eps: 5.000000\n",
            " 2053/8000: episode: 93, duration: 0.690s, episode steps:  27, steps per second:  39, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 2.291361, mae: 7.466728, mean_q: 14.313595, mean_eps: 5.000000\n",
            " 2064/8000: episode: 94, duration: 0.325s, episode steps:  11, steps per second:  34, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 2.929801, mae: 7.574157, mean_q: 14.420604, mean_eps: 5.000000\n",
            " 2078/8000: episode: 95, duration: 0.395s, episode steps:  14, steps per second:  35, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 2.302692, mae: 7.494544, mean_q: 14.347335, mean_eps: 5.000000\n",
            " 2102/8000: episode: 96, duration: 0.593s, episode steps:  24, steps per second:  40, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 2.394089, mae: 7.588859, mean_q: 14.501073, mean_eps: 5.000000\n",
            " 2116/8000: episode: 97, duration: 0.238s, episode steps:  14, steps per second:  59, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.555139, mae: 7.698216, mean_q: 14.737892, mean_eps: 5.000000\n",
            " 2131/8000: episode: 98, duration: 0.285s, episode steps:  15, steps per second:  53, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 2.743088, mae: 7.661614, mean_q: 14.625847, mean_eps: 5.000000\n",
            " 2155/8000: episode: 99, duration: 0.424s, episode steps:  24, steps per second:  57, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 2.146020, mae: 7.756385, mean_q: 14.919923, mean_eps: 5.000000\n",
            " 2186/8000: episode: 100, duration: 0.588s, episode steps:  31, steps per second:  53, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.613 [0.000, 1.000],  loss: 2.698031, mae: 7.787332, mean_q: 14.871434, mean_eps: 5.000000\n",
            " 2200/8000: episode: 101, duration: 0.285s, episode steps:  14, steps per second:  49, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 2.591694, mae: 7.877442, mean_q: 15.055899, mean_eps: 5.000000\n",
            " 2234/8000: episode: 102, duration: 0.584s, episode steps:  34, steps per second:  58, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 3.309954, mae: 7.888717, mean_q: 14.940118, mean_eps: 5.000000\n",
            " 2271/8000: episode: 103, duration: 0.797s, episode steps:  37, steps per second:  46, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 2.952582, mae: 7.978336, mean_q: 15.207306, mean_eps: 5.000000\n",
            " 2287/8000: episode: 104, duration: 0.471s, episode steps:  16, steps per second:  34, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 2.899797, mae: 7.985024, mean_q: 15.152582, mean_eps: 5.000000\n",
            " 2321/8000: episode: 105, duration: 0.966s, episode steps:  34, steps per second:  35, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.441 [0.000, 1.000],  loss: 2.663857, mae: 8.034313, mean_q: 15.410172, mean_eps: 5.000000\n",
            " 2342/8000: episode: 106, duration: 0.503s, episode steps:  21, steps per second:  42, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 3.270718, mae: 8.127058, mean_q: 15.473463, mean_eps: 5.000000\n",
            " 2363/8000: episode: 107, duration: 0.374s, episode steps:  21, steps per second:  56, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 3.309645, mae: 8.169015, mean_q: 15.522079, mean_eps: 5.000000\n",
            " 2377/8000: episode: 108, duration: 0.299s, episode steps:  14, steps per second:  47, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 3.913986, mae: 8.219102, mean_q: 15.548929, mean_eps: 5.000000\n",
            " 2394/8000: episode: 109, duration: 0.433s, episode steps:  17, steps per second:  39, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 2.511015, mae: 8.156402, mean_q: 15.603108, mean_eps: 5.000000\n",
            " 2406/8000: episode: 110, duration: 0.366s, episode steps:  12, steps per second:  33, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 3.014374, mae: 8.157425, mean_q: 15.686540, mean_eps: 5.000000\n",
            " 2421/8000: episode: 111, duration: 0.394s, episode steps:  15, steps per second:  38, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 3.422554, mae: 8.305910, mean_q: 15.835276, mean_eps: 5.000000\n",
            " 2435/8000: episode: 112, duration: 0.390s, episode steps:  14, steps per second:  36, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 2.638131, mae: 8.239333, mean_q: 15.777659, mean_eps: 5.000000\n",
            " 2464/8000: episode: 113, duration: 0.797s, episode steps:  29, steps per second:  36, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 2.454184, mae: 8.295017, mean_q: 15.952876, mean_eps: 5.000000\n",
            " 2494/8000: episode: 114, duration: 0.735s, episode steps:  30, steps per second:  41, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.367 [0.000, 1.000],  loss: 3.294539, mae: 8.444037, mean_q: 16.169231, mean_eps: 5.000000\n",
            " 2507/8000: episode: 115, duration: 0.378s, episode steps:  13, steps per second:  34, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 3.202354, mae: 8.511538, mean_q: 16.215576, mean_eps: 5.000000\n",
            " 2520/8000: episode: 116, duration: 0.371s, episode steps:  13, steps per second:  35, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 3.507586, mae: 8.433768, mean_q: 15.973674, mean_eps: 5.000000\n",
            " 2596/8000: episode: 117, duration: 1.696s, episode steps:  76, steps per second:  45, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.461 [0.000, 1.000],  loss: 3.229954, mae: 8.531231, mean_q: 16.287431, mean_eps: 5.000000\n",
            " 2614/8000: episode: 118, duration: 0.486s, episode steps:  18, steps per second:  37, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 3.534991, mae: 8.611108, mean_q: 16.504592, mean_eps: 5.000000\n",
            " 2625/8000: episode: 119, duration: 0.269s, episode steps:  11, steps per second:  41, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 2.522631, mae: 8.634061, mean_q: 16.643602, mean_eps: 5.000000\n",
            " 2676/8000: episode: 120, duration: 0.955s, episode steps:  51, steps per second:  53, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 3.358169, mae: 8.668488, mean_q: 16.620238, mean_eps: 5.000000\n",
            " 2695/8000: episode: 121, duration: 0.504s, episode steps:  19, steps per second:  38, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 2.221987, mae: 8.685492, mean_q: 16.865492, mean_eps: 5.000000\n",
            " 2711/8000: episode: 122, duration: 0.444s, episode steps:  16, steps per second:  36, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 2.735504, mae: 8.772896, mean_q: 17.022889, mean_eps: 5.000000\n",
            " 2727/8000: episode: 123, duration: 0.373s, episode steps:  16, steps per second:  43, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 2.808199, mae: 8.881129, mean_q: 17.165591, mean_eps: 5.000000\n",
            " 2744/8000: episode: 124, duration: 0.431s, episode steps:  17, steps per second:  39, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 3.893040, mae: 8.894524, mean_q: 17.099379, mean_eps: 5.000000\n",
            " 2760/8000: episode: 125, duration: 0.442s, episode steps:  16, steps per second:  36, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 4.512024, mae: 8.994609, mean_q: 17.159911, mean_eps: 5.000000\n",
            " 2780/8000: episode: 126, duration: 0.460s, episode steps:  20, steps per second:  44, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 4.205301, mae: 9.033865, mean_q: 17.274433, mean_eps: 5.000000\n",
            " 2802/8000: episode: 127, duration: 0.491s, episode steps:  22, steps per second:  45, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 3.946000, mae: 8.994658, mean_q: 17.188764, mean_eps: 5.000000\n",
            " 2819/8000: episode: 128, duration: 0.472s, episode steps:  17, steps per second:  36, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 2.624805, mae: 8.985877, mean_q: 17.374335, mean_eps: 5.000000\n",
            " 2839/8000: episode: 129, duration: 0.541s, episode steps:  20, steps per second:  37, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 3.537667, mae: 9.178324, mean_q: 17.683382, mean_eps: 5.000000\n",
            " 2866/8000: episode: 130, duration: 0.704s, episode steps:  27, steps per second:  38, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 3.388798, mae: 9.179081, mean_q: 17.679232, mean_eps: 5.000000\n",
            " 2883/8000: episode: 131, duration: 0.451s, episode steps:  17, steps per second:  38, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 3.300561, mae: 9.184435, mean_q: 17.735514, mean_eps: 5.000000\n",
            " 2900/8000: episode: 132, duration: 0.422s, episode steps:  17, steps per second:  40, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 3.516411, mae: 9.297848, mean_q: 17.878494, mean_eps: 5.000000\n",
            " 2910/8000: episode: 133, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 4.772002, mae: 9.385122, mean_q: 17.878597, mean_eps: 5.000000\n",
            " 2928/8000: episode: 134, duration: 0.466s, episode steps:  18, steps per second:  39, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 2.909578, mae: 9.180863, mean_q: 17.754058, mean_eps: 5.000000\n",
            " 2949/8000: episode: 135, duration: 0.488s, episode steps:  21, steps per second:  43, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 3.806620, mae: 9.450200, mean_q: 18.201646, mean_eps: 5.000000\n",
            " 2958/8000: episode: 136, duration: 0.234s, episode steps:   9, steps per second:  38, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.713833, mae: 9.407252, mean_q: 18.269304, mean_eps: 5.000000\n",
            " 2981/8000: episode: 137, duration: 0.404s, episode steps:  23, steps per second:  57, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 4.495231, mae: 9.478027, mean_q: 18.145912, mean_eps: 5.000000\n",
            " 3005/8000: episode: 138, duration: 0.435s, episode steps:  24, steps per second:  55, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 3.665999, mae: 9.507418, mean_q: 18.356838, mean_eps: 5.000000\n",
            " 3018/8000: episode: 139, duration: 0.250s, episode steps:  13, steps per second:  52, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 3.230315, mae: 9.413775, mean_q: 18.167162, mean_eps: 5.000000\n",
            " 3069/8000: episode: 140, duration: 0.963s, episode steps:  51, steps per second:  53, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.451 [0.000, 1.000],  loss: 3.954346, mae: 9.553433, mean_q: 18.342664, mean_eps: 5.000000\n",
            " 3105/8000: episode: 141, duration: 0.611s, episode steps:  36, steps per second:  59, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 3.993689, mae: 9.655250, mean_q: 18.517650, mean_eps: 5.000000\n",
            " 3114/8000: episode: 142, duration: 0.193s, episode steps:   9, steps per second:  47, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 3.285475, mae: 9.649161, mean_q: 18.538191, mean_eps: 5.000000\n",
            " 3130/8000: episode: 143, duration: 0.395s, episode steps:  16, steps per second:  41, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 3.725002, mae: 9.607534, mean_q: 18.456122, mean_eps: 5.000000\n",
            " 3147/8000: episode: 144, duration: 0.458s, episode steps:  17, steps per second:  37, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 6.760557, mae: 9.857953, mean_q: 18.561589, mean_eps: 5.000000\n",
            " 3177/8000: episode: 145, duration: 0.675s, episode steps:  30, steps per second:  44, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 3.296817, mae: 9.759952, mean_q: 18.734588, mean_eps: 5.000000\n",
            " 3188/8000: episode: 146, duration: 0.261s, episode steps:  11, steps per second:  42, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 3.726414, mae: 9.810770, mean_q: 18.923583, mean_eps: 5.000000\n",
            " 3200/8000: episode: 147, duration: 0.222s, episode steps:  12, steps per second:  54, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 3.844053, mae: 9.862612, mean_q: 19.059028, mean_eps: 5.000000\n",
            " 3210/8000: episode: 148, duration: 0.193s, episode steps:  10, steps per second:  52, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 3.820279, mae: 9.892020, mean_q: 19.034983, mean_eps: 5.000000\n",
            " 3221/8000: episode: 149, duration: 0.219s, episode steps:  11, steps per second:  50, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 3.693338, mae: 9.841992, mean_q: 18.941966, mean_eps: 5.000000\n",
            " 3240/8000: episode: 150, duration: 0.345s, episode steps:  19, steps per second:  55, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 5.012388, mae: 9.828754, mean_q: 18.725394, mean_eps: 5.000000\n",
            " 3257/8000: episode: 151, duration: 0.295s, episode steps:  17, steps per second:  58, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 3.620415, mae: 9.924223, mean_q: 19.025200, mean_eps: 5.000000\n",
            " 3328/8000: episode: 152, duration: 1.664s, episode steps:  71, steps per second:  43, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 4.931176, mae: 10.036256, mean_q: 19.146078, mean_eps: 5.000000\n",
            " 3348/8000: episode: 153, duration: 0.336s, episode steps:  20, steps per second:  60, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 4.340842, mae: 9.933746, mean_q: 18.862986, mean_eps: 5.000000\n",
            " 3401/8000: episode: 154, duration: 0.873s, episode steps:  53, steps per second:  61, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.585 [0.000, 1.000],  loss: 3.760070, mae: 9.997196, mean_q: 19.189285, mean_eps: 5.000000\n",
            " 3417/8000: episode: 155, duration: 0.263s, episode steps:  16, steps per second:  61, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 3.976975, mae: 10.138112, mean_q: 19.472871, mean_eps: 5.000000\n",
            " 3428/8000: episode: 156, duration: 0.205s, episode steps:  11, steps per second:  54, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 4.754959, mae: 10.088814, mean_q: 19.227115, mean_eps: 5.000000\n",
            " 3444/8000: episode: 157, duration: 0.298s, episode steps:  16, steps per second:  54, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 4.420660, mae: 10.242794, mean_q: 19.572746, mean_eps: 5.000000\n",
            " 3455/8000: episode: 158, duration: 0.192s, episode steps:  11, steps per second:  57, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 5.897772, mae: 10.233290, mean_q: 19.477917, mean_eps: 5.000000\n",
            " 3481/8000: episode: 159, duration: 0.462s, episode steps:  26, steps per second:  56, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.720850, mae: 10.166289, mean_q: 19.591825, mean_eps: 5.000000\n",
            " 3504/8000: episode: 160, duration: 0.442s, episode steps:  23, steps per second:  52, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.304 [0.000, 1.000],  loss: 5.064553, mae: 10.315490, mean_q: 19.723585, mean_eps: 5.000000\n",
            " 3518/8000: episode: 161, duration: 0.257s, episode steps:  14, steps per second:  54, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 5.195366, mae: 10.330550, mean_q: 19.651822, mean_eps: 5.000000\n",
            " 3532/8000: episode: 162, duration: 0.326s, episode steps:  14, steps per second:  43, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 2.864759, mae: 10.171849, mean_q: 19.634064, mean_eps: 5.000000\n",
            " 3563/8000: episode: 163, duration: 0.572s, episode steps:  31, steps per second:  54, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.613 [0.000, 1.000],  loss: 3.939491, mae: 10.406364, mean_q: 20.057849, mean_eps: 5.000000\n",
            " 3583/8000: episode: 164, duration: 0.356s, episode steps:  20, steps per second:  56, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 3.772013, mae: 10.394296, mean_q: 20.060056, mean_eps: 5.000000\n",
            " 3613/8000: episode: 165, duration: 0.479s, episode steps:  30, steps per second:  63, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.054136, mae: 10.486063, mean_q: 20.264576, mean_eps: 5.000000\n",
            " 3643/8000: episode: 166, duration: 0.488s, episode steps:  30, steps per second:  62, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 3.954759, mae: 10.519982, mean_q: 20.349391, mean_eps: 5.000000\n",
            " 3671/8000: episode: 167, duration: 0.543s, episode steps:  28, steps per second:  52, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.584097, mae: 10.570546, mean_q: 20.411599, mean_eps: 5.000000\n",
            " 3688/8000: episode: 168, duration: 0.331s, episode steps:  17, steps per second:  51, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 2.958736, mae: 10.559836, mean_q: 20.551052, mean_eps: 5.000000\n",
            " 3707/8000: episode: 169, duration: 0.335s, episode steps:  19, steps per second:  57, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 4.019532, mae: 10.761213, mean_q: 20.854912, mean_eps: 5.000000\n",
            " 3720/8000: episode: 170, duration: 0.225s, episode steps:  13, steps per second:  58, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 3.317077, mae: 10.530795, mean_q: 20.603130, mean_eps: 5.000000\n",
            " 3742/8000: episode: 171, duration: 0.392s, episode steps:  22, steps per second:  56, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 5.117191, mae: 10.740736, mean_q: 20.762695, mean_eps: 5.000000\n",
            " 3763/8000: episode: 172, duration: 0.360s, episode steps:  21, steps per second:  58, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.381 [0.000, 1.000],  loss: 3.235140, mae: 10.652645, mean_q: 20.800567, mean_eps: 5.000000\n",
            " 3778/8000: episode: 173, duration: 0.268s, episode steps:  15, steps per second:  56, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 3.790775, mae: 10.822628, mean_q: 21.002096, mean_eps: 5.000000\n",
            " 3805/8000: episode: 174, duration: 0.439s, episode steps:  27, steps per second:  62, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 4.018011, mae: 10.834338, mean_q: 21.012106, mean_eps: 5.000000\n",
            " 3824/8000: episode: 175, duration: 0.338s, episode steps:  19, steps per second:  56, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.737 [0.000, 1.000],  loss: 5.088496, mae: 11.001077, mean_q: 21.244835, mean_eps: 5.000000\n",
            " 3840/8000: episode: 176, duration: 0.271s, episode steps:  16, steps per second:  59, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 3.812745, mae: 10.977069, mean_q: 21.322093, mean_eps: 5.000000\n",
            " 3854/8000: episode: 177, duration: 0.246s, episode steps:  14, steps per second:  57, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 3.454797, mae: 10.914712, mean_q: 21.329018, mean_eps: 5.000000\n",
            " 3873/8000: episode: 178, duration: 0.345s, episode steps:  19, steps per second:  55, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 3.953061, mae: 11.019152, mean_q: 21.512781, mean_eps: 5.000000\n",
            " 3900/8000: episode: 179, duration: 0.736s, episode steps:  27, steps per second:  37, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 4.711607, mae: 11.092473, mean_q: 21.488843, mean_eps: 5.000000\n",
            " 3921/8000: episode: 180, duration: 0.507s, episode steps:  21, steps per second:  41, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 4.058120, mae: 11.124828, mean_q: 21.638781, mean_eps: 5.000000\n",
            " 3934/8000: episode: 181, duration: 0.318s, episode steps:  13, steps per second:  41, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 6.772492, mae: 11.325540, mean_q: 21.721435, mean_eps: 5.000000\n",
            " 3948/8000: episode: 182, duration: 0.399s, episode steps:  14, steps per second:  35, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 3.582527, mae: 11.055366, mean_q: 21.481285, mean_eps: 5.000000\n",
            " 3968/8000: episode: 183, duration: 0.483s, episode steps:  20, steps per second:  41, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.650 [0.000, 1.000],  loss: 4.554388, mae: 11.242440, mean_q: 21.789164, mean_eps: 5.000000\n",
            " 3980/8000: episode: 184, duration: 0.245s, episode steps:  12, steps per second:  49, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 3.815008, mae: 11.321467, mean_q: 22.029133, mean_eps: 5.000000\n",
            " 4009/8000: episode: 185, duration: 0.506s, episode steps:  29, steps per second:  57, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 4.393494, mae: 11.291343, mean_q: 21.900224, mean_eps: 5.000000\n",
            " 4024/8000: episode: 186, duration: 0.256s, episode steps:  15, steps per second:  58, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 4.077301, mae: 11.306133, mean_q: 21.962290, mean_eps: 5.000000\n",
            " 4066/8000: episode: 187, duration: 0.682s, episode steps:  42, steps per second:  62, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 4.970295, mae: 11.431033, mean_q: 22.157974, mean_eps: 5.000000\n",
            " 4079/8000: episode: 188, duration: 0.338s, episode steps:  13, steps per second:  38, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 6.805670, mae: 11.591733, mean_q: 22.346366, mean_eps: 5.000000\n",
            " 4097/8000: episode: 189, duration: 0.490s, episode steps:  18, steps per second:  37, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.683538, mae: 11.424592, mean_q: 22.149267, mean_eps: 5.000000\n",
            " 4113/8000: episode: 190, duration: 0.377s, episode steps:  16, steps per second:  42, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 4.662961, mae: 11.630754, mean_q: 22.523688, mean_eps: 5.000000\n",
            " 4130/8000: episode: 191, duration: 0.281s, episode steps:  17, steps per second:  60, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 3.798187, mae: 11.479938, mean_q: 22.460996, mean_eps: 5.000000\n",
            " 4142/8000: episode: 192, duration: 0.216s, episode steps:  12, steps per second:  56, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 4.147102, mae: 11.665827, mean_q: 22.641302, mean_eps: 5.000000\n",
            " 4157/8000: episode: 193, duration: 0.275s, episode steps:  15, steps per second:  55, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 3.603165, mae: 11.555565, mean_q: 22.525385, mean_eps: 5.000000\n",
            " 4171/8000: episode: 194, duration: 0.268s, episode steps:  14, steps per second:  52, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 4.070335, mae: 11.541137, mean_q: 22.579420, mean_eps: 5.000000\n",
            " 4212/8000: episode: 195, duration: 0.705s, episode steps:  41, steps per second:  58, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 5.014998, mae: 11.749793, mean_q: 22.775861, mean_eps: 5.000000\n",
            " 4236/8000: episode: 196, duration: 0.426s, episode steps:  24, steps per second:  56, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 5.011194, mae: 11.845631, mean_q: 23.033815, mean_eps: 5.000000\n",
            " 4279/8000: episode: 197, duration: 0.782s, episode steps:  43, steps per second:  55, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 3.931422, mae: 11.831525, mean_q: 23.150496, mean_eps: 5.000000\n",
            " 4307/8000: episode: 198, duration: 0.517s, episode steps:  28, steps per second:  54, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.607 [0.000, 1.000],  loss: 5.175964, mae: 12.062350, mean_q: 23.484584, mean_eps: 5.000000\n",
            " 4327/8000: episode: 199, duration: 0.396s, episode steps:  20, steps per second:  50, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 5.773476, mae: 11.958292, mean_q: 23.170260, mean_eps: 5.000000\n",
            " 4344/8000: episode: 200, duration: 0.315s, episode steps:  17, steps per second:  54, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 4.397042, mae: 12.082699, mean_q: 23.488079, mean_eps: 5.000000\n",
            " 4356/8000: episode: 201, duration: 0.217s, episode steps:  12, steps per second:  55, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 8.743027, mae: 12.256289, mean_q: 23.434570, mean_eps: 5.000000\n",
            " 4390/8000: episode: 202, duration: 0.640s, episode steps:  34, steps per second:  53, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 6.313410, mae: 12.139757, mean_q: 23.318061, mean_eps: 5.000000\n",
            " 4403/8000: episode: 203, duration: 0.247s, episode steps:  13, steps per second:  53, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 4.353747, mae: 12.013503, mean_q: 23.316117, mean_eps: 5.000000\n",
            " 4417/8000: episode: 204, duration: 0.254s, episode steps:  14, steps per second:  55, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 5.215209, mae: 12.218157, mean_q: 23.637552, mean_eps: 5.000000\n",
            " 4446/8000: episode: 205, duration: 0.588s, episode steps:  29, steps per second:  49, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 6.003728, mae: 12.327387, mean_q: 23.745264, mean_eps: 5.000000\n",
            " 4476/8000: episode: 206, duration: 0.540s, episode steps:  30, steps per second:  56, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.433 [0.000, 1.000],  loss: 5.751167, mae: 12.282311, mean_q: 23.737669, mean_eps: 5.000000\n",
            " 4495/8000: episode: 207, duration: 0.326s, episode steps:  19, steps per second:  58, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 4.958365, mae: 12.387654, mean_q: 24.003301, mean_eps: 5.000000\n",
            " 4515/8000: episode: 208, duration: 0.320s, episode steps:  20, steps per second:  62, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 6.107390, mae: 12.268693, mean_q: 23.643000, mean_eps: 5.000000\n",
            " 4529/8000: episode: 209, duration: 0.248s, episode steps:  14, steps per second:  56, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 6.491128, mae: 12.426984, mean_q: 23.867979, mean_eps: 5.000000\n",
            " 4543/8000: episode: 210, duration: 0.236s, episode steps:  14, steps per second:  59, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 4.384733, mae: 12.451827, mean_q: 24.128322, mean_eps: 5.000000\n",
            " 4554/8000: episode: 211, duration: 0.216s, episode steps:  11, steps per second:  51, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 5.563244, mae: 12.550378, mean_q: 24.266408, mean_eps: 5.000000\n",
            " 4564/8000: episode: 212, duration: 0.169s, episode steps:  10, steps per second:  59, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 7.266378, mae: 12.554836, mean_q: 24.022115, mean_eps: 5.000000\n",
            " 4584/8000: episode: 213, duration: 0.344s, episode steps:  20, steps per second:  58, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 5.527871, mae: 12.395676, mean_q: 23.952287, mean_eps: 5.000000\n",
            " 4598/8000: episode: 214, duration: 0.261s, episode steps:  14, steps per second:  54, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 7.779371, mae: 12.525840, mean_q: 23.983387, mean_eps: 5.000000\n",
            " 4619/8000: episode: 215, duration: 0.383s, episode steps:  21, steps per second:  55, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.381 [0.000, 1.000],  loss: 6.578151, mae: 12.409965, mean_q: 23.855560, mean_eps: 5.000000\n",
            " 4634/8000: episode: 216, duration: 0.251s, episode steps:  15, steps per second:  60, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 6.660952, mae: 12.515983, mean_q: 24.019085, mean_eps: 5.000000\n",
            " 4647/8000: episode: 217, duration: 0.228s, episode steps:  13, steps per second:  57, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 5.286600, mae: 12.477058, mean_q: 24.071737, mean_eps: 5.000000\n",
            " 4664/8000: episode: 218, duration: 0.296s, episode steps:  17, steps per second:  57, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 6.936319, mae: 12.572737, mean_q: 24.074641, mean_eps: 5.000000\n",
            " 4687/8000: episode: 219, duration: 0.395s, episode steps:  23, steps per second:  58, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  loss: 5.603685, mae: 12.462978, mean_q: 23.997370, mean_eps: 5.000000\n",
            " 4714/8000: episode: 220, duration: 0.482s, episode steps:  27, steps per second:  56, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 6.357200, mae: 12.610583, mean_q: 24.302323, mean_eps: 5.000000\n",
            " 4733/8000: episode: 221, duration: 0.360s, episode steps:  19, steps per second:  53, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 5.185863, mae: 12.515658, mean_q: 24.226283, mean_eps: 5.000000\n",
            " 4748/8000: episode: 222, duration: 0.431s, episode steps:  15, steps per second:  35, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 5.676799, mae: 12.577324, mean_q: 24.262995, mean_eps: 5.000000\n",
            " 4759/8000: episode: 223, duration: 0.351s, episode steps:  11, steps per second:  31, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 7.388989, mae: 12.603763, mean_q: 24.040987, mean_eps: 5.000000\n",
            " 4779/8000: episode: 224, duration: 0.397s, episode steps:  20, steps per second:  50, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 5.080138, mae: 12.614448, mean_q: 24.369298, mean_eps: 5.000000\n",
            " 4796/8000: episode: 225, duration: 0.293s, episode steps:  17, steps per second:  58, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 5.368632, mae: 12.660465, mean_q: 24.499202, mean_eps: 5.000000\n",
            " 4808/8000: episode: 226, duration: 0.223s, episode steps:  12, steps per second:  54, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 5.214819, mae: 12.828462, mean_q: 24.764878, mean_eps: 5.000000\n",
            " 4826/8000: episode: 227, duration: 0.304s, episode steps:  18, steps per second:  59, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 4.828226, mae: 12.712531, mean_q: 24.617753, mean_eps: 5.000000\n",
            " 4839/8000: episode: 228, duration: 0.225s, episode steps:  13, steps per second:  58, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 5.051226, mae: 12.623274, mean_q: 24.456267, mean_eps: 5.000000\n",
            " 4854/8000: episode: 229, duration: 0.260s, episode steps:  15, steps per second:  58, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 5.798121, mae: 12.667505, mean_q: 24.364290, mean_eps: 5.000000\n",
            " 4864/8000: episode: 230, duration: 0.190s, episode steps:  10, steps per second:  53, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 6.916610, mae: 12.572064, mean_q: 24.258601, mean_eps: 5.000000\n",
            " 4883/8000: episode: 231, duration: 0.346s, episode steps:  19, steps per second:  55, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 5.826385, mae: 12.556843, mean_q: 24.273511, mean_eps: 5.000000\n",
            " 4912/8000: episode: 232, duration: 0.489s, episode steps:  29, steps per second:  59, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 4.782378, mae: 12.827059, mean_q: 24.880047, mean_eps: 5.000000\n",
            " 4928/8000: episode: 233, duration: 0.294s, episode steps:  16, steps per second:  54, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 7.568284, mae: 12.851044, mean_q: 24.586669, mean_eps: 5.000000\n",
            " 4942/8000: episode: 234, duration: 0.246s, episode steps:  14, steps per second:  57, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.214 [0.000, 1.000],  loss: 5.916332, mae: 12.753253, mean_q: 24.497604, mean_eps: 5.000000\n",
            " 4955/8000: episode: 235, duration: 0.272s, episode steps:  13, steps per second:  48, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 4.610560, mae: 12.644567, mean_q: 24.577511, mean_eps: 5.000000\n",
            " 4965/8000: episode: 236, duration: 0.213s, episode steps:  10, steps per second:  47, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 3.389822, mae: 12.863363, mean_q: 25.092892, mean_eps: 5.000000\n",
            " 4977/8000: episode: 237, duration: 0.228s, episode steps:  12, steps per second:  53, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 5.941742, mae: 12.882341, mean_q: 24.856196, mean_eps: 5.000000\n",
            " 4997/8000: episode: 238, duration: 0.429s, episode steps:  20, steps per second:  47, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 5.973218, mae: 12.886004, mean_q: 24.903434, mean_eps: 5.000000\n",
            " 5025/8000: episode: 239, duration: 0.765s, episode steps:  28, steps per second:  37, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.607 [0.000, 1.000],  loss: 5.979917, mae: 12.831037, mean_q: 24.926942, mean_eps: 5.000000\n",
            " 5036/8000: episode: 240, duration: 0.298s, episode steps:  11, steps per second:  37, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 4.549180, mae: 12.986964, mean_q: 25.205337, mean_eps: 5.000000\n",
            " 5056/8000: episode: 241, duration: 0.503s, episode steps:  20, steps per second:  40, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 5.162094, mae: 12.908214, mean_q: 24.989335, mean_eps: 5.000000\n",
            " 5065/8000: episode: 242, duration: 0.268s, episode steps:   9, steps per second:  34, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 5.326081, mae: 12.878465, mean_q: 25.015963, mean_eps: 5.000000\n",
            " 5090/8000: episode: 243, duration: 0.479s, episode steps:  25, steps per second:  52, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 5.339998, mae: 12.932911, mean_q: 25.123873, mean_eps: 5.000000\n",
            " 5103/8000: episode: 244, duration: 0.242s, episode steps:  13, steps per second:  54, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 8.505126, mae: 13.065574, mean_q: 24.926114, mean_eps: 5.000000\n",
            " 5127/8000: episode: 245, duration: 0.413s, episode steps:  24, steps per second:  58, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 5.332727, mae: 13.087631, mean_q: 25.421470, mean_eps: 5.000000\n",
            " 5145/8000: episode: 246, duration: 0.304s, episode steps:  18, steps per second:  59, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.095910, mae: 13.078126, mean_q: 25.482800, mean_eps: 5.000000\n",
            " 5162/8000: episode: 247, duration: 0.279s, episode steps:  17, steps per second:  61, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 6.896677, mae: 12.991374, mean_q: 25.152023, mean_eps: 5.000000\n",
            " 5193/8000: episode: 248, duration: 0.509s, episode steps:  31, steps per second:  61, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.613 [0.000, 1.000],  loss: 4.756165, mae: 13.166610, mean_q: 25.528289, mean_eps: 5.000000\n",
            " 5208/8000: episode: 249, duration: 0.262s, episode steps:  15, steps per second:  57, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 4.951451, mae: 13.114993, mean_q: 25.577103, mean_eps: 5.000000\n",
            " 5221/8000: episode: 250, duration: 0.237s, episode steps:  13, steps per second:  55, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 6.199809, mae: 13.141586, mean_q: 25.573568, mean_eps: 5.000000\n",
            " 5243/8000: episode: 251, duration: 0.384s, episode steps:  22, steps per second:  57, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 5.552604, mae: 13.253262, mean_q: 25.759597, mean_eps: 5.000000\n",
            " 5260/8000: episode: 252, duration: 0.284s, episode steps:  17, steps per second:  60, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 7.028587, mae: 13.279446, mean_q: 25.638704, mean_eps: 5.000000\n",
            " 5273/8000: episode: 253, duration: 0.208s, episode steps:  13, steps per second:  63, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 5.170040, mae: 13.139889, mean_q: 25.677817, mean_eps: 5.000000\n",
            " 5299/8000: episode: 254, duration: 0.430s, episode steps:  26, steps per second:  60, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.423 [0.000, 1.000],  loss: 5.935542, mae: 13.199252, mean_q: 25.645803, mean_eps: 5.000000\n",
            " 5322/8000: episode: 255, duration: 0.383s, episode steps:  23, steps per second:  60, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 7.407724, mae: 13.202291, mean_q: 25.395695, mean_eps: 5.000000\n",
            " 5333/8000: episode: 256, duration: 0.199s, episode steps:  11, steps per second:  55, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 7.594568, mae: 13.209259, mean_q: 25.307174, mean_eps: 5.000000\n",
            " 5364/8000: episode: 257, duration: 0.539s, episode steps:  31, steps per second:  57, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 8.361289, mae: 13.252064, mean_q: 25.417754, mean_eps: 5.000000\n",
            " 5403/8000: episode: 258, duration: 0.654s, episode steps:  39, steps per second:  60, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 6.583176, mae: 13.337287, mean_q: 25.777973, mean_eps: 5.000000\n",
            " 5425/8000: episode: 259, duration: 0.376s, episode steps:  22, steps per second:  58, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 5.864557, mae: 13.523729, mean_q: 26.197683, mean_eps: 5.000000\n",
            " 5450/8000: episode: 260, duration: 0.417s, episode steps:  25, steps per second:  60, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 6.409070, mae: 13.274227, mean_q: 25.731263, mean_eps: 5.000000\n",
            " 5460/8000: episode: 261, duration: 0.168s, episode steps:  10, steps per second:  60, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 5.851863, mae: 13.564868, mean_q: 26.303101, mean_eps: 5.000000\n",
            " 5482/8000: episode: 262, duration: 0.350s, episode steps:  22, steps per second:  63, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 6.369289, mae: 13.496206, mean_q: 26.175441, mean_eps: 5.000000\n",
            " 5505/8000: episode: 263, duration: 0.382s, episode steps:  23, steps per second:  60, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  loss: 6.407652, mae: 13.535513, mean_q: 26.194649, mean_eps: 5.000000\n",
            " 5528/8000: episode: 264, duration: 0.378s, episode steps:  23, steps per second:  61, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 3.997355, mae: 13.473670, mean_q: 26.333419, mean_eps: 5.000000\n",
            " 5546/8000: episode: 265, duration: 0.314s, episode steps:  18, steps per second:  57, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 9.001180, mae: 13.639187, mean_q: 26.163481, mean_eps: 5.000000\n",
            " 5561/8000: episode: 266, duration: 0.281s, episode steps:  15, steps per second:  53, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 4.245114, mae: 13.356696, mean_q: 25.997816, mean_eps: 5.000000\n",
            " 5573/8000: episode: 267, duration: 0.223s, episode steps:  12, steps per second:  54, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 4.922049, mae: 13.546738, mean_q: 26.327209, mean_eps: 5.000000\n",
            " 5603/8000: episode: 268, duration: 0.487s, episode steps:  30, steps per second:  62, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.713126, mae: 13.593111, mean_q: 26.337450, mean_eps: 5.000000\n",
            " 5612/8000: episode: 269, duration: 0.200s, episode steps:   9, steps per second:  45, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 7.439772, mae: 13.752267, mean_q: 26.611470, mean_eps: 5.000000\n",
            " 5622/8000: episode: 270, duration: 0.243s, episode steps:  10, steps per second:  41, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 10.638075, mae: 13.657586, mean_q: 25.999537, mean_eps: 5.000000\n",
            " 5636/8000: episode: 271, duration: 0.324s, episode steps:  14, steps per second:  43, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 5.867232, mae: 13.447857, mean_q: 26.000444, mean_eps: 5.000000\n",
            " 5645/8000: episode: 272, duration: 0.229s, episode steps:   9, steps per second:  39, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 7.717702, mae: 13.468217, mean_q: 26.119102, mean_eps: 5.000000\n",
            " 5655/8000: episode: 273, duration: 0.233s, episode steps:  10, steps per second:  43, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 3.710066, mae: 13.520900, mean_q: 26.581974, mean_eps: 5.000000\n",
            " 5674/8000: episode: 274, duration: 0.330s, episode steps:  19, steps per second:  58, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.684 [0.000, 1.000],  loss: 8.463592, mae: 13.655152, mean_q: 26.399952, mean_eps: 5.000000\n",
            " 5687/8000: episode: 275, duration: 0.225s, episode steps:  13, steps per second:  58, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 6.448976, mae: 13.608240, mean_q: 26.482925, mean_eps: 5.000000\n",
            " 5720/8000: episode: 276, duration: 0.547s, episode steps:  33, steps per second:  60, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.576 [0.000, 1.000],  loss: 6.795901, mae: 13.650326, mean_q: 26.430299, mean_eps: 5.000000\n",
            " 5740/8000: episode: 277, duration: 0.316s, episode steps:  20, steps per second:  63, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 6.556547, mae: 13.721330, mean_q: 26.509179, mean_eps: 5.000000\n",
            " 5767/8000: episode: 278, duration: 0.463s, episode steps:  27, steps per second:  58, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.407 [0.000, 1.000],  loss: 7.579116, mae: 13.792065, mean_q: 26.658130, mean_eps: 5.000000\n",
            " 5778/8000: episode: 279, duration: 0.194s, episode steps:  11, steps per second:  57, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 8.975269, mae: 13.920571, mean_q: 26.641555, mean_eps: 5.000000\n",
            " 5791/8000: episode: 280, duration: 0.220s, episode steps:  13, steps per second:  59, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 3.765989, mae: 13.410155, mean_q: 26.208110, mean_eps: 5.000000\n",
            " 5803/8000: episode: 281, duration: 0.200s, episode steps:  12, steps per second:  60, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 6.210995, mae: 13.757012, mean_q: 26.616220, mean_eps: 5.000000\n",
            " 5819/8000: episode: 282, duration: 0.278s, episode steps:  16, steps per second:  57, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 7.988510, mae: 13.814101, mean_q: 26.787073, mean_eps: 5.000000\n",
            " 5838/8000: episode: 283, duration: 0.315s, episode steps:  19, steps per second:  60, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 5.533892, mae: 13.694765, mean_q: 26.675491, mean_eps: 5.000000\n",
            " 5862/8000: episode: 284, duration: 0.405s, episode steps:  24, steps per second:  59, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 5.991081, mae: 13.849418, mean_q: 27.007430, mean_eps: 5.000000\n",
            " 5875/8000: episode: 285, duration: 0.245s, episode steps:  13, steps per second:  53, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 6.240185, mae: 14.004142, mean_q: 27.203857, mean_eps: 5.000000\n",
            " 5893/8000: episode: 286, duration: 0.324s, episode steps:  18, steps per second:  56, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.493361, mae: 13.926567, mean_q: 27.190206, mean_eps: 5.000000\n",
            " 5949/8000: episode: 287, duration: 0.985s, episode steps:  56, steps per second:  57, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 4.751375, mae: 13.978840, mean_q: 27.361686, mean_eps: 5.000000\n",
            " 5961/8000: episode: 288, duration: 0.217s, episode steps:  12, steps per second:  55, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 7.380687, mae: 14.166371, mean_q: 27.480817, mean_eps: 5.000000\n",
            " 5978/8000: episode: 289, duration: 0.280s, episode steps:  17, steps per second:  61, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 7.085904, mae: 14.026558, mean_q: 27.242487, mean_eps: 5.000000\n",
            " 6039/8000: episode: 290, duration: 1.089s, episode steps:  61, steps per second:  56, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 7.083561, mae: 14.050525, mean_q: 27.280781, mean_eps: 5.000000\n",
            " 6056/8000: episode: 291, duration: 0.318s, episode steps:  17, steps per second:  53, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 7.081106, mae: 14.171376, mean_q: 27.504663, mean_eps: 5.000000\n",
            " 6073/8000: episode: 292, duration: 0.447s, episode steps:  17, steps per second:  38, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 6.282103, mae: 14.129886, mean_q: 27.523457, mean_eps: 5.000000\n",
            " 6099/8000: episode: 293, duration: 0.673s, episode steps:  26, steps per second:  39, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.577 [0.000, 1.000],  loss: 5.333528, mae: 14.093890, mean_q: 27.514144, mean_eps: 5.000000\n",
            " 6126/8000: episode: 294, duration: 0.453s, episode steps:  27, steps per second:  60, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 5.989607, mae: 14.354420, mean_q: 28.010394, mean_eps: 5.000000\n",
            " 6147/8000: episode: 295, duration: 0.363s, episode steps:  21, steps per second:  58, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 8.250546, mae: 14.262853, mean_q: 27.660572, mean_eps: 5.000000\n",
            " 6166/8000: episode: 296, duration: 0.364s, episode steps:  19, steps per second:  52, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 6.769220, mae: 14.313219, mean_q: 27.823306, mean_eps: 5.000000\n",
            " 6209/8000: episode: 297, duration: 0.956s, episode steps:  43, steps per second:  45, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 5.326993, mae: 14.271128, mean_q: 27.879519, mean_eps: 5.000000\n",
            " 6231/8000: episode: 298, duration: 0.387s, episode steps:  22, steps per second:  57, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.104290, mae: 14.524840, mean_q: 28.265550, mean_eps: 5.000000\n",
            " 6256/8000: episode: 299, duration: 0.441s, episode steps:  25, steps per second:  57, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 8.080189, mae: 14.459375, mean_q: 28.051032, mean_eps: 5.000000\n",
            " 6278/8000: episode: 300, duration: 0.360s, episode steps:  22, steps per second:  61, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 6.908496, mae: 14.370035, mean_q: 27.876995, mean_eps: 5.000000\n",
            " 6299/8000: episode: 301, duration: 0.358s, episode steps:  21, steps per second:  59, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 7.421010, mae: 14.418723, mean_q: 27.985389, mean_eps: 5.000000\n",
            " 6309/8000: episode: 302, duration: 0.190s, episode steps:  10, steps per second:  53, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 7.318054, mae: 14.229832, mean_q: 27.859648, mean_eps: 5.000000\n",
            " 6342/8000: episode: 303, duration: 0.544s, episode steps:  33, steps per second:  61, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 6.431250, mae: 14.513207, mean_q: 28.238297, mean_eps: 5.000000\n",
            " 6364/8000: episode: 304, duration: 0.366s, episode steps:  22, steps per second:  60, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  loss: 7.350135, mae: 14.496938, mean_q: 28.237574, mean_eps: 5.000000\n",
            " 6378/8000: episode: 305, duration: 0.240s, episode steps:  14, steps per second:  58, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.214 [0.000, 1.000],  loss: 5.925879, mae: 14.321572, mean_q: 27.971881, mean_eps: 5.000000\n",
            " 6404/8000: episode: 306, duration: 0.425s, episode steps:  26, steps per second:  61, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.577 [0.000, 1.000],  loss: 5.663067, mae: 14.498877, mean_q: 28.312931, mean_eps: 5.000000\n",
            " 6417/8000: episode: 307, duration: 0.212s, episode steps:  13, steps per second:  61, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 9.001257, mae: 14.956144, mean_q: 28.857449, mean_eps: 5.000000\n",
            " 6436/8000: episode: 308, duration: 0.332s, episode steps:  19, steps per second:  57, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 7.533906, mae: 14.417847, mean_q: 28.015743, mean_eps: 5.000000\n",
            " 6453/8000: episode: 309, duration: 0.454s, episode steps:  17, steps per second:  37, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 7.293740, mae: 14.388127, mean_q: 27.947884, mean_eps: 5.000000\n",
            " 6463/8000: episode: 310, duration: 0.292s, episode steps:  10, steps per second:  34, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 7.019331, mae: 14.548284, mean_q: 28.280469, mean_eps: 5.000000\n",
            " 6481/8000: episode: 311, duration: 0.523s, episode steps:  18, steps per second:  34, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.141098, mae: 14.634056, mean_q: 28.521878, mean_eps: 5.000000\n",
            " 6543/8000: episode: 312, duration: 1.578s, episode steps:  62, steps per second:  39, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 7.392825, mae: 14.656023, mean_q: 28.428908, mean_eps: 5.000000\n",
            " 6569/8000: episode: 313, duration: 0.612s, episode steps:  26, steps per second:  43, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 6.385664, mae: 14.647174, mean_q: 28.596163, mean_eps: 5.000000\n",
            " 6579/8000: episode: 314, duration: 0.244s, episode steps:  10, steps per second:  41, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 7.476018, mae: 14.669587, mean_q: 28.519918, mean_eps: 5.000000\n",
            " 6594/8000: episode: 315, duration: 0.356s, episode steps:  15, steps per second:  42, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 6.895642, mae: 14.603867, mean_q: 28.513442, mean_eps: 5.000000\n",
            " 6609/8000: episode: 316, duration: 0.290s, episode steps:  15, steps per second:  52, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 7.085345, mae: 14.570421, mean_q: 28.445932, mean_eps: 5.000000\n",
            " 6624/8000: episode: 317, duration: 0.256s, episode steps:  15, steps per second:  59, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 8.116870, mae: 14.776205, mean_q: 28.690954, mean_eps: 5.000000\n",
            " 6649/8000: episode: 318, duration: 0.474s, episode steps:  25, steps per second:  53, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 6.403336, mae: 14.730438, mean_q: 28.717070, mean_eps: 5.000000\n",
            " 6685/8000: episode: 319, duration: 0.670s, episode steps:  36, steps per second:  54, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 7.638499, mae: 14.828851, mean_q: 28.875838, mean_eps: 5.000000\n",
            " 6696/8000: episode: 320, duration: 0.236s, episode steps:  11, steps per second:  47, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 6.586570, mae: 14.789278, mean_q: 29.019141, mean_eps: 5.000000\n",
            " 6718/8000: episode: 321, duration: 0.369s, episode steps:  22, steps per second:  60, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 5.713190, mae: 15.103746, mean_q: 29.489721, mean_eps: 5.000000\n",
            " 6733/8000: episode: 322, duration: 0.272s, episode steps:  15, steps per second:  55, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 6.538568, mae: 14.795217, mean_q: 28.838843, mean_eps: 5.000000\n",
            " 6746/8000: episode: 323, duration: 0.243s, episode steps:  13, steps per second:  54, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 5.105000, mae: 14.971321, mean_q: 29.411799, mean_eps: 5.000000\n",
            " 6768/8000: episode: 324, duration: 0.360s, episode steps:  22, steps per second:  61, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 7.657051, mae: 15.078962, mean_q: 29.401168, mean_eps: 5.000000\n",
            " 6787/8000: episode: 325, duration: 0.309s, episode steps:  19, steps per second:  62, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 6.936224, mae: 15.148961, mean_q: 29.578038, mean_eps: 5.000000\n",
            " 6801/8000: episode: 326, duration: 0.259s, episode steps:  14, steps per second:  54, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 8.007751, mae: 15.071506, mean_q: 29.328820, mean_eps: 5.000000\n",
            " 6828/8000: episode: 327, duration: 0.488s, episode steps:  27, steps per second:  55, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 8.965845, mae: 15.029536, mean_q: 29.047977, mean_eps: 5.000000\n",
            " 6840/8000: episode: 328, duration: 0.214s, episode steps:  12, steps per second:  56, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 4.788219, mae: 14.762952, mean_q: 28.943046, mean_eps: 5.000000\n",
            " 6858/8000: episode: 329, duration: 0.307s, episode steps:  18, steps per second:  59, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 6.605589, mae: 14.858167, mean_q: 29.158277, mean_eps: 5.000000\n",
            " 6869/8000: episode: 330, duration: 0.186s, episode steps:  11, steps per second:  59, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 3.790120, mae: 14.985897, mean_q: 29.600792, mean_eps: 5.000000\n",
            " 6887/8000: episode: 331, duration: 0.280s, episode steps:  18, steps per second:  64, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 4.942073, mae: 15.308967, mean_q: 30.075231, mean_eps: 5.000000\n",
            " 6900/8000: episode: 332, duration: 0.223s, episode steps:  13, steps per second:  58, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 6.269506, mae: 15.195601, mean_q: 29.717308, mean_eps: 5.000000\n",
            " 6913/8000: episode: 333, duration: 0.221s, episode steps:  13, steps per second:  59, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 6.132855, mae: 15.163141, mean_q: 29.679886, mean_eps: 5.000000\n",
            " 6949/8000: episode: 334, duration: 0.589s, episode steps:  36, steps per second:  61, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 9.215403, mae: 15.271822, mean_q: 29.545496, mean_eps: 5.000000\n",
            " 6980/8000: episode: 335, duration: 0.496s, episode steps:  31, steps per second:  62, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 7.692861, mae: 15.144242, mean_q: 29.401845, mean_eps: 5.000000\n",
            " 6999/8000: episode: 336, duration: 0.344s, episode steps:  19, steps per second:  55, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 7.239911, mae: 15.165770, mean_q: 29.578823, mean_eps: 5.000000\n",
            " 7012/8000: episode: 337, duration: 0.213s, episode steps:  13, steps per second:  61, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 12.372498, mae: 15.019959, mean_q: 28.892224, mean_eps: 5.000000\n",
            " 7054/8000: episode: 338, duration: 0.650s, episode steps:  42, steps per second:  65, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 6.821392, mae: 15.243583, mean_q: 29.751269, mean_eps: 5.000000\n",
            " 7075/8000: episode: 339, duration: 0.371s, episode steps:  21, steps per second:  57, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 7.210957, mae: 15.323355, mean_q: 29.902619, mean_eps: 5.000000\n",
            " 7088/8000: episode: 340, duration: 0.217s, episode steps:  13, steps per second:  60, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 6.498340, mae: 15.318737, mean_q: 29.881623, mean_eps: 5.000000\n",
            " 7138/8000: episode: 341, duration: 0.808s, episode steps:  50, steps per second:  62, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 8.681058, mae: 15.377299, mean_q: 29.912072, mean_eps: 5.000000\n",
            " 7175/8000: episode: 342, duration: 0.627s, episode steps:  37, steps per second:  59, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 8.486581, mae: 15.421633, mean_q: 29.990580, mean_eps: 5.000000\n",
            " 7216/8000: episode: 343, duration: 0.650s, episode steps:  41, steps per second:  63, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 6.700168, mae: 15.471102, mean_q: 30.284564, mean_eps: 5.000000\n",
            " 7227/8000: episode: 344, duration: 0.175s, episode steps:  11, steps per second:  63, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 6.082455, mae: 15.526584, mean_q: 30.368234, mean_eps: 5.000000\n",
            " 7249/8000: episode: 345, duration: 0.360s, episode steps:  22, steps per second:  61, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 4.353006, mae: 15.649113, mean_q: 30.824513, mean_eps: 5.000000\n",
            " 7268/8000: episode: 346, duration: 0.381s, episode steps:  19, steps per second:  50, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 10.542385, mae: 15.780323, mean_q: 30.636207, mean_eps: 5.000000\n",
            " 7284/8000: episode: 347, duration: 0.462s, episode steps:  16, steps per second:  35, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 8.681236, mae: 15.465159, mean_q: 30.073658, mean_eps: 5.000000\n",
            " 7294/8000: episode: 348, duration: 0.278s, episode steps:  10, steps per second:  36, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 7.838663, mae: 15.462112, mean_q: 29.940011, mean_eps: 5.000000\n",
            " 7344/8000: episode: 349, duration: 0.845s, episode steps:  50, steps per second:  59, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 7.478472, mae: 15.523551, mean_q: 30.318891, mean_eps: 5.000000\n",
            " 7374/8000: episode: 350, duration: 0.469s, episode steps:  30, steps per second:  64, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.433 [0.000, 1.000],  loss: 8.705474, mae: 15.653051, mean_q: 30.547507, mean_eps: 5.000000\n",
            " 7392/8000: episode: 351, duration: 0.306s, episode steps:  18, steps per second:  59, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 10.192016, mae: 15.583199, mean_q: 30.390826, mean_eps: 5.000000\n",
            " 7413/8000: episode: 352, duration: 0.344s, episode steps:  21, steps per second:  61, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 8.081031, mae: 15.720271, mean_q: 30.680099, mean_eps: 5.000000\n",
            " 7438/8000: episode: 353, duration: 0.539s, episode steps:  25, steps per second:  46, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 8.429995, mae: 15.560157, mean_q: 30.384929, mean_eps: 5.000000\n",
            " 7454/8000: episode: 354, duration: 0.391s, episode steps:  16, steps per second:  41, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 9.029777, mae: 15.537142, mean_q: 30.209882, mean_eps: 5.000000\n",
            " 7488/8000: episode: 355, duration: 0.654s, episode steps:  34, steps per second:  52, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 6.213537, mae: 15.578075, mean_q: 30.673183, mean_eps: 5.000000\n",
            " 7516/8000: episode: 356, duration: 0.469s, episode steps:  28, steps per second:  60, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 9.631901, mae: 15.794994, mean_q: 30.785061, mean_eps: 5.000000\n",
            " 7550/8000: episode: 357, duration: 0.648s, episode steps:  34, steps per second:  53, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 7.126038, mae: 15.720814, mean_q: 30.744504, mean_eps: 5.000000\n",
            " 7559/8000: episode: 358, duration: 0.177s, episode steps:   9, steps per second:  51, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 7.682358, mae: 15.868955, mean_q: 30.953172, mean_eps: 5.000000\n",
            " 7568/8000: episode: 359, duration: 0.173s, episode steps:   9, steps per second:  52, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 5.598572, mae: 15.863203, mean_q: 31.355080, mean_eps: 5.000000\n",
            " 7587/8000: episode: 360, duration: 0.298s, episode steps:  19, steps per second:  64, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 7.227607, mae: 15.744040, mean_q: 30.761000, mean_eps: 5.000000\n",
            " 7611/8000: episode: 361, duration: 0.470s, episode steps:  24, steps per second:  51, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 9.525342, mae: 15.769382, mean_q: 30.694048, mean_eps: 5.000000\n",
            " 7630/8000: episode: 362, duration: 0.355s, episode steps:  19, steps per second:  54, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 6.976626, mae: 15.671233, mean_q: 30.723005, mean_eps: 5.000000\n",
            " 7643/8000: episode: 363, duration: 0.268s, episode steps:  13, steps per second:  48, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 10.325013, mae: 15.789530, mean_q: 30.760005, mean_eps: 5.000000\n",
            " 7681/8000: episode: 364, duration: 0.731s, episode steps:  38, steps per second:  52, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  loss: 6.900573, mae: 15.743323, mean_q: 30.859742, mean_eps: 5.000000\n",
            " 7695/8000: episode: 365, duration: 0.255s, episode steps:  14, steps per second:  55, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 9.259751, mae: 16.067760, mean_q: 31.207104, mean_eps: 5.000000\n",
            " 7721/8000: episode: 366, duration: 0.458s, episode steps:  26, steps per second:  57, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.947214, mae: 15.994269, mean_q: 31.315659, mean_eps: 5.000000\n",
            " 7742/8000: episode: 367, duration: 0.370s, episode steps:  21, steps per second:  57, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 11.119671, mae: 16.125774, mean_q: 31.256094, mean_eps: 5.000000\n",
            " 7757/8000: episode: 368, duration: 0.412s, episode steps:  15, steps per second:  36, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 9.596292, mae: 15.776186, mean_q: 30.612498, mean_eps: 5.000000\n",
            " 7767/8000: episode: 369, duration: 0.330s, episode steps:  10, steps per second:  30, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 9.623648, mae: 16.049430, mean_q: 31.041741, mean_eps: 5.000000\n",
            " 7802/8000: episode: 370, duration: 0.939s, episode steps:  35, steps per second:  37, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 6.321682, mae: 16.018836, mean_q: 31.325036, mean_eps: 5.000000\n",
            " 7819/8000: episode: 371, duration: 0.442s, episode steps:  17, steps per second:  38, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: 7.270566, mae: 16.303474, mean_q: 31.837448, mean_eps: 5.000000\n",
            " 7859/8000: episode: 372, duration: 0.692s, episode steps:  40, steps per second:  58, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 7.507668, mae: 16.038129, mean_q: 31.313329, mean_eps: 5.000000\n",
            " 7874/8000: episode: 373, duration: 0.282s, episode steps:  15, steps per second:  53, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 7.984047, mae: 16.132806, mean_q: 31.457886, mean_eps: 5.000000\n",
            " 7893/8000: episode: 374, duration: 0.342s, episode steps:  19, steps per second:  55, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 5.360261, mae: 15.954558, mean_q: 31.396539, mean_eps: 5.000000\n",
            " 7907/8000: episode: 375, duration: 0.263s, episode steps:  14, steps per second:  53, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 5.605363, mae: 16.115763, mean_q: 31.590185, mean_eps: 5.000000\n",
            " 7957/8000: episode: 376, duration: 0.873s, episode steps:  50, steps per second:  57, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 8.939943, mae: 16.250688, mean_q: 31.562950, mean_eps: 5.000000\n",
            " 7976/8000: episode: 377, duration: 0.338s, episode steps:  19, steps per second:  56, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 6.770095, mae: 15.925462, mean_q: 31.105033, mean_eps: 5.000000\n",
            "done, took 167.377 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 95.000, steps: 95\n",
            "Episode 2: reward: 78.000, steps: 78\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 79.000, steps: 79\n",
            "Episode 5: reward: 165.000, steps: 165\n",
            "Episode 6: reward: 200.000, steps: 200\n",
            "Episode 7: reward: 169.000, steps: 169\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 200.000, steps: 200\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 65.000, steps: 65\n",
            "Episode 13: reward: 168.000, steps: 168\n",
            "Episode 14: reward: 90.000, steps: 90\n",
            "Episode 15: reward: 200.000, steps: 200\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 86.000, steps: 86\n",
            "Episode 18: reward: 200.000, steps: 200\n",
            "Episode 19: reward: 200.000, steps: 200\n",
            "Episode 20: reward: 200.000, steps: 200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb16f89e450>"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    }
  ]
}