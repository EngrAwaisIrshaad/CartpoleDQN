{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPoleDQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Megacity1/CartpoleDQN/blob/main/Week%203%20Deep%20RL%202/CartPoleDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Deep Q Network (DQN) for CartPole Using Boltzmann Q Policy\n",
        "This exercise implements a DQN for CartPole using a Boltzmann Q policy for selecting the actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGsC7cJ5jNcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda73cbf-3020-40f4-e45e-7b87053448f1"
      },
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.24.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (13.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.44.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.10.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMIHLgQ3Z-lF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5a8608-456f-4127-ce8a-f551e5f4e4f3"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0AMLzq08ap0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b380e1-6c3d-4bce-8233-a8ac01deaf26"
      },
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "!pip install Adam\n",
        "#from keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Adam in /usr/local/lib/python3.7/dist-packages (0.0.0.dev0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll6bNdUm54WS"
      },
      "source": [
        "Implementation of DQN for CartPole, applying policy BoltzmannQPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCrPKNy40PC"
      },
      "source": [
        "##Implement DQN with BoltzmannGumbelQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efM9jkXr5A3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "202d9826-40f8-4cb9-bcb8-b1483cf3a0be"
      },
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import BoltzmannGumbelQPolicy\n",
        "from rl.policy import LinearAnnealedPolicy\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n",
        "# setup experience replay buffer\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
        "                               attr='eps',            \n",
        "                               value_max=5.,\n",
        "                               value_min=.5, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=20)\n",
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "# add extra layers here\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=20,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy) \n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=8000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_37\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_35 (Flatten)        (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_71 (Dense)            (None, 16)                80        \n",
            "                                                                 \n",
            " dense_72 (Dense)            (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 8000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   17/8000: episode: 1, duration: 1.817s, episode steps:  17, steps per second:   9, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   39/8000: episode: 2, duration: 6.314s, episode steps:  22, steps per second:   3, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.318 [0.000, 1.000],  loss: 0.500788, mae: 0.554375, mean_q: 0.020877, mean_eps: 0.500000\n",
            "   52/8000: episode: 3, duration: 0.161s, episode steps:  13, steps per second:  81, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.404463, mae: 0.525994, mean_q: 0.156828, mean_eps: 0.500000\n",
            "   64/8000: episode: 4, duration: 0.156s, episode steps:  12, steps per second:  77, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.359710, mae: 0.507065, mean_q: 0.293145, mean_eps: 0.500000\n",
            "   80/8000: episode: 5, duration: 0.228s, episode steps:  16, steps per second:  70, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.343485, mae: 0.497616, mean_q: 0.435040, mean_eps: 0.500000\n",
            "   91/8000: episode: 6, duration: 0.136s, episode steps:  11, steps per second:  81, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.284903, mae: 0.454385, mean_q: 0.595698, mean_eps: 0.500000\n",
            "  100/8000: episode: 7, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.267423, mae: 0.463789, mean_q: 0.708922, mean_eps: 0.500000\n",
            "  109/8000: episode: 8, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.240418, mae: 0.472633, mean_q: 0.784573, mean_eps: 0.500000\n",
            "  123/8000: episode: 9, duration: 0.181s, episode steps:  14, steps per second:  77, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.226542, mae: 0.509812, mean_q: 0.909253, mean_eps: 0.500000\n",
            "  149/8000: episode: 10, duration: 0.326s, episode steps:  26, steps per second:  80, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.264612, mae: 0.596084, mean_q: 0.984964, mean_eps: 0.500000\n",
            "  168/8000: episode: 11, duration: 0.248s, episode steps:  19, steps per second:  77, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.316 [0.000, 1.000],  loss: 0.268410, mae: 0.691947, mean_q: 1.143812, mean_eps: 0.500000\n",
            "  186/8000: episode: 12, duration: 0.238s, episode steps:  18, steps per second:  76, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.278 [0.000, 1.000],  loss: 0.257797, mae: 0.765956, mean_q: 1.248094, mean_eps: 0.500000\n",
            "  210/8000: episode: 13, duration: 0.308s, episode steps:  24, steps per second:  78, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.235126, mae: 0.833266, mean_q: 1.340978, mean_eps: 0.500000\n",
            "  225/8000: episode: 14, duration: 0.190s, episode steps:  15, steps per second:  79, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.206514, mae: 0.883415, mean_q: 1.494715, mean_eps: 0.500000\n",
            "  245/8000: episode: 15, duration: 0.267s, episode steps:  20, steps per second:  75, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.210762, mae: 0.979474, mean_q: 1.667669, mean_eps: 0.500000\n",
            "  260/8000: episode: 16, duration: 0.183s, episode steps:  15, steps per second:  82, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.185741, mae: 1.017438, mean_q: 1.733589, mean_eps: 0.500000\n",
            "  268/8000: episode: 17, duration: 0.112s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.190811, mae: 1.063648, mean_q: 1.856689, mean_eps: 0.500000\n",
            "  278/8000: episode: 18, duration: 0.143s, episode steps:  10, steps per second:  70, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.197081, mae: 1.104503, mean_q: 1.936040, mean_eps: 0.500000\n",
            "  288/8000: episode: 19, duration: 0.140s, episode steps:  10, steps per second:  71, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.185094, mae: 1.151307, mean_q: 2.022246, mean_eps: 0.500000\n",
            "  299/8000: episode: 20, duration: 0.149s, episode steps:  11, steps per second:  74, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.218986, mae: 1.231204, mean_q: 2.122031, mean_eps: 0.500000\n",
            "  310/8000: episode: 21, duration: 0.161s, episode steps:  11, steps per second:  68, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.248567, mae: 1.279136, mean_q: 2.176884, mean_eps: 0.500000\n",
            "  324/8000: episode: 22, duration: 0.173s, episode steps:  14, steps per second:  81, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.229818, mae: 1.298588, mean_q: 2.224286, mean_eps: 0.500000\n",
            "  336/8000: episode: 23, duration: 0.162s, episode steps:  12, steps per second:  74, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.167827, mae: 1.296513, mean_q: 2.331004, mean_eps: 0.500000\n",
            "  358/8000: episode: 24, duration: 0.281s, episode steps:  22, steps per second:  78, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.221005, mae: 1.381395, mean_q: 2.524201, mean_eps: 0.500000\n",
            "  373/8000: episode: 25, duration: 0.194s, episode steps:  15, steps per second:  77, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.295891, mae: 1.468951, mean_q: 2.651210, mean_eps: 0.500000\n",
            "  395/8000: episode: 26, duration: 0.290s, episode steps:  22, steps per second:  76, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.318 [0.000, 1.000],  loss: 0.252866, mae: 1.493253, mean_q: 2.712696, mean_eps: 0.500000\n",
            "  413/8000: episode: 27, duration: 0.231s, episode steps:  18, steps per second:  78, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 0.322522, mae: 1.613351, mean_q: 2.906623, mean_eps: 0.500000\n",
            "  425/8000: episode: 28, duration: 0.153s, episode steps:  12, steps per second:  78, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.281921, mae: 1.648654, mean_q: 2.962767, mean_eps: 0.500000\n",
            "  435/8000: episode: 29, duration: 0.135s, episode steps:  10, steps per second:  74, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.326424, mae: 1.691225, mean_q: 3.090675, mean_eps: 0.500000\n",
            "  445/8000: episode: 30, duration: 0.135s, episode steps:  10, steps per second:  74, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.345096, mae: 1.730359, mean_q: 3.152859, mean_eps: 0.500000\n",
            "  458/8000: episode: 31, duration: 0.166s, episode steps:  13, steps per second:  78, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.335664, mae: 1.765450, mean_q: 3.220526, mean_eps: 0.500000\n",
            "  468/8000: episode: 32, duration: 0.150s, episode steps:  10, steps per second:  67, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.255582, mae: 1.765187, mean_q: 3.304232, mean_eps: 0.500000\n",
            "  485/8000: episode: 33, duration: 0.220s, episode steps:  17, steps per second:  77, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.317199, mae: 1.840990, mean_q: 3.381953, mean_eps: 0.500000\n",
            "  496/8000: episode: 34, duration: 0.154s, episode steps:  11, steps per second:  71, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.246746, mae: 1.869322, mean_q: 3.483624, mean_eps: 0.500000\n",
            "  509/8000: episode: 35, duration: 0.193s, episode steps:  13, steps per second:  67, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.216480, mae: 1.894245, mean_q: 3.640379, mean_eps: 0.500000\n",
            "  520/8000: episode: 36, duration: 0.145s, episode steps:  11, steps per second:  76, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.278508, mae: 1.964337, mean_q: 3.774800, mean_eps: 0.500000\n",
            "  535/8000: episode: 37, duration: 0.224s, episode steps:  15, steps per second:  67, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.284754, mae: 1.987059, mean_q: 3.814326, mean_eps: 0.500000\n",
            "  547/8000: episode: 38, duration: 0.146s, episode steps:  12, steps per second:  82, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.337735, mae: 2.026649, mean_q: 3.838076, mean_eps: 0.500000\n",
            "  560/8000: episode: 39, duration: 0.172s, episode steps:  13, steps per second:  75, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.377855, mae: 2.081555, mean_q: 3.962867, mean_eps: 0.500000\n",
            "  576/8000: episode: 40, duration: 0.199s, episode steps:  16, steps per second:  80, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.309120, mae: 2.108974, mean_q: 4.056880, mean_eps: 0.500000\n",
            "  587/8000: episode: 41, duration: 0.145s, episode steps:  11, steps per second:  76, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.378622, mae: 2.201449, mean_q: 4.186776, mean_eps: 0.500000\n",
            "  603/8000: episode: 42, duration: 0.204s, episode steps:  16, steps per second:  79, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 0.349764, mae: 2.215470, mean_q: 4.224583, mean_eps: 0.500000\n",
            "  616/8000: episode: 43, duration: 0.173s, episode steps:  13, steps per second:  75, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.425810, mae: 2.281836, mean_q: 4.243603, mean_eps: 0.500000\n",
            "  627/8000: episode: 44, duration: 0.144s, episode steps:  11, steps per second:  76, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.349630, mae: 2.312263, mean_q: 4.342130, mean_eps: 0.500000\n",
            "  645/8000: episode: 45, duration: 0.229s, episode steps:  18, steps per second:  79, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 0.260641, mae: 2.341604, mean_q: 4.591864, mean_eps: 0.500000\n",
            "  656/8000: episode: 46, duration: 0.150s, episode steps:  11, steps per second:  73, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.244201, mae: 2.401724, mean_q: 4.741500, mean_eps: 0.500000\n",
            "  672/8000: episode: 47, duration: 0.214s, episode steps:  16, steps per second:  75, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.404126, mae: 2.499500, mean_q: 4.735285, mean_eps: 0.500000\n",
            "  691/8000: episode: 48, duration: 0.249s, episode steps:  19, steps per second:  76, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 0.358913, mae: 2.529322, mean_q: 4.771201, mean_eps: 0.500000\n",
            "  707/8000: episode: 49, duration: 0.200s, episode steps:  16, steps per second:  80, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.330194, mae: 2.588142, mean_q: 4.910387, mean_eps: 0.500000\n",
            "  720/8000: episode: 50, duration: 0.170s, episode steps:  13, steps per second:  76, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.415953, mae: 2.662005, mean_q: 5.017883, mean_eps: 0.500000\n",
            "  729/8000: episode: 51, duration: 0.123s, episode steps:   9, steps per second:  73, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.329129, mae: 2.687371, mean_q: 5.035758, mean_eps: 0.500000\n",
            "  749/8000: episode: 52, duration: 0.256s, episode steps:  20, steps per second:  78, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 0.379804, mae: 2.735744, mean_q: 5.218499, mean_eps: 0.500000\n",
            "  761/8000: episode: 53, duration: 0.156s, episode steps:  12, steps per second:  77, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.381166, mae: 2.793791, mean_q: 5.280351, mean_eps: 0.500000\n",
            "  774/8000: episode: 54, duration: 0.178s, episode steps:  13, steps per second:  73, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.405226, mae: 2.834097, mean_q: 5.321160, mean_eps: 0.500000\n",
            "  788/8000: episode: 55, duration: 0.179s, episode steps:  14, steps per second:  78, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.326202, mae: 2.859681, mean_q: 5.421084, mean_eps: 0.500000\n",
            "  801/8000: episode: 56, duration: 0.174s, episode steps:  13, steps per second:  75, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.351942, mae: 2.913008, mean_q: 5.533737, mean_eps: 0.500000\n",
            "  812/8000: episode: 57, duration: 0.143s, episode steps:  11, steps per second:  77, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.481892, mae: 2.949103, mean_q: 5.480601, mean_eps: 0.500000\n",
            "  829/8000: episode: 58, duration: 0.214s, episode steps:  17, steps per second:  80, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.469525, mae: 2.994140, mean_q: 5.559446, mean_eps: 0.500000\n",
            "  845/8000: episode: 59, duration: 0.207s, episode steps:  16, steps per second:  77, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.368771, mae: 3.011558, mean_q: 5.673829, mean_eps: 0.500000\n",
            "  878/8000: episode: 60, duration: 0.417s, episode steps:  33, steps per second:  79, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.465894, mae: 3.090176, mean_q: 5.832377, mean_eps: 0.500000\n",
            "  893/8000: episode: 61, duration: 0.187s, episode steps:  15, steps per second:  80, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.345190, mae: 3.149798, mean_q: 5.964896, mean_eps: 0.500000\n",
            "  910/8000: episode: 62, duration: 0.232s, episode steps:  17, steps per second:  73, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.605489, mae: 3.255139, mean_q: 6.089602, mean_eps: 0.500000\n",
            "  953/8000: episode: 63, duration: 0.552s, episode steps:  43, steps per second:  78, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 0.415950, mae: 3.292777, mean_q: 6.223205, mean_eps: 0.500000\n",
            "  984/8000: episode: 64, duration: 0.418s, episode steps:  31, steps per second:  74, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.710 [0.000, 1.000],  loss: 0.611899, mae: 3.451733, mean_q: 6.524566, mean_eps: 0.500000\n",
            " 1006/8000: episode: 65, duration: 0.293s, episode steps:  22, steps per second:  75, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.440467, mae: 3.501189, mean_q: 6.633270, mean_eps: 0.500000\n",
            " 1022/8000: episode: 66, duration: 0.202s, episode steps:  16, steps per second:  79, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.614814, mae: 3.573682, mean_q: 6.765984, mean_eps: 0.500000\n",
            " 1048/8000: episode: 67, duration: 0.351s, episode steps:  26, steps per second:  74, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.543703, mae: 3.641494, mean_q: 6.957946, mean_eps: 0.500000\n",
            " 1065/8000: episode: 68, duration: 0.216s, episode steps:  17, steps per second:  79, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.539921, mae: 3.648874, mean_q: 6.890678, mean_eps: 0.500000\n",
            " 1081/8000: episode: 69, duration: 0.215s, episode steps:  16, steps per second:  75, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.637113, mae: 3.730691, mean_q: 7.058469, mean_eps: 0.500000\n",
            " 1123/8000: episode: 70, duration: 0.535s, episode steps:  42, steps per second:  78, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.605292, mae: 3.795598, mean_q: 7.140832, mean_eps: 0.500000\n",
            " 1156/8000: episode: 71, duration: 0.412s, episode steps:  33, steps per second:  80, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.606 [0.000, 1.000],  loss: 0.648475, mae: 3.905786, mean_q: 7.451504, mean_eps: 0.500000\n",
            " 1187/8000: episode: 72, duration: 0.405s, episode steps:  31, steps per second:  77, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.645 [0.000, 1.000],  loss: 0.867428, mae: 4.029348, mean_q: 7.586689, mean_eps: 0.500000\n",
            " 1203/8000: episode: 73, duration: 0.203s, episode steps:  16, steps per second:  79, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.760882, mae: 4.088980, mean_q: 7.724967, mean_eps: 0.500000\n",
            " 1334/8000: episode: 74, duration: 1.659s, episode steps: 131, steps per second:  79, episode reward: 131.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 0.787201, mae: 4.355121, mean_q: 8.289454, mean_eps: 0.500000\n",
            " 1471/8000: episode: 75, duration: 1.666s, episode steps: 137, steps per second:  82, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 0.895766, mae: 4.853198, mean_q: 9.277112, mean_eps: 0.500000\n",
            " 1503/8000: episode: 76, duration: 0.408s, episode steps:  32, steps per second:  78, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.957855, mae: 5.186226, mean_q: 9.952342, mean_eps: 0.500000\n",
            " 1518/8000: episode: 77, duration: 0.181s, episode steps:  15, steps per second:  83, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.615315, mae: 5.220979, mean_q: 10.096452, mean_eps: 0.500000\n",
            " 1577/8000: episode: 78, duration: 0.716s, episode steps:  59, steps per second:  82, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 0.839161, mae: 5.318416, mean_q: 10.296411, mean_eps: 0.500000\n",
            " 1593/8000: episode: 79, duration: 0.202s, episode steps:  16, steps per second:  79, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.083350, mae: 5.379802, mean_q: 10.387115, mean_eps: 0.500000\n",
            " 1633/8000: episode: 80, duration: 0.492s, episode steps:  40, steps per second:  81, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 1.437737, mae: 5.558535, mean_q: 10.605902, mean_eps: 0.500000\n",
            " 1730/8000: episode: 81, duration: 1.161s, episode steps:  97, steps per second:  84, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.433 [0.000, 1.000],  loss: 1.221672, mae: 5.682030, mean_q: 10.914773, mean_eps: 0.500000\n",
            " 1755/8000: episode: 82, duration: 0.316s, episode steps:  25, steps per second:  79, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 1.157742, mae: 5.834706, mean_q: 11.248649, mean_eps: 0.500000\n",
            " 1955/8000: episode: 83, duration: 2.408s, episode steps: 200, steps per second:  83, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 1.126821, mae: 6.154453, mean_q: 11.971566, mean_eps: 0.500000\n",
            " 2155/8000: episode: 84, duration: 2.385s, episode steps: 200, steps per second:  84, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 1.341059, mae: 6.850431, mean_q: 13.352302, mean_eps: 0.500000\n",
            " 2203/8000: episode: 85, duration: 0.590s, episode steps:  48, steps per second:  81, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.640869, mae: 7.241868, mean_q: 14.123495, mean_eps: 0.500000\n",
            " 2254/8000: episode: 86, duration: 0.676s, episode steps:  51, steps per second:  75, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.431 [0.000, 1.000],  loss: 1.158050, mae: 7.423842, mean_q: 14.522763, mean_eps: 0.500000\n",
            " 2280/8000: episode: 87, duration: 0.342s, episode steps:  26, steps per second:  76, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1.506227, mae: 7.603723, mean_q: 14.899384, mean_eps: 0.500000\n",
            " 2312/8000: episode: 88, duration: 0.397s, episode steps:  32, steps per second:  81, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 1.730574, mae: 7.614759, mean_q: 14.906997, mean_eps: 0.500000\n",
            " 2392/8000: episode: 89, duration: 0.962s, episode steps:  80, steps per second:  83, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 1.946488, mae: 7.806942, mean_q: 15.191173, mean_eps: 0.500000\n",
            " 2418/8000: episode: 90, duration: 0.339s, episode steps:  26, steps per second:  77, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.506201, mae: 7.994480, mean_q: 15.642311, mean_eps: 0.500000\n",
            " 2437/8000: episode: 91, duration: 0.257s, episode steps:  19, steps per second:  74, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.649073, mae: 7.926236, mean_q: 15.644483, mean_eps: 0.500000\n",
            " 2488/8000: episode: 92, duration: 0.647s, episode steps:  51, steps per second:  79, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.431 [0.000, 1.000],  loss: 2.609086, mae: 8.254552, mean_q: 16.094562, mean_eps: 0.500000\n",
            " 2537/8000: episode: 93, duration: 0.622s, episode steps:  49, steps per second:  79, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 1.356633, mae: 8.271388, mean_q: 16.211077, mean_eps: 0.500000\n",
            " 2575/8000: episode: 94, duration: 0.491s, episode steps:  38, steps per second:  77, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2.436222, mae: 8.516634, mean_q: 16.584437, mean_eps: 0.500000\n",
            " 2615/8000: episode: 95, duration: 0.523s, episode steps:  40, steps per second:  76, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 2.774074, mae: 8.674926, mean_q: 16.843015, mean_eps: 0.500000\n",
            " 2686/8000: episode: 96, duration: 0.887s, episode steps:  71, steps per second:  80, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 2.158501, mae: 8.745240, mean_q: 17.071625, mean_eps: 0.500000\n",
            " 2753/8000: episode: 97, duration: 0.837s, episode steps:  67, steps per second:  80, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 1.851051, mae: 8.969071, mean_q: 17.639126, mean_eps: 0.500000\n",
            " 2795/8000: episode: 98, duration: 0.489s, episode steps:  42, steps per second:  86, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 2.848938, mae: 9.121111, mean_q: 17.852794, mean_eps: 0.500000\n",
            " 2841/8000: episode: 99, duration: 0.565s, episode steps:  46, steps per second:  81, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 2.695075, mae: 9.239661, mean_q: 18.055242, mean_eps: 0.500000\n",
            " 2904/8000: episode: 100, duration: 0.800s, episode steps:  63, steps per second:  79, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 2.375263, mae: 9.398117, mean_q: 18.381215, mean_eps: 0.500000\n",
            " 2926/8000: episode: 101, duration: 0.293s, episode steps:  22, steps per second:  75, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 3.512305, mae: 9.640028, mean_q: 18.858945, mean_eps: 0.500000\n",
            " 2947/8000: episode: 102, duration: 0.267s, episode steps:  21, steps per second:  79, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 4.535560, mae: 9.619086, mean_q: 18.631449, mean_eps: 0.500000\n",
            " 2970/8000: episode: 103, duration: 0.275s, episode steps:  23, steps per second:  84, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 4.060014, mae: 9.593099, mean_q: 18.527637, mean_eps: 0.500000\n",
            " 2987/8000: episode: 104, duration: 0.233s, episode steps:  17, steps per second:  73, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 3.095618, mae: 9.790156, mean_q: 18.991749, mean_eps: 0.500000\n",
            " 3019/8000: episode: 105, duration: 0.406s, episode steps:  32, steps per second:  79, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 3.802097, mae: 9.811642, mean_q: 19.147345, mean_eps: 0.500000\n",
            " 3036/8000: episode: 106, duration: 0.232s, episode steps:  17, steps per second:  73, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 2.030845, mae: 9.698887, mean_q: 19.086755, mean_eps: 0.500000\n",
            " 3071/8000: episode: 107, duration: 0.452s, episode steps:  35, steps per second:  77, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 2.756539, mae: 9.805121, mean_q: 19.189565, mean_eps: 0.500000\n",
            " 3093/8000: episode: 108, duration: 0.280s, episode steps:  22, steps per second:  79, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1.359403, mae: 9.855001, mean_q: 19.528925, mean_eps: 0.500000\n",
            " 3155/8000: episode: 109, duration: 0.747s, episode steps:  62, steps per second:  83, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 3.762736, mae: 10.123888, mean_q: 19.748206, mean_eps: 0.500000\n",
            " 3169/8000: episode: 110, duration: 0.173s, episode steps:  14, steps per second:  81, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.343777, mae: 10.232774, mean_q: 20.024319, mean_eps: 0.500000\n",
            " 3193/8000: episode: 111, duration: 0.305s, episode steps:  24, steps per second:  79, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 3.997733, mae: 10.519503, mean_q: 20.523419, mean_eps: 0.500000\n",
            " 3230/8000: episode: 112, duration: 0.465s, episode steps:  37, steps per second:  80, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 3.842790, mae: 10.452590, mean_q: 20.361443, mean_eps: 0.500000\n",
            " 3248/8000: episode: 113, duration: 0.219s, episode steps:  18, steps per second:  82, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.760514, mae: 10.368518, mean_q: 20.299936, mean_eps: 0.500000\n",
            " 3295/8000: episode: 114, duration: 0.564s, episode steps:  47, steps per second:  83, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 4.107994, mae: 10.478667, mean_q: 20.491863, mean_eps: 0.500000\n",
            " 3318/8000: episode: 115, duration: 0.293s, episode steps:  23, steps per second:  79, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 3.987774, mae: 10.650403, mean_q: 20.703781, mean_eps: 0.500000\n",
            " 3379/8000: episode: 116, duration: 0.735s, episode steps:  61, steps per second:  83, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 4.201744, mae: 10.832221, mean_q: 21.017134, mean_eps: 0.500000\n",
            " 3454/8000: episode: 117, duration: 0.957s, episode steps:  75, steps per second:  78, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 3.416422, mae: 10.937198, mean_q: 21.349473, mean_eps: 0.500000\n",
            " 3472/8000: episode: 118, duration: 0.233s, episode steps:  18, steps per second:  77, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 2.797074, mae: 11.107791, mean_q: 21.811677, mean_eps: 0.500000\n",
            " 3503/8000: episode: 119, duration: 0.380s, episode steps:  31, steps per second:  82, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 5.992505, mae: 11.115246, mean_q: 21.557025, mean_eps: 0.500000\n",
            " 3529/8000: episode: 120, duration: 0.327s, episode steps:  26, steps per second:  79, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 4.542684, mae: 11.119510, mean_q: 21.514588, mean_eps: 0.500000\n",
            " 3547/8000: episode: 121, duration: 0.254s, episode steps:  18, steps per second:  71, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.758328, mae: 11.310252, mean_q: 21.880887, mean_eps: 0.500000\n",
            " 3591/8000: episode: 122, duration: 0.527s, episode steps:  44, steps per second:  84, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 4.880337, mae: 11.300333, mean_q: 21.936250, mean_eps: 0.500000\n",
            " 3640/8000: episode: 123, duration: 0.613s, episode steps:  49, steps per second:  80, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 4.036834, mae: 11.423843, mean_q: 22.273946, mean_eps: 0.500000\n",
            " 3840/8000: episode: 124, duration: 2.425s, episode steps: 200, steps per second:  82, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 5.219657, mae: 11.646780, mean_q: 22.616455, mean_eps: 0.500000\n",
            " 3879/8000: episode: 125, duration: 0.529s, episode steps:  39, steps per second:  74, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 4.688675, mae: 11.905188, mean_q: 23.113533, mean_eps: 0.500000\n",
            " 3993/8000: episode: 126, duration: 1.419s, episode steps: 114, steps per second:  80, episode reward: 114.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 4.185973, mae: 12.050038, mean_q: 23.621772, mean_eps: 0.500000\n",
            " 4006/8000: episode: 127, duration: 0.171s, episode steps:  13, steps per second:  76, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 7.841157, mae: 12.421164, mean_q: 23.947054, mean_eps: 0.500000\n",
            " 4028/8000: episode: 128, duration: 0.277s, episode steps:  22, steps per second:  79, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 6.621592, mae: 12.390115, mean_q: 23.885053, mean_eps: 0.500000\n",
            " 4056/8000: episode: 129, duration: 0.365s, episode steps:  28, steps per second:  77, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.878574, mae: 12.349736, mean_q: 23.898940, mean_eps: 0.500000\n",
            " 4113/8000: episode: 130, duration: 0.718s, episode steps:  57, steps per second:  79, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.456 [0.000, 1.000],  loss: 5.314496, mae: 12.280905, mean_q: 23.859301, mean_eps: 0.500000\n",
            " 4147/8000: episode: 131, duration: 0.427s, episode steps:  34, steps per second:  80, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 7.744584, mae: 12.482915, mean_q: 24.041117, mean_eps: 0.500000\n",
            " 4169/8000: episode: 132, duration: 0.273s, episode steps:  22, steps per second:  81, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.370275, mae: 12.460812, mean_q: 24.193280, mean_eps: 0.500000\n",
            " 4223/8000: episode: 133, duration: 0.664s, episode steps:  54, steps per second:  81, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.574 [0.000, 1.000],  loss: 5.903278, mae: 12.505461, mean_q: 24.263791, mean_eps: 0.500000\n",
            " 4250/8000: episode: 134, duration: 0.382s, episode steps:  27, steps per second:  71, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 4.823841, mae: 12.614943, mean_q: 24.545056, mean_eps: 0.500000\n",
            " 4303/8000: episode: 135, duration: 0.678s, episode steps:  53, steps per second:  78, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.566 [0.000, 1.000],  loss: 5.846452, mae: 12.646749, mean_q: 24.575043, mean_eps: 0.500000\n",
            " 4327/8000: episode: 136, duration: 0.301s, episode steps:  24, steps per second:  80, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 5.307811, mae: 12.727031, mean_q: 24.733216, mean_eps: 0.500000\n",
            " 4357/8000: episode: 137, duration: 0.376s, episode steps:  30, steps per second:  80, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.716781, mae: 12.897043, mean_q: 24.944892, mean_eps: 0.500000\n",
            " 4380/8000: episode: 138, duration: 0.303s, episode steps:  23, steps per second:  76, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 5.749221, mae: 12.789172, mean_q: 24.812657, mean_eps: 0.500000\n",
            " 4406/8000: episode: 139, duration: 0.328s, episode steps:  26, steps per second:  79, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 6.343580, mae: 12.929303, mean_q: 25.072826, mean_eps: 0.500000\n",
            " 4477/8000: episode: 140, duration: 0.923s, episode steps:  71, steps per second:  77, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 6.307155, mae: 12.969479, mean_q: 25.123874, mean_eps: 0.500000\n",
            " 4524/8000: episode: 141, duration: 0.627s, episode steps:  47, steps per second:  75, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  loss: 4.991107, mae: 12.999326, mean_q: 25.295785, mean_eps: 0.500000\n",
            " 4599/8000: episode: 142, duration: 0.935s, episode steps:  75, steps per second:  80, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 6.548507, mae: 13.086597, mean_q: 25.290148, mean_eps: 0.500000\n",
            " 4615/8000: episode: 143, duration: 0.212s, episode steps:  16, steps per second:  75, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 5.067929, mae: 13.173683, mean_q: 25.698375, mean_eps: 0.500000\n",
            " 4675/8000: episode: 144, duration: 0.731s, episode steps:  60, steps per second:  82, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 6.278943, mae: 13.220269, mean_q: 25.654805, mean_eps: 0.500000\n",
            " 4719/8000: episode: 145, duration: 0.576s, episode steps:  44, steps per second:  76, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 7.480460, mae: 13.320748, mean_q: 25.700750, mean_eps: 0.500000\n",
            " 4783/8000: episode: 146, duration: 0.772s, episode steps:  64, steps per second:  83, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 6.651846, mae: 13.370069, mean_q: 25.915650, mean_eps: 0.500000\n",
            " 4813/8000: episode: 147, duration: 0.384s, episode steps:  30, steps per second:  78, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 7.805566, mae: 13.367034, mean_q: 25.841520, mean_eps: 0.500000\n",
            " 4950/8000: episode: 148, duration: 1.656s, episode steps: 137, steps per second:  83, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 6.337795, mae: 13.493751, mean_q: 26.210830, mean_eps: 0.500000\n",
            " 4997/8000: episode: 149, duration: 0.565s, episode steps:  47, steps per second:  83, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 5.525154, mae: 13.571039, mean_q: 26.521086, mean_eps: 0.500000\n",
            " 5100/8000: episode: 150, duration: 1.257s, episode steps: 103, steps per second:  82, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 6.850842, mae: 13.670078, mean_q: 26.507934, mean_eps: 0.500000\n",
            " 5225/8000: episode: 151, duration: 1.484s, episode steps: 125, steps per second:  84, episode reward: 125.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 5.903509, mae: 13.867497, mean_q: 27.039734, mean_eps: 0.500000\n",
            " 5326/8000: episode: 152, duration: 1.221s, episode steps: 101, steps per second:  83, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 6.406800, mae: 14.068783, mean_q: 27.356797, mean_eps: 0.500000\n",
            " 5462/8000: episode: 153, duration: 1.640s, episode steps: 136, steps per second:  83, episode reward: 136.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 5.342124, mae: 14.154402, mean_q: 27.639359, mean_eps: 0.500000\n",
            " 5527/8000: episode: 154, duration: 0.807s, episode steps:  65, steps per second:  81, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 6.437046, mae: 14.472986, mean_q: 28.244579, mean_eps: 0.500000\n",
            " 5620/8000: episode: 155, duration: 1.131s, episode steps:  93, steps per second:  82, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 5.819420, mae: 14.502749, mean_q: 28.355257, mean_eps: 0.500000\n",
            " 5738/8000: episode: 156, duration: 1.422s, episode steps: 118, steps per second:  83, episode reward: 118.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 6.288969, mae: 14.669332, mean_q: 28.673895, mean_eps: 0.500000\n",
            " 5831/8000: episode: 157, duration: 1.139s, episode steps:  93, steps per second:  82, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 6.526017, mae: 14.808090, mean_q: 28.927138, mean_eps: 0.500000\n",
            " 5938/8000: episode: 158, duration: 1.301s, episode steps: 107, steps per second:  82, episode reward: 107.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 6.828639, mae: 15.047111, mean_q: 29.402397, mean_eps: 0.500000\n",
            " 6138/8000: episode: 159, duration: 2.394s, episode steps: 200, steps per second:  84, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 7.748948, mae: 15.173600, mean_q: 29.492962, mean_eps: 0.500000\n",
            " 6295/8000: episode: 160, duration: 1.893s, episode steps: 157, steps per second:  83, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 6.910642, mae: 15.314866, mean_q: 29.919660, mean_eps: 0.500000\n",
            " 6311/8000: episode: 161, duration: 0.196s, episode steps:  16, steps per second:  82, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 4.804385, mae: 15.469315, mean_q: 30.418010, mean_eps: 0.500000\n",
            " 6511/8000: episode: 162, duration: 2.446s, episode steps: 200, steps per second:  82, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.809338, mae: 15.691306, mean_q: 30.618527, mean_eps: 0.500000\n",
            " 6711/8000: episode: 163, duration: 2.760s, episode steps: 200, steps per second:  72, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 8.400716, mae: 15.869512, mean_q: 30.913930, mean_eps: 0.500000\n",
            " 6736/8000: episode: 164, duration: 0.335s, episode steps:  25, steps per second:  75, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 6.948894, mae: 15.865708, mean_q: 30.946265, mean_eps: 0.500000\n",
            " 6893/8000: episode: 165, duration: 1.985s, episode steps: 157, steps per second:  79, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 7.163589, mae: 16.076801, mean_q: 31.451279, mean_eps: 0.500000\n",
            " 7002/8000: episode: 166, duration: 1.373s, episode steps: 109, steps per second:  79, episode reward: 109.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 8.522013, mae: 16.292271, mean_q: 31.776845, mean_eps: 0.500000\n",
            " 7187/8000: episode: 167, duration: 2.353s, episode steps: 185, steps per second:  79, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 5.599619, mae: 16.384091, mean_q: 32.303546, mean_eps: 0.500000\n",
            " 7387/8000: episode: 168, duration: 2.550s, episode steps: 200, steps per second:  78, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 7.939923, mae: 16.831065, mean_q: 33.025138, mean_eps: 0.500000\n",
            " 7529/8000: episode: 169, duration: 1.746s, episode steps: 142, steps per second:  81, episode reward: 142.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 7.529859, mae: 17.081760, mean_q: 33.590342, mean_eps: 0.500000\n",
            " 7729/8000: episode: 170, duration: 2.526s, episode steps: 200, steps per second:  79, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 7.879336, mae: 17.282300, mean_q: 33.968541, mean_eps: 0.500000\n",
            " 7792/8000: episode: 171, duration: 0.776s, episode steps:  63, steps per second:  81, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 7.847778, mae: 17.587830, mean_q: 34.611354, mean_eps: 0.500000\n",
            " 7952/8000: episode: 172, duration: 1.986s, episode steps: 160, steps per second:  81, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 6.793722, mae: 17.705671, mean_q: 34.970816, mean_eps: 0.500000\n",
            "done, took 108.139 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 200.000, steps: 200\n",
            "Episode 6: reward: 200.000, steps: 200\n",
            "Episode 7: reward: 200.000, steps: 200\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 200.000, steps: 200\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 200.000, steps: 200\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 200.000, steps: 200\n",
            "Episode 15: reward: 200.000, steps: 200\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 200.000, steps: 200\n",
            "Episode 18: reward: 200.000, steps: 200\n",
            "Episode 19: reward: 200.000, steps: 200\n",
            "Episode 20: reward: 200.000, steps: 200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb172187950>"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    }
  ]
}