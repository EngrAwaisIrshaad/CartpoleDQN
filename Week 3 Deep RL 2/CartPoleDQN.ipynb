{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPoleDQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Megacity1/CartpoleDQN/blob/main/Week%203%20Deep%20RL%202/CartPoleDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Deep Q Network (DQN) for CartPole Using Boltzmann Q Policy\n",
        "This exercise implements a DQN for CartPole using a Boltzmann Q policy for selecting the actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGsC7cJ5jNcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda73cbf-3020-40f4-e45e-7b87053448f1"
      },
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.24.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (13.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.44.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.10.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMIHLgQ3Z-lF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5a8608-456f-4127-ce8a-f551e5f4e4f3"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0AMLzq08ap0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b380e1-6c3d-4bce-8233-a8ac01deaf26"
      },
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "!pip install Adam\n",
        "#from keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Adam in /usr/local/lib/python3.7/dist-packages (0.0.0.dev0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll6bNdUm54WS"
      },
      "source": [
        "Implementation of DQN for CartPole, applying policy BoltzmannQPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCrPKNy40PC"
      },
      "source": [
        "##Implement DQN with BoltzmannGumbelQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efM9jkXr5A3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f9dcedf4-c32e-4d65-dbac-1f2bbe0a41e3"
      },
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import BoltzmannGumbelQPolicy\n",
        "from rl.policy import LinearAnnealedPolicy\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n",
        "# setup experience replay buffer\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
        "                               attr='eps',            \n",
        "                               value_max=2.,\n",
        "                               value_min=.2, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=20)\n",
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "# add extra layers here\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=15,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy) \n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=6000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_35\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_33 (Flatten)        (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_67 (Dense)            (None, 16)                80        \n",
            "                                                                 \n",
            " dense_68 (Dense)            (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 6000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   14/6000: episode: 1, duration: 2.073s, episode steps:  14, steps per second:   7, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   28/6000: episode: 2, duration: 6.024s, episode steps:  14, steps per second:   2, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.143 [0.000, 1.000],  loss: 0.495308, mae: 0.559647, mean_q: -0.079533, mean_eps: 0.275000\n",
            "   38/6000: episode: 3, duration: 0.126s, episode steps:  10, steps per second:  79, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.405486, mae: 0.517495, mean_q: -0.014644, mean_eps: 0.200000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   50/6000: episode: 4, duration: 0.159s, episode steps:  12, steps per second:  76, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.332634, mae: 0.469260, mean_q: 0.104848, mean_eps: 0.200000\n",
            "   63/6000: episode: 5, duration: 0.174s, episode steps:  13, steps per second:  75, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.154 [0.000, 1.000],  loss: 0.282886, mae: 0.428036, mean_q: 0.230231, mean_eps: 0.200000\n",
            "   73/6000: episode: 6, duration: 0.136s, episode steps:  10, steps per second:  74, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.253495, mae: 0.394705, mean_q: 0.331508, mean_eps: 0.200000\n",
            "   84/6000: episode: 7, duration: 0.143s, episode steps:  11, steps per second:  77, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.217944, mae: 0.361479, mean_q: 0.455927, mean_eps: 0.200000\n",
            "   94/6000: episode: 8, duration: 0.136s, episode steps:  10, steps per second:  74, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.184629, mae: 0.315827, mean_q: 0.585234, mean_eps: 0.200000\n",
            "  103/6000: episode: 9, duration: 0.119s, episode steps:   9, steps per second:  75, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.193891, mae: 0.343059, mean_q: 0.666955, mean_eps: 0.200000\n",
            "  118/6000: episode: 10, duration: 0.186s, episode steps:  15, steps per second:  80, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.190927, mae: 0.389305, mean_q: 0.748984, mean_eps: 0.200000\n",
            "  127/6000: episode: 11, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.176696, mae: 0.428343, mean_q: 0.863094, mean_eps: 0.200000\n",
            "  139/6000: episode: 12, duration: 0.179s, episode steps:  12, steps per second:  67, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.180137, mae: 0.490892, mean_q: 0.946346, mean_eps: 0.200000\n",
            "  152/6000: episode: 13, duration: 0.177s, episode steps:  13, steps per second:  74, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.163861, mae: 0.529502, mean_q: 1.035627, mean_eps: 0.200000\n",
            "  162/6000: episode: 14, duration: 0.139s, episode steps:  10, steps per second:  72, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.168843, mae: 0.587923, mean_q: 1.101887, mean_eps: 0.200000\n",
            "  172/6000: episode: 15, duration: 0.134s, episode steps:  10, steps per second:  75, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.164318, mae: 0.631747, mean_q: 1.155073, mean_eps: 0.200000\n",
            "  182/6000: episode: 16, duration: 0.151s, episode steps:  10, steps per second:  66, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.172851, mae: 0.675161, mean_q: 1.212775, mean_eps: 0.200000\n",
            "  191/6000: episode: 17, duration: 0.132s, episode steps:   9, steps per second:  68, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.157252, mae: 0.702819, mean_q: 1.308706, mean_eps: 0.200000\n",
            "  201/6000: episode: 18, duration: 0.135s, episode steps:  10, steps per second:  74, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.143735, mae: 0.728316, mean_q: 1.405859, mean_eps: 0.200000\n",
            "  213/6000: episode: 19, duration: 0.178s, episode steps:  12, steps per second:  67, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.145458, mae: 0.781219, mean_q: 1.527530, mean_eps: 0.200000\n",
            "  223/6000: episode: 20, duration: 0.156s, episode steps:  10, steps per second:  64, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.177224, mae: 0.828843, mean_q: 1.611862, mean_eps: 0.200000\n",
            "  233/6000: episode: 21, duration: 0.145s, episode steps:  10, steps per second:  69, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.172949, mae: 0.877126, mean_q: 1.645430, mean_eps: 0.200000\n",
            "  242/6000: episode: 22, duration: 0.134s, episode steps:   9, steps per second:  67, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.174850, mae: 0.913416, mean_q: 1.695929, mean_eps: 0.200000\n",
            "  251/6000: episode: 23, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.172911, mae: 0.926842, mean_q: 1.729687, mean_eps: 0.200000\n",
            "  260/6000: episode: 24, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.159405, mae: 0.947625, mean_q: 1.832297, mean_eps: 0.200000\n",
            "  270/6000: episode: 25, duration: 0.144s, episode steps:  10, steps per second:  69, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.166809, mae: 0.960648, mean_q: 1.909036, mean_eps: 0.200000\n",
            "  282/6000: episode: 26, duration: 0.172s, episode steps:  12, steps per second:  70, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.083 [0.000, 1.000],  loss: 0.179978, mae: 1.010286, mean_q: 2.011861, mean_eps: 0.200000\n",
            "  293/6000: episode: 27, duration: 0.144s, episode steps:  11, steps per second:  76, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.194737, mae: 1.070973, mean_q: 2.019791, mean_eps: 0.200000\n",
            "  307/6000: episode: 28, duration: 0.181s, episode steps:  14, steps per second:  77, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.214 [0.000, 1.000],  loss: 0.204066, mae: 1.095221, mean_q: 2.092621, mean_eps: 0.200000\n",
            "  325/6000: episode: 29, duration: 0.225s, episode steps:  18, steps per second:  80, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.278 [0.000, 1.000],  loss: 0.222998, mae: 1.161624, mean_q: 2.196857, mean_eps: 0.200000\n",
            "  335/6000: episode: 30, duration: 0.128s, episode steps:  10, steps per second:  78, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.257508, mae: 1.213691, mean_q: 2.257866, mean_eps: 0.200000\n",
            "  343/6000: episode: 31, duration: 0.126s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.125 [0.000, 1.000],  loss: 0.286796, mae: 1.281004, mean_q: 2.329676, mean_eps: 0.200000\n",
            "  360/6000: episode: 32, duration: 0.248s, episode steps:  17, steps per second:  69, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: 0.244488, mae: 1.263454, mean_q: 2.390127, mean_eps: 0.200000\n",
            "  370/6000: episode: 33, duration: 0.147s, episode steps:  10, steps per second:  68, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.229525, mae: 1.254677, mean_q: 2.523162, mean_eps: 0.200000\n",
            "  381/6000: episode: 34, duration: 0.194s, episode steps:  11, steps per second:  57, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.233910, mae: 1.274806, mean_q: 2.581594, mean_eps: 0.200000\n",
            "  392/6000: episode: 35, duration: 0.172s, episode steps:  11, steps per second:  64, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.244741, mae: 1.324260, mean_q: 2.632812, mean_eps: 0.200000\n",
            "  401/6000: episode: 36, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.249313, mae: 1.344592, mean_q: 2.749826, mean_eps: 0.200000\n",
            "  411/6000: episode: 37, duration: 0.146s, episode steps:  10, steps per second:  69, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.234095, mae: 1.340715, mean_q: 2.834456, mean_eps: 0.200000\n",
            "  419/6000: episode: 38, duration: 0.141s, episode steps:   8, steps per second:  57, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.272617, mae: 1.401935, mean_q: 2.950734, mean_eps: 0.200000\n",
            "  429/6000: episode: 39, duration: 0.157s, episode steps:  10, steps per second:  64, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.324558, mae: 1.461193, mean_q: 2.979675, mean_eps: 0.200000\n",
            "  450/6000: episode: 40, duration: 0.268s, episode steps:  21, steps per second:  78, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.290556, mae: 1.465200, mean_q: 3.064228, mean_eps: 0.200000\n",
            "  462/6000: episode: 41, duration: 0.154s, episode steps:  12, steps per second:  78, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.083 [0.000, 1.000],  loss: 0.246187, mae: 1.505469, mean_q: 3.180981, mean_eps: 0.200000\n",
            "  473/6000: episode: 42, duration: 0.158s, episode steps:  11, steps per second:  70, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.295788, mae: 1.554655, mean_q: 3.283481, mean_eps: 0.200000\n",
            "  484/6000: episode: 43, duration: 0.147s, episode steps:  11, steps per second:  75, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.297500, mae: 1.591152, mean_q: 3.343190, mean_eps: 0.200000\n",
            "  494/6000: episode: 44, duration: 0.144s, episode steps:  10, steps per second:  69, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.340898, mae: 1.629355, mean_q: 3.403123, mean_eps: 0.200000\n",
            "  502/6000: episode: 45, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.328954, mae: 1.651881, mean_q: 3.437301, mean_eps: 0.200000\n",
            "  516/6000: episode: 46, duration: 0.219s, episode steps:  14, steps per second:  64, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.214 [0.000, 1.000],  loss: 0.287481, mae: 1.669626, mean_q: 3.520853, mean_eps: 0.200000\n",
            "  526/6000: episode: 47, duration: 0.157s, episode steps:  10, steps per second:  64, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.244974, mae: 1.690361, mean_q: 3.656456, mean_eps: 0.200000\n",
            "  536/6000: episode: 48, duration: 0.134s, episode steps:  10, steps per second:  75, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.336994, mae: 1.752097, mean_q: 3.722592, mean_eps: 0.200000\n",
            "  545/6000: episode: 49, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.308416, mae: 1.798885, mean_q: 3.727839, mean_eps: 0.200000\n",
            "  555/6000: episode: 50, duration: 0.144s, episode steps:  10, steps per second:  69, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.400569, mae: 1.901102, mean_q: 3.761123, mean_eps: 0.200000\n",
            "  565/6000: episode: 51, duration: 0.137s, episode steps:  10, steps per second:  73, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.281072, mae: 1.878939, mean_q: 3.807255, mean_eps: 0.200000\n",
            "  574/6000: episode: 52, duration: 0.121s, episode steps:   9, steps per second:  75, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.264622, mae: 1.880941, mean_q: 3.977497, mean_eps: 0.200000\n",
            "  583/6000: episode: 53, duration: 0.132s, episode steps:   9, steps per second:  68, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.313802, mae: 1.925138, mean_q: 4.037464, mean_eps: 0.200000\n",
            "  595/6000: episode: 54, duration: 0.159s, episode steps:  12, steps per second:  76, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.341766, mae: 2.032721, mean_q: 4.069799, mean_eps: 0.200000\n",
            "  606/6000: episode: 55, duration: 0.142s, episode steps:  11, steps per second:  78, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.293454, mae: 2.023308, mean_q: 4.169477, mean_eps: 0.200000\n",
            "  619/6000: episode: 56, duration: 0.200s, episode steps:  13, steps per second:  65, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.292221, mae: 2.088078, mean_q: 4.306394, mean_eps: 0.200000\n",
            "  629/6000: episode: 57, duration: 0.142s, episode steps:  10, steps per second:  71, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.383484, mae: 2.153978, mean_q: 4.275726, mean_eps: 0.200000\n",
            "  638/6000: episode: 58, duration: 0.131s, episode steps:   9, steps per second:  69, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.376244, mae: 2.165769, mean_q: 4.296124, mean_eps: 0.200000\n",
            "  649/6000: episode: 59, duration: 0.144s, episode steps:  11, steps per second:  76, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.392840, mae: 2.194920, mean_q: 4.331594, mean_eps: 0.200000\n",
            "  658/6000: episode: 60, duration: 0.124s, episode steps:   9, steps per second:  73, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.312181, mae: 2.128413, mean_q: 4.384566, mean_eps: 0.200000\n",
            "  668/6000: episode: 61, duration: 0.129s, episode steps:  10, steps per second:  77, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.376301, mae: 2.183855, mean_q: 4.467431, mean_eps: 0.200000\n",
            "  679/6000: episode: 62, duration: 0.142s, episode steps:  11, steps per second:  77, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.320433, mae: 2.191058, mean_q: 4.543687, mean_eps: 0.200000\n",
            "  691/6000: episode: 63, duration: 0.149s, episode steps:  12, steps per second:  81, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.362408, mae: 2.249309, mean_q: 4.600227, mean_eps: 0.200000\n",
            "  699/6000: episode: 64, duration: 0.119s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.125 [0.000, 1.000],  loss: 0.338843, mae: 2.261835, mean_q: 4.670751, mean_eps: 0.200000\n",
            "  709/6000: episode: 65, duration: 0.135s, episode steps:  10, steps per second:  74, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.328689, mae: 2.290770, mean_q: 4.744686, mean_eps: 0.200000\n",
            "  719/6000: episode: 66, duration: 0.131s, episode steps:  10, steps per second:  76, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.335116, mae: 2.300663, mean_q: 4.833202, mean_eps: 0.200000\n",
            "  728/6000: episode: 67, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.336706, mae: 2.346770, mean_q: 4.808717, mean_eps: 0.200000\n",
            "  738/6000: episode: 68, duration: 0.138s, episode steps:  10, steps per second:  72, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.287105, mae: 2.346842, mean_q: 4.971541, mean_eps: 0.200000\n",
            "  748/6000: episode: 69, duration: 0.126s, episode steps:  10, steps per second:  79, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.349999, mae: 2.372672, mean_q: 4.917765, mean_eps: 0.200000\n",
            "  758/6000: episode: 70, duration: 0.137s, episode steps:  10, steps per second:  73, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.389421, mae: 2.402764, mean_q: 4.881684, mean_eps: 0.200000\n",
            "  767/6000: episode: 71, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.337812, mae: 2.385982, mean_q: 4.834079, mean_eps: 0.200000\n",
            "  777/6000: episode: 72, duration: 0.138s, episode steps:  10, steps per second:  72, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.371251, mae: 2.407464, mean_q: 5.014966, mean_eps: 0.200000\n",
            "  787/6000: episode: 73, duration: 0.143s, episode steps:  10, steps per second:  70, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.403547, mae: 2.456954, mean_q: 5.041111, mean_eps: 0.200000\n",
            "  796/6000: episode: 74, duration: 0.132s, episode steps:   9, steps per second:  68, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.269153, mae: 2.450791, mean_q: 5.182605, mean_eps: 0.200000\n",
            "  809/6000: episode: 75, duration: 0.175s, episode steps:  13, steps per second:  74, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.375747, mae: 2.519417, mean_q: 5.160198, mean_eps: 0.200000\n",
            "  820/6000: episode: 76, duration: 0.146s, episode steps:  11, steps per second:  75, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.296349, mae: 2.556678, mean_q: 5.216036, mean_eps: 0.200000\n",
            "  834/6000: episode: 77, duration: 0.188s, episode steps:  14, steps per second:  75, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.301196, mae: 2.575428, mean_q: 5.374629, mean_eps: 0.200000\n",
            "  846/6000: episode: 78, duration: 0.159s, episode steps:  12, steps per second:  75, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.296132, mae: 2.610616, mean_q: 5.325654, mean_eps: 0.200000\n",
            "  857/6000: episode: 79, duration: 0.159s, episode steps:  11, steps per second:  69, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.293092, mae: 2.632824, mean_q: 5.309249, mean_eps: 0.200000\n",
            "  865/6000: episode: 80, duration: 0.120s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.125 [0.000, 1.000],  loss: 0.282593, mae: 2.633873, mean_q: 5.405137, mean_eps: 0.200000\n",
            "  874/6000: episode: 81, duration: 0.116s, episode steps:   9, steps per second:  77, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.298324, mae: 2.687588, mean_q: 5.484632, mean_eps: 0.200000\n",
            "  883/6000: episode: 82, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.277638, mae: 2.730107, mean_q: 5.606676, mean_eps: 0.200000\n",
            "  893/6000: episode: 83, duration: 0.133s, episode steps:  10, steps per second:  75, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.393483, mae: 2.732133, mean_q: 5.411675, mean_eps: 0.200000\n",
            "  902/6000: episode: 84, duration: 0.128s, episode steps:   9, steps per second:  70, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.326801, mae: 2.715419, mean_q: 5.367199, mean_eps: 0.200000\n",
            "  911/6000: episode: 85, duration: 0.116s, episode steps:   9, steps per second:  77, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.291250, mae: 2.720929, mean_q: 5.537312, mean_eps: 0.200000\n",
            "  923/6000: episode: 86, duration: 0.164s, episode steps:  12, steps per second:  73, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.301486, mae: 2.693021, mean_q: 5.391655, mean_eps: 0.200000\n",
            "  934/6000: episode: 87, duration: 0.153s, episode steps:  11, steps per second:  72, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.304586, mae: 2.718999, mean_q: 5.469197, mean_eps: 0.200000\n",
            "  946/6000: episode: 88, duration: 0.161s, episode steps:  12, steps per second:  75, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.278234, mae: 2.752246, mean_q: 5.607921, mean_eps: 0.200000\n",
            "  957/6000: episode: 89, duration: 0.136s, episode steps:  11, steps per second:  81, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.261499, mae: 2.770333, mean_q: 5.569148, mean_eps: 0.200000\n",
            "  970/6000: episode: 90, duration: 0.176s, episode steps:  13, steps per second:  74, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.273747, mae: 2.828309, mean_q: 5.589882, mean_eps: 0.200000\n",
            "  979/6000: episode: 91, duration: 0.112s, episode steps:   9, steps per second:  81, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.312311, mae: 2.831933, mean_q: 5.519370, mean_eps: 0.200000\n",
            "  992/6000: episode: 92, duration: 0.177s, episode steps:  13, steps per second:  74, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.290375, mae: 2.832559, mean_q: 5.528592, mean_eps: 0.200000\n",
            " 1000/6000: episode: 93, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.125 [0.000, 1.000],  loss: 0.229250, mae: 2.867313, mean_q: 5.671693, mean_eps: 0.200000\n",
            " 1011/6000: episode: 94, duration: 0.156s, episode steps:  11, steps per second:  71, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.238141, mae: 2.932496, mean_q: 5.800392, mean_eps: 0.200000\n",
            " 1022/6000: episode: 95, duration: 0.140s, episode steps:  11, steps per second:  78, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.269699, mae: 2.898340, mean_q: 5.644471, mean_eps: 0.200000\n",
            " 1032/6000: episode: 96, duration: 0.153s, episode steps:  10, steps per second:  65, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.236752, mae: 2.959213, mean_q: 5.785749, mean_eps: 0.200000\n",
            " 1047/6000: episode: 97, duration: 0.181s, episode steps:  15, steps per second:  83, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.226717, mae: 3.030994, mean_q: 5.957066, mean_eps: 0.200000\n",
            " 1058/6000: episode: 98, duration: 0.139s, episode steps:  11, steps per second:  79, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.249664, mae: 3.059779, mean_q: 5.964716, mean_eps: 0.200000\n",
            " 1069/6000: episode: 99, duration: 0.140s, episode steps:  11, steps per second:  79, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.241458, mae: 2.960695, mean_q: 5.703576, mean_eps: 0.200000\n",
            " 1082/6000: episode: 100, duration: 0.167s, episode steps:  13, steps per second:  78, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.241155, mae: 3.067517, mean_q: 5.929156, mean_eps: 0.200000\n",
            " 1105/6000: episode: 101, duration: 0.294s, episode steps:  23, steps per second:  78, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 0.206171, mae: 3.109448, mean_q: 6.004481, mean_eps: 0.200000\n",
            " 1127/6000: episode: 102, duration: 0.270s, episode steps:  22, steps per second:  81, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.183757, mae: 3.178561, mean_q: 6.092206, mean_eps: 0.200000\n",
            " 1183/6000: episode: 103, duration: 0.665s, episode steps:  56, steps per second:  84, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.554 [0.000, 1.000],  loss: 0.243701, mae: 3.264909, mean_q: 6.198996, mean_eps: 0.200000\n",
            " 1207/6000: episode: 104, duration: 0.279s, episode steps:  24, steps per second:  86, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 0.178081, mae: 3.427573, mean_q: 6.594708, mean_eps: 0.200000\n",
            " 1219/6000: episode: 105, duration: 0.147s, episode steps:  12, steps per second:  81, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.390772, mae: 3.458953, mean_q: 6.595710, mean_eps: 0.200000\n",
            " 1232/6000: episode: 106, duration: 0.166s, episode steps:  13, steps per second:  78, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.300075, mae: 3.541370, mean_q: 6.839145, mean_eps: 0.200000\n",
            " 1251/6000: episode: 107, duration: 0.253s, episode steps:  19, steps per second:  75, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.684 [0.000, 1.000],  loss: 0.554461, mae: 3.698253, mean_q: 7.124333, mean_eps: 0.200000\n",
            " 1263/6000: episode: 108, duration: 0.159s, episode steps:  12, steps per second:  76, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.442374, mae: 3.647108, mean_q: 6.924773, mean_eps: 0.200000\n",
            " 1274/6000: episode: 109, duration: 0.139s, episode steps:  11, steps per second:  79, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.346625, mae: 3.597746, mean_q: 6.901107, mean_eps: 0.200000\n",
            " 1298/6000: episode: 110, duration: 0.288s, episode steps:  24, steps per second:  83, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.358604, mae: 3.646409, mean_q: 6.963385, mean_eps: 0.200000\n",
            " 1312/6000: episode: 111, duration: 0.174s, episode steps:  14, steps per second:  81, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.439667, mae: 3.816707, mean_q: 7.272398, mean_eps: 0.200000\n",
            " 1331/6000: episode: 112, duration: 0.240s, episode steps:  19, steps per second:  79, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 0.320547, mae: 3.832501, mean_q: 7.374165, mean_eps: 0.200000\n",
            " 1347/6000: episode: 113, duration: 0.211s, episode steps:  16, steps per second:  76, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.939275, mae: 3.961916, mean_q: 7.533132, mean_eps: 0.200000\n",
            " 1364/6000: episode: 114, duration: 0.221s, episode steps:  17, steps per second:  77, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.992793, mae: 3.959918, mean_q: 7.506565, mean_eps: 0.200000\n",
            " 1386/6000: episode: 115, duration: 0.265s, episode steps:  22, steps per second:  83, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.781795, mae: 4.001622, mean_q: 7.613571, mean_eps: 0.200000\n",
            " 1397/6000: episode: 116, duration: 0.142s, episode steps:  11, steps per second:  78, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 1.214629, mae: 4.196424, mean_q: 7.939154, mean_eps: 0.200000\n",
            " 1406/6000: episode: 117, duration: 0.140s, episode steps:   9, steps per second:  64, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.331432, mae: 4.207277, mean_q: 7.897053, mean_eps: 0.200000\n",
            " 1417/6000: episode: 118, duration: 0.162s, episode steps:  11, steps per second:  68, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.523601, mae: 4.263807, mean_q: 8.101349, mean_eps: 0.200000\n",
            " 1426/6000: episode: 119, duration: 0.128s, episode steps:   9, steps per second:  70, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.026778, mae: 4.219625, mean_q: 7.980832, mean_eps: 0.200000\n",
            " 1437/6000: episode: 120, duration: 0.136s, episode steps:  11, steps per second:  81, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.958569, mae: 4.207013, mean_q: 8.023868, mean_eps: 0.200000\n",
            " 1448/6000: episode: 121, duration: 0.145s, episode steps:  11, steps per second:  76, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 1.315462, mae: 4.357566, mean_q: 8.254217, mean_eps: 0.200000\n",
            " 1456/6000: episode: 122, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.916386, mae: 4.303860, mean_q: 8.208172, mean_eps: 0.200000\n",
            " 1466/6000: episode: 123, duration: 0.138s, episode steps:  10, steps per second:  73, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.457926, mae: 4.371553, mean_q: 8.401551, mean_eps: 0.200000\n",
            " 1475/6000: episode: 124, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 1.499434, mae: 4.312753, mean_q: 8.176460, mean_eps: 0.200000\n",
            " 1485/6000: episode: 125, duration: 0.144s, episode steps:  10, steps per second:  69, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.702236, mae: 4.408773, mean_q: 8.424351, mean_eps: 0.200000\n",
            " 1495/6000: episode: 126, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 1.768283, mae: 4.487112, mean_q: 8.384881, mean_eps: 0.200000\n",
            " 1508/6000: episode: 127, duration: 0.160s, episode steps:  13, steps per second:  81, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 1.277391, mae: 4.542145, mean_q: 8.604105, mean_eps: 0.200000\n",
            " 1521/6000: episode: 128, duration: 0.153s, episode steps:  13, steps per second:  85, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 1.892512, mae: 4.677891, mean_q: 8.813899, mean_eps: 0.200000\n",
            " 1533/6000: episode: 129, duration: 0.157s, episode steps:  12, steps per second:  76, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.788806, mae: 4.681255, mean_q: 8.788210, mean_eps: 0.200000\n",
            " 1548/6000: episode: 130, duration: 0.185s, episode steps:  15, steps per second:  81, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 1.679753, mae: 4.762226, mean_q: 8.901239, mean_eps: 0.200000\n",
            " 1557/6000: episode: 131, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.497447, mae: 4.845192, mean_q: 9.054576, mean_eps: 0.200000\n",
            " 1566/6000: episode: 132, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.815320, mae: 4.579069, mean_q: 8.729220, mean_eps: 0.200000\n",
            " 1576/6000: episode: 133, duration: 0.133s, episode steps:  10, steps per second:  75, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 2.386830, mae: 4.969353, mean_q: 9.360258, mean_eps: 0.200000\n",
            " 1586/6000: episode: 134, duration: 0.126s, episode steps:  10, steps per second:  79, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 2.873213, mae: 4.870021, mean_q: 8.951168, mean_eps: 0.200000\n",
            " 1596/6000: episode: 135, duration: 0.140s, episode steps:  10, steps per second:  71, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 1.588554, mae: 4.842542, mean_q: 8.866314, mean_eps: 0.200000\n",
            " 1606/6000: episode: 136, duration: 0.123s, episode steps:  10, steps per second:  81, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 1.518065, mae: 4.771214, mean_q: 8.744933, mean_eps: 0.200000\n",
            " 1618/6000: episode: 137, duration: 0.168s, episode steps:  12, steps per second:  71, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 2.037474, mae: 4.827209, mean_q: 8.956309, mean_eps: 0.200000\n",
            " 1627/6000: episode: 138, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 3.131437, mae: 5.098271, mean_q: 9.322982, mean_eps: 0.200000\n",
            " 1637/6000: episode: 139, duration: 0.137s, episode steps:  10, steps per second:  73, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 1.563220, mae: 4.949878, mean_q: 9.153828, mean_eps: 0.200000\n",
            " 1649/6000: episode: 140, duration: 0.175s, episode steps:  12, steps per second:  69, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 2.485970, mae: 5.125842, mean_q: 9.374296, mean_eps: 0.200000\n",
            " 1660/6000: episode: 141, duration: 0.136s, episode steps:  11, steps per second:  81, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 1.475750, mae: 4.750715, mean_q: 8.816164, mean_eps: 0.200000\n",
            " 1673/6000: episode: 142, duration: 0.165s, episode steps:  13, steps per second:  79, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 3.167926, mae: 5.141871, mean_q: 9.314396, mean_eps: 0.200000\n",
            " 1691/6000: episode: 143, duration: 0.232s, episode steps:  18, steps per second:  77, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 2.379252, mae: 4.986177, mean_q: 9.109634, mean_eps: 0.200000\n",
            " 1707/6000: episode: 144, duration: 0.193s, episode steps:  16, steps per second:  83, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 1.347618, mae: 5.071007, mean_q: 9.489913, mean_eps: 0.200000\n",
            " 1722/6000: episode: 145, duration: 0.204s, episode steps:  15, steps per second:  74, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.239513, mae: 5.151616, mean_q: 9.589792, mean_eps: 0.200000\n",
            " 1734/6000: episode: 146, duration: 0.151s, episode steps:  12, steps per second:  80, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 1.654247, mae: 5.028924, mean_q: 9.415071, mean_eps: 0.200000\n",
            " 1757/6000: episode: 147, duration: 0.299s, episode steps:  23, steps per second:  77, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 3.290365, mae: 5.168357, mean_q: 9.278672, mean_eps: 0.200000\n",
            " 1814/6000: episode: 148, duration: 0.712s, episode steps:  57, steps per second:  80, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 2.340588, mae: 5.208996, mean_q: 9.546458, mean_eps: 0.200000\n",
            " 1836/6000: episode: 149, duration: 0.285s, episode steps:  22, steps per second:  77, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.510784, mae: 5.245377, mean_q: 9.642138, mean_eps: 0.200000\n",
            " 1860/6000: episode: 150, duration: 0.314s, episode steps:  24, steps per second:  76, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.519841, mae: 5.318725, mean_q: 9.663902, mean_eps: 0.200000\n",
            " 1882/6000: episode: 151, duration: 0.302s, episode steps:  22, steps per second:  73, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.981743, mae: 5.296042, mean_q: 9.775457, mean_eps: 0.200000\n",
            " 1899/6000: episode: 152, duration: 0.230s, episode steps:  17, steps per second:  74, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 2.187656, mae: 5.318539, mean_q: 9.885346, mean_eps: 0.200000\n",
            " 1922/6000: episode: 153, duration: 0.287s, episode steps:  23, steps per second:  80, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 2.552184, mae: 5.411230, mean_q: 9.948980, mean_eps: 0.200000\n",
            " 1948/6000: episode: 154, duration: 0.340s, episode steps:  26, steps per second:  77, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.086903, mae: 5.367347, mean_q: 9.879379, mean_eps: 0.200000\n",
            " 1965/6000: episode: 155, duration: 0.235s, episode steps:  17, steps per second:  72, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.458684, mae: 5.347971, mean_q: 9.852753, mean_eps: 0.200000\n",
            " 1992/6000: episode: 156, duration: 0.354s, episode steps:  27, steps per second:  76, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 1.544857, mae: 5.459879, mean_q: 10.241346, mean_eps: 0.200000\n",
            " 2015/6000: episode: 157, duration: 0.279s, episode steps:  23, steps per second:  83, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 2.001222, mae: 5.432702, mean_q: 10.070364, mean_eps: 0.200000\n",
            " 2063/6000: episode: 158, duration: 0.597s, episode steps:  48, steps per second:  80, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 2.068036, mae: 5.483273, mean_q: 10.156581, mean_eps: 0.200000\n",
            " 2117/6000: episode: 159, duration: 0.687s, episode steps:  54, steps per second:  79, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 1.873263, mae: 5.657254, mean_q: 10.589782, mean_eps: 0.200000\n",
            " 2150/6000: episode: 160, duration: 0.401s, episode steps:  33, steps per second:  82, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 1.386544, mae: 5.683213, mean_q: 10.764984, mean_eps: 0.200000\n",
            " 2207/6000: episode: 161, duration: 0.742s, episode steps:  57, steps per second:  77, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 1.906624, mae: 5.839196, mean_q: 10.990529, mean_eps: 0.200000\n",
            " 2243/6000: episode: 162, duration: 0.443s, episode steps:  36, steps per second:  81, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 2.414178, mae: 5.788727, mean_q: 10.761299, mean_eps: 0.200000\n",
            " 2315/6000: episode: 163, duration: 0.857s, episode steps:  72, steps per second:  84, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 1.839072, mae: 5.916986, mean_q: 11.163329, mean_eps: 0.200000\n",
            " 2367/6000: episode: 164, duration: 0.644s, episode steps:  52, steps per second:  81, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 2.002217, mae: 6.092717, mean_q: 11.455740, mean_eps: 0.200000\n",
            " 2431/6000: episode: 165, duration: 0.770s, episode steps:  64, steps per second:  83, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 1.941340, mae: 6.194264, mean_q: 11.706501, mean_eps: 0.200000\n",
            " 2552/6000: episode: 166, duration: 1.425s, episode steps: 121, steps per second:  85, episode reward: 121.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 2.336775, mae: 6.300971, mean_q: 11.832594, mean_eps: 0.200000\n",
            " 2614/6000: episode: 167, duration: 0.737s, episode steps:  62, steps per second:  84, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.799240, mae: 6.477371, mean_q: 12.336664, mean_eps: 0.200000\n",
            " 2680/6000: episode: 168, duration: 0.802s, episode steps:  66, steps per second:  82, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 1.938479, mae: 6.610904, mean_q: 12.577566, mean_eps: 0.200000\n",
            " 2737/6000: episode: 169, duration: 0.662s, episode steps:  57, steps per second:  86, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 2.401447, mae: 6.753019, mean_q: 12.791793, mean_eps: 0.200000\n",
            " 2790/6000: episode: 170, duration: 0.645s, episode steps:  53, steps per second:  82, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 2.299534, mae: 6.808068, mean_q: 12.900455, mean_eps: 0.200000\n",
            " 2860/6000: episode: 171, duration: 0.850s, episode steps:  70, steps per second:  82, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.341421, mae: 6.925639, mean_q: 13.127703, mean_eps: 0.200000\n",
            " 2893/6000: episode: 172, duration: 0.404s, episode steps:  33, steps per second:  82, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 2.303547, mae: 6.973665, mean_q: 13.247427, mean_eps: 0.200000\n",
            " 3034/6000: episode: 173, duration: 1.664s, episode steps: 141, steps per second:  85, episode reward: 141.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 2.197098, mae: 7.094057, mean_q: 13.483263, mean_eps: 0.200000\n",
            " 3102/6000: episode: 174, duration: 0.824s, episode steps:  68, steps per second:  83, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 2.114417, mae: 7.297567, mean_q: 13.969727, mean_eps: 0.200000\n",
            " 3195/6000: episode: 175, duration: 1.113s, episode steps:  93, steps per second:  84, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 2.163200, mae: 7.454266, mean_q: 14.309602, mean_eps: 0.200000\n",
            " 3224/6000: episode: 176, duration: 0.361s, episode steps:  29, steps per second:  80, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 1.579044, mae: 7.486896, mean_q: 14.444993, mean_eps: 0.200000\n",
            " 3293/6000: episode: 177, duration: 0.802s, episode steps:  69, steps per second:  86, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 2.271633, mae: 7.686868, mean_q: 14.778585, mean_eps: 0.200000\n",
            " 3336/6000: episode: 178, duration: 0.534s, episode steps:  43, steps per second:  80, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 2.911773, mae: 7.941347, mean_q: 15.130882, mean_eps: 0.200000\n",
            " 3410/6000: episode: 179, duration: 0.867s, episode steps:  74, steps per second:  85, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 2.320547, mae: 7.888828, mean_q: 15.140657, mean_eps: 0.200000\n",
            " 3453/6000: episode: 180, duration: 0.529s, episode steps:  43, steps per second:  81, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 3.175237, mae: 8.068853, mean_q: 15.381300, mean_eps: 0.200000\n",
            " 3512/6000: episode: 181, duration: 1.480s, episode steps:  59, steps per second:  40, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 2.526437, mae: 8.079186, mean_q: 15.535372, mean_eps: 0.200000\n",
            " 3571/6000: episode: 182, duration: 1.062s, episode steps:  59, steps per second:  56, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 2.964840, mae: 8.154759, mean_q: 15.592696, mean_eps: 0.200000\n",
            " 3618/6000: episode: 183, duration: 0.562s, episode steps:  47, steps per second:  84, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 2.612647, mae: 8.221634, mean_q: 15.775309, mean_eps: 0.200000\n",
            " 3727/6000: episode: 184, duration: 1.293s, episode steps: 109, steps per second:  84, episode reward: 109.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 2.043675, mae: 8.397284, mean_q: 16.240506, mean_eps: 0.200000\n",
            " 3787/6000: episode: 185, duration: 0.709s, episode steps:  60, steps per second:  85, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.583395, mae: 8.612649, mean_q: 16.628343, mean_eps: 0.200000\n",
            " 3900/6000: episode: 186, duration: 1.356s, episode steps: 113, steps per second:  83, episode reward: 113.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 2.751261, mae: 8.797120, mean_q: 16.994207, mean_eps: 0.200000\n",
            " 4050/6000: episode: 187, duration: 1.722s, episode steps: 150, steps per second:  87, episode reward: 150.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 2.861665, mae: 9.081550, mean_q: 17.534990, mean_eps: 0.200000\n",
            " 4125/6000: episode: 188, duration: 0.887s, episode steps:  75, steps per second:  85, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 2.617629, mae: 9.181231, mean_q: 17.785927, mean_eps: 0.200000\n",
            " 4229/6000: episode: 189, duration: 1.242s, episode steps: 104, steps per second:  84, episode reward: 104.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 3.923906, mae: 9.450769, mean_q: 18.138321, mean_eps: 0.200000\n",
            " 4288/6000: episode: 190, duration: 0.742s, episode steps:  59, steps per second:  80, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 3.052248, mae: 9.562761, mean_q: 18.513722, mean_eps: 0.200000\n",
            " 4365/6000: episode: 191, duration: 1.011s, episode steps:  77, steps per second:  76, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 3.117933, mae: 9.637307, mean_q: 18.682639, mean_eps: 0.200000\n",
            " 4450/6000: episode: 192, duration: 1.067s, episode steps:  85, steps per second:  80, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 2.660304, mae: 9.751269, mean_q: 19.013860, mean_eps: 0.200000\n",
            " 4514/6000: episode: 193, duration: 0.824s, episode steps:  64, steps per second:  78, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 3.527987, mae: 9.905264, mean_q: 19.089410, mean_eps: 0.200000\n",
            " 4575/6000: episode: 194, duration: 0.808s, episode steps:  61, steps per second:  76, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 3.107367, mae: 9.971789, mean_q: 19.336585, mean_eps: 0.200000\n",
            " 4695/6000: episode: 195, duration: 1.467s, episode steps: 120, steps per second:  82, episode reward: 120.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 3.337129, mae: 10.182564, mean_q: 19.745585, mean_eps: 0.200000\n",
            " 4759/6000: episode: 196, duration: 0.826s, episode steps:  64, steps per second:  78, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 4.108836, mae: 10.292537, mean_q: 19.843874, mean_eps: 0.200000\n",
            " 4827/6000: episode: 197, duration: 0.866s, episode steps:  68, steps per second:  79, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 2.928500, mae: 10.391173, mean_q: 20.198343, mean_eps: 0.200000\n",
            " 4904/6000: episode: 198, duration: 0.995s, episode steps:  77, steps per second:  77, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 3.127545, mae: 10.445200, mean_q: 20.276197, mean_eps: 0.200000\n",
            " 4975/6000: episode: 199, duration: 0.950s, episode steps:  71, steps per second:  75, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 3.465452, mae: 10.528797, mean_q: 20.443630, mean_eps: 0.200000\n",
            " 5044/6000: episode: 200, duration: 0.880s, episode steps:  69, steps per second:  78, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 4.202000, mae: 10.666693, mean_q: 20.572365, mean_eps: 0.200000\n",
            " 5158/6000: episode: 201, duration: 1.381s, episode steps: 114, steps per second:  83, episode reward: 114.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 3.307937, mae: 10.723900, mean_q: 20.849246, mean_eps: 0.200000\n",
            " 5220/6000: episode: 202, duration: 0.769s, episode steps:  62, steps per second:  81, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 2.780104, mae: 10.911846, mean_q: 21.297878, mean_eps: 0.200000\n",
            " 5279/6000: episode: 203, duration: 0.779s, episode steps:  59, steps per second:  76, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 3.741701, mae: 11.089009, mean_q: 21.553861, mean_eps: 0.200000\n",
            " 5323/6000: episode: 204, duration: 0.570s, episode steps:  44, steps per second:  77, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 3.511536, mae: 11.084851, mean_q: 21.602083, mean_eps: 0.200000\n",
            " 5384/6000: episode: 205, duration: 0.805s, episode steps:  61, steps per second:  76, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 2.880581, mae: 11.189730, mean_q: 21.914052, mean_eps: 0.200000\n",
            " 5434/6000: episode: 206, duration: 0.651s, episode steps:  50, steps per second:  77, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 4.102165, mae: 11.264711, mean_q: 21.895192, mean_eps: 0.200000\n",
            " 5530/6000: episode: 207, duration: 1.279s, episode steps:  96, steps per second:  75, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 3.934198, mae: 11.487267, mean_q: 22.371871, mean_eps: 0.200000\n",
            " 5631/6000: episode: 208, duration: 1.321s, episode steps: 101, steps per second:  76, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 3.874461, mae: 11.585683, mean_q: 22.574237, mean_eps: 0.200000\n",
            " 5716/6000: episode: 209, duration: 1.069s, episode steps:  85, steps per second:  80, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 3.968236, mae: 11.677668, mean_q: 22.746749, mean_eps: 0.200000\n",
            " 5822/6000: episode: 210, duration: 1.338s, episode steps: 106, steps per second:  79, episode reward: 106.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 3.941522, mae: 11.826872, mean_q: 23.063805, mean_eps: 0.200000\n",
            " 5880/6000: episode: 211, duration: 0.724s, episode steps:  58, steps per second:  80, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 3.569974, mae: 12.060047, mean_q: 23.545627, mean_eps: 0.200000\n",
            " 5954/6000: episode: 212, duration: 0.931s, episode steps:  74, steps per second:  79, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 3.511974, mae: 12.057698, mean_q: 23.624106, mean_eps: 0.200000\n",
            "done, took 85.171 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 58.000, steps: 58\n",
            "Episode 3: reward: 77.000, steps: 77\n",
            "Episode 4: reward: 73.000, steps: 73\n",
            "Episode 5: reward: 143.000, steps: 143\n",
            "Episode 6: reward: 91.000, steps: 91\n",
            "Episode 7: reward: 96.000, steps: 96\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 155.000, steps: 155\n",
            "Episode 10: reward: 95.000, steps: 95\n",
            "Episode 11: reward: 71.000, steps: 71\n",
            "Episode 12: reward: 200.000, steps: 200\n",
            "Episode 13: reward: 103.000, steps: 103\n",
            "Episode 14: reward: 200.000, steps: 200\n",
            "Episode 15: reward: 91.000, steps: 91\n",
            "Episode 16: reward: 114.000, steps: 114\n",
            "Episode 17: reward: 52.000, steps: 52\n",
            "Episode 18: reward: 72.000, steps: 72\n",
            "Episode 19: reward: 161.000, steps: 161\n",
            "Episode 20: reward: 151.000, steps: 151\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb17210e810>"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    }
  ]
}