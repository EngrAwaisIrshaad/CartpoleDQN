{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPoleDQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Megacity1/CartpoleDQN/blob/main/Week%203%20Deep%20RL%202/CartPoleDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Deep Q Network (DQN) for CartPole Using Boltzmann Q Policy\n",
        "This exercise implements a DQN for CartPole using a Boltzmann Q policy for selecting the actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGsC7cJ5jNcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda73cbf-3020-40f4-e45e-7b87053448f1"
      },
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.24.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (13.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.44.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.10.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMIHLgQ3Z-lF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5a8608-456f-4127-ce8a-f551e5f4e4f3"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0AMLzq08ap0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b380e1-6c3d-4bce-8233-a8ac01deaf26"
      },
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "!pip install Adam\n",
        "#from keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Adam in /usr/local/lib/python3.7/dist-packages (0.0.0.dev0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll6bNdUm54WS"
      },
      "source": [
        "Implementation of DQN for CartPole, applying policy BoltzmannQPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCrPKNy40PC"
      },
      "source": [
        "##Implement DQN with BoltzmannGumbelQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efM9jkXr5A3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1e98f0eb-c3cd-4b61-a318-92410d160792"
      },
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import BoltzmannGumbelQPolicy\n",
        "from rl.policy import LinearAnnealedPolicy\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n",
        "# setup experience replay buffer\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
        "                               attr='eps',            \n",
        "                               value_max=2.,\n",
        "                               value_min=.2, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=200)\n",
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "# add extra layers here\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=15,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy) \n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=2000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_32\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_30 (Flatten)        (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_61 (Dense)            (None, 16)                80        \n",
            "                                                                 \n",
            " dense_62 (Dense)            (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 2000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   14/2000: episode: 1, duration: 1.571s, episode steps:  14, steps per second:   9, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   34/2000: episode: 2, duration: 6.177s, episode steps:  20, steps per second:   3, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.899688, mae: 0.751043, mean_q: 0.365827, mean_eps: 1.779500\n",
            "   50/2000: episode: 3, duration: 0.273s, episode steps:  16, steps per second:  59, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.833486, mae: 0.791114, mean_q: 0.470564, mean_eps: 1.626500\n",
            "   77/2000: episode: 4, duration: 0.507s, episode steps:  27, steps per second:  53, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.593 [0.000, 1.000],  loss: 0.625459, mae: 0.734029, mean_q: 0.524784, mean_eps: 1.433000\n",
            "  113/2000: episode: 5, duration: 0.634s, episode steps:  36, steps per second:  57, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 0.488246, mae: 0.729002, mean_q: 0.680736, mean_eps: 1.149500\n",
            "  180/2000: episode: 6, duration: 0.775s, episode steps:  67, steps per second:  86, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 0.347620, mae: 0.741666, mean_q: 0.919038, mean_eps: 0.686000\n",
            "  290/2000: episode: 7, duration: 1.353s, episode steps: 110, steps per second:  81, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.205146, mae: 0.884738, mean_q: 1.408184, mean_eps: 0.217182\n",
            "  380/2000: episode: 8, duration: 1.380s, episode steps:  90, steps per second:  65, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.422 [0.000, 1.000],  loss: 0.120673, mae: 1.093424, mean_q: 1.994561, mean_eps: 0.200000\n",
            "  400/2000: episode: 9, duration: 0.385s, episode steps:  20, steps per second:  52, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.097482, mae: 1.276082, mean_q: 2.398572, mean_eps: 0.200000\n",
            "  439/2000: episode: 10, duration: 1.048s, episode steps:  39, steps per second:  37, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.410 [0.000, 1.000],  loss: 0.106863, mae: 1.382903, mean_q: 2.604045, mean_eps: 0.200000\n",
            "  458/2000: episode: 11, duration: 0.281s, episode steps:  19, steps per second:  68, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.124513, mae: 1.508691, mean_q: 2.877559, mean_eps: 0.200000\n",
            "  471/2000: episode: 12, duration: 0.166s, episode steps:  13, steps per second:  78, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.141845, mae: 1.560816, mean_q: 2.991259, mean_eps: 0.200000\n",
            "  483/2000: episode: 13, duration: 0.161s, episode steps:  12, steps per second:  75, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.137425, mae: 1.621714, mean_q: 3.090226, mean_eps: 0.200000\n",
            "  492/2000: episode: 14, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.167400, mae: 1.699350, mean_q: 3.214472, mean_eps: 0.200000\n",
            "  504/2000: episode: 15, duration: 0.148s, episode steps:  12, steps per second:  81, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.167392, mae: 1.747686, mean_q: 3.317535, mean_eps: 0.200000\n",
            "  515/2000: episode: 16, duration: 0.152s, episode steps:  11, steps per second:  72, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.199893, mae: 1.787435, mean_q: 3.395979, mean_eps: 0.200000\n",
            "  525/2000: episode: 17, duration: 0.167s, episode steps:  10, steps per second:  60, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.270425, mae: 1.859007, mean_q: 3.534766, mean_eps: 0.200000\n",
            "  534/2000: episode: 18, duration: 0.148s, episode steps:   9, steps per second:  61, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.200255, mae: 1.894756, mean_q: 3.604235, mean_eps: 0.200000\n",
            "  547/2000: episode: 19, duration: 0.258s, episode steps:  13, steps per second:  50, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.216622, mae: 1.928614, mean_q: 3.653558, mean_eps: 0.200000\n",
            "  560/2000: episode: 20, duration: 0.217s, episode steps:  13, steps per second:  60, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.202088, mae: 1.976316, mean_q: 3.776271, mean_eps: 0.200000\n",
            "  570/2000: episode: 21, duration: 0.160s, episode steps:  10, steps per second:  62, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.107533, mae: 1.975980, mean_q: 3.877126, mean_eps: 0.200000\n",
            "  586/2000: episode: 22, duration: 0.260s, episode steps:  16, steps per second:  61, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.273157, mae: 2.123136, mean_q: 4.057530, mean_eps: 0.200000\n",
            "  596/2000: episode: 23, duration: 0.185s, episode steps:  10, steps per second:  54, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.249595, mae: 2.153014, mean_q: 4.121398, mean_eps: 0.200000\n",
            "  605/2000: episode: 24, duration: 0.181s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.388164, mae: 2.227551, mean_q: 4.198116, mean_eps: 0.200000\n",
            "  617/2000: episode: 25, duration: 0.218s, episode steps:  12, steps per second:  55, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.263437, mae: 2.234965, mean_q: 4.236139, mean_eps: 0.200000\n",
            "  629/2000: episode: 26, duration: 0.233s, episode steps:  12, steps per second:  52, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.361800, mae: 2.319313, mean_q: 4.367151, mean_eps: 0.200000\n",
            "  638/2000: episode: 27, duration: 0.174s, episode steps:   9, steps per second:  52, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.293500, mae: 2.343189, mean_q: 4.429906, mean_eps: 0.200000\n",
            "  650/2000: episode: 28, duration: 0.206s, episode steps:  12, steps per second:  58, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.405954, mae: 2.394959, mean_q: 4.570412, mean_eps: 0.200000\n",
            "  661/2000: episode: 29, duration: 0.190s, episode steps:  11, steps per second:  58, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.422680, mae: 2.463625, mean_q: 4.652074, mean_eps: 0.200000\n",
            "  671/2000: episode: 30, duration: 0.166s, episode steps:  10, steps per second:  60, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.399210, mae: 2.498803, mean_q: 4.679854, mean_eps: 0.200000\n",
            "  682/2000: episode: 31, duration: 0.168s, episode steps:  11, steps per second:  65, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.325542, mae: 2.513469, mean_q: 4.788930, mean_eps: 0.200000\n",
            "  696/2000: episode: 32, duration: 0.279s, episode steps:  14, steps per second:  50, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.581945, mae: 2.638103, mean_q: 4.934806, mean_eps: 0.200000\n",
            "  708/2000: episode: 33, duration: 0.198s, episode steps:  12, steps per second:  61, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.332892, mae: 2.632415, mean_q: 4.984365, mean_eps: 0.200000\n",
            "  724/2000: episode: 34, duration: 0.291s, episode steps:  16, steps per second:  55, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.349416, mae: 2.690593, mean_q: 5.103841, mean_eps: 0.200000\n",
            "  737/2000: episode: 35, duration: 0.201s, episode steps:  13, steps per second:  65, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.417649, mae: 2.737391, mean_q: 5.223825, mean_eps: 0.200000\n",
            "  747/2000: episode: 36, duration: 0.197s, episode steps:  10, steps per second:  51, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.760051, mae: 2.878369, mean_q: 5.294669, mean_eps: 0.200000\n",
            "  758/2000: episode: 37, duration: 0.182s, episode steps:  11, steps per second:  60, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.336145, mae: 2.788516, mean_q: 5.304614, mean_eps: 0.200000\n",
            "  771/2000: episode: 38, duration: 0.216s, episode steps:  13, steps per second:  60, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.632528, mae: 2.904939, mean_q: 5.408675, mean_eps: 0.200000\n",
            "  782/2000: episode: 39, duration: 0.176s, episode steps:  11, steps per second:  62, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.535977, mae: 2.920811, mean_q: 5.440019, mean_eps: 0.200000\n",
            "  795/2000: episode: 40, duration: 0.199s, episode steps:  13, steps per second:  65, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.534920, mae: 2.971030, mean_q: 5.522153, mean_eps: 0.200000\n",
            "  807/2000: episode: 41, duration: 0.170s, episode steps:  12, steps per second:  71, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.464786, mae: 2.996482, mean_q: 5.666369, mean_eps: 0.200000\n",
            "  816/2000: episode: 42, duration: 0.143s, episode steps:   9, steps per second:  63, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.500067, mae: 3.040205, mean_q: 5.752996, mean_eps: 0.200000\n",
            "  829/2000: episode: 43, duration: 0.211s, episode steps:  13, steps per second:  62, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.386290, mae: 3.054968, mean_q: 5.789260, mean_eps: 0.200000\n",
            "  839/2000: episode: 44, duration: 0.171s, episode steps:  10, steps per second:  59, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.611466, mae: 3.123103, mean_q: 5.850303, mean_eps: 0.200000\n",
            "  848/2000: episode: 45, duration: 0.160s, episode steps:   9, steps per second:  56, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.708611, mae: 3.188086, mean_q: 5.887483, mean_eps: 0.200000\n",
            "  861/2000: episode: 46, duration: 0.215s, episode steps:  13, steps per second:  60, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.666200, mae: 3.208565, mean_q: 5.916858, mean_eps: 0.200000\n",
            "  878/2000: episode: 47, duration: 0.348s, episode steps:  17, steps per second:  49, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.683806, mae: 3.249419, mean_q: 5.996763, mean_eps: 0.200000\n",
            "  894/2000: episode: 48, duration: 0.294s, episode steps:  16, steps per second:  54, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.544114, mae: 3.272247, mean_q: 6.155938, mean_eps: 0.200000\n",
            "  907/2000: episode: 49, duration: 0.229s, episode steps:  13, steps per second:  57, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.866275, mae: 3.381006, mean_q: 6.206154, mean_eps: 0.200000\n",
            "  922/2000: episode: 50, duration: 0.259s, episode steps:  15, steps per second:  58, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.659092, mae: 3.357698, mean_q: 6.145340, mean_eps: 0.200000\n",
            "  940/2000: episode: 51, duration: 0.318s, episode steps:  18, steps per second:  57, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.791932, mae: 3.431623, mean_q: 6.312369, mean_eps: 0.200000\n",
            " 1001/2000: episode: 52, duration: 0.986s, episode steps:  61, steps per second:  62, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 0.767742, mae: 3.564150, mean_q: 6.620716, mean_eps: 0.200000\n",
            " 1027/2000: episode: 53, duration: 0.511s, episode steps:  26, steps per second:  51, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.908140, mae: 3.705603, mean_q: 6.867019, mean_eps: 0.200000\n",
            " 1067/2000: episode: 54, duration: 0.592s, episode steps:  40, steps per second:  68, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.425 [0.000, 1.000],  loss: 0.740047, mae: 3.784612, mean_q: 7.079413, mean_eps: 0.200000\n",
            " 1081/2000: episode: 55, duration: 0.190s, episode steps:  14, steps per second:  74, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.798075, mae: 3.878528, mean_q: 7.397736, mean_eps: 0.200000\n",
            " 1092/2000: episode: 56, duration: 0.165s, episode steps:  11, steps per second:  67, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.617684, mae: 3.924247, mean_q: 7.583890, mean_eps: 0.200000\n",
            " 1104/2000: episode: 57, duration: 0.163s, episode steps:  12, steps per second:  74, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.654764, mae: 3.990749, mean_q: 7.640935, mean_eps: 0.200000\n",
            " 1116/2000: episode: 58, duration: 0.150s, episode steps:  12, steps per second:  80, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.816025, mae: 4.092064, mean_q: 7.796501, mean_eps: 0.200000\n",
            " 1127/2000: episode: 59, duration: 0.151s, episode steps:  11, steps per second:  73, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 1.005947, mae: 4.154926, mean_q: 7.885609, mean_eps: 0.200000\n",
            " 1137/2000: episode: 60, duration: 0.120s, episode steps:  10, steps per second:  83, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.858624, mae: 4.140475, mean_q: 7.880416, mean_eps: 0.200000\n",
            " 1147/2000: episode: 61, duration: 0.129s, episode steps:  10, steps per second:  77, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.965023, mae: 4.198728, mean_q: 7.973377, mean_eps: 0.200000\n",
            " 1157/2000: episode: 62, duration: 0.129s, episode steps:  10, steps per second:  77, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 1.130062, mae: 4.274638, mean_q: 8.089899, mean_eps: 0.200000\n",
            " 1167/2000: episode: 63, duration: 0.122s, episode steps:  10, steps per second:  82, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.858555, mae: 4.254182, mean_q: 8.214531, mean_eps: 0.200000\n",
            " 1176/2000: episode: 64, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 1.461465, mae: 4.408504, mean_q: 8.441209, mean_eps: 0.200000\n",
            " 1187/2000: episode: 65, duration: 0.131s, episode steps:  11, steps per second:  84, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 1.007382, mae: 4.385073, mean_q: 8.438300, mean_eps: 0.200000\n",
            " 1200/2000: episode: 66, duration: 0.170s, episode steps:  13, steps per second:  77, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.154 [0.000, 1.000],  loss: 1.264859, mae: 4.506972, mean_q: 8.494665, mean_eps: 0.200000\n",
            " 1209/2000: episode: 67, duration: 0.177s, episode steps:   9, steps per second:  51, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 1.395419, mae: 4.558847, mean_q: 8.505241, mean_eps: 0.200000\n",
            " 1219/2000: episode: 68, duration: 0.194s, episode steps:  10, steps per second:  52, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 1.340246, mae: 4.584823, mean_q: 8.671597, mean_eps: 0.200000\n",
            " 1229/2000: episode: 69, duration: 0.198s, episode steps:  10, steps per second:  51, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 1.200019, mae: 4.575129, mean_q: 8.697819, mean_eps: 0.200000\n",
            " 1240/2000: episode: 70, duration: 0.194s, episode steps:  11, steps per second:  57, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 1.115109, mae: 4.629575, mean_q: 8.780353, mean_eps: 0.200000\n",
            " 1254/2000: episode: 71, duration: 0.297s, episode steps:  14, steps per second:  47, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 1.296615, mae: 4.702733, mean_q: 8.886038, mean_eps: 0.200000\n",
            " 1271/2000: episode: 72, duration: 0.210s, episode steps:  17, steps per second:  81, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 1.276043, mae: 4.799971, mean_q: 9.015297, mean_eps: 0.200000\n",
            " 1279/2000: episode: 73, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.594403, mae: 4.854610, mean_q: 9.140503, mean_eps: 0.200000\n",
            " 1291/2000: episode: 74, duration: 0.150s, episode steps:  12, steps per second:  80, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 1.899055, mae: 4.892327, mean_q: 9.142580, mean_eps: 0.200000\n",
            " 1300/2000: episode: 75, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 2.396253, mae: 5.055702, mean_q: 9.374602, mean_eps: 0.200000\n",
            " 1311/2000: episode: 76, duration: 0.145s, episode steps:  11, steps per second:  76, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 2.233189, mae: 5.011671, mean_q: 9.206787, mean_eps: 0.200000\n",
            " 1327/2000: episode: 77, duration: 0.199s, episode steps:  16, steps per second:  80, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.188 [0.000, 1.000],  loss: 1.629394, mae: 4.972987, mean_q: 9.302064, mean_eps: 0.200000\n",
            " 1338/2000: episode: 78, duration: 0.192s, episode steps:  11, steps per second:  57, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 2.372165, mae: 5.111377, mean_q: 9.500704, mean_eps: 0.200000\n",
            " 1349/2000: episode: 79, duration: 0.204s, episode steps:  11, steps per second:  54, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.086707, mae: 5.136796, mean_q: 9.625984, mean_eps: 0.200000\n",
            " 1358/2000: episode: 80, duration: 0.136s, episode steps:   9, steps per second:  66, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.426696, mae: 5.140136, mean_q: 9.711855, mean_eps: 0.200000\n",
            " 1368/2000: episode: 81, duration: 0.179s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.083196, mae: 5.212393, mean_q: 9.676547, mean_eps: 0.200000\n",
            " 1379/2000: episode: 82, duration: 0.210s, episode steps:  11, steps per second:  52, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 2.253923, mae: 5.216745, mean_q: 9.647386, mean_eps: 0.200000\n",
            " 1390/2000: episode: 83, duration: 0.167s, episode steps:  11, steps per second:  66, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 2.214410, mae: 5.251764, mean_q: 9.762350, mean_eps: 0.200000\n",
            " 1399/2000: episode: 84, duration: 0.131s, episode steps:   9, steps per second:  69, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.796810, mae: 5.310365, mean_q: 9.992132, mean_eps: 0.200000\n",
            " 1408/2000: episode: 85, duration: 0.127s, episode steps:   9, steps per second:  71, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 2.773472, mae: 5.472165, mean_q: 10.191018, mean_eps: 0.200000\n",
            " 1416/2000: episode: 86, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 3.518426, mae: 5.498236, mean_q: 10.016832, mean_eps: 0.200000\n",
            " 1427/2000: episode: 87, duration: 0.134s, episode steps:  11, steps per second:  82, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 3.646658, mae: 5.561256, mean_q: 9.980270, mean_eps: 0.200000\n",
            " 1438/2000: episode: 88, duration: 0.141s, episode steps:  11, steps per second:  78, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 2.786410, mae: 5.529806, mean_q: 9.963594, mean_eps: 0.200000\n",
            " 1452/2000: episode: 89, duration: 0.190s, episode steps:  14, steps per second:  74, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 2.166981, mae: 5.423243, mean_q: 9.956988, mean_eps: 0.200000\n",
            " 1470/2000: episode: 90, duration: 0.252s, episode steps:  18, steps per second:  72, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 3.415735, mae: 5.649265, mean_q: 10.132037, mean_eps: 0.200000\n",
            " 1482/2000: episode: 91, duration: 0.165s, episode steps:  12, steps per second:  73, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 2.903966, mae: 5.591853, mean_q: 10.100474, mean_eps: 0.200000\n",
            " 1498/2000: episode: 92, duration: 0.198s, episode steps:  16, steps per second:  81, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 2.588380, mae: 5.618770, mean_q: 10.256416, mean_eps: 0.200000\n",
            " 1510/2000: episode: 93, duration: 0.149s, episode steps:  12, steps per second:  81, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 2.002577, mae: 5.588821, mean_q: 10.418835, mean_eps: 0.200000\n",
            " 1520/2000: episode: 94, duration: 0.134s, episode steps:  10, steps per second:  74, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 3.329927, mae: 5.730438, mean_q: 10.486454, mean_eps: 0.200000\n",
            " 1530/2000: episode: 95, duration: 0.135s, episode steps:  10, steps per second:  74, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 1.990607, mae: 5.613620, mean_q: 10.484213, mean_eps: 0.200000\n",
            " 1541/2000: episode: 96, duration: 0.145s, episode steps:  11, steps per second:  76, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 4.166582, mae: 5.910553, mean_q: 10.657000, mean_eps: 0.200000\n",
            " 1565/2000: episode: 97, duration: 0.309s, episode steps:  24, steps per second:  78, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.876578, mae: 5.818918, mean_q: 10.527646, mean_eps: 0.200000\n",
            " 1599/2000: episode: 98, duration: 0.444s, episode steps:  34, steps per second:  77, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 3.281600, mae: 5.884109, mean_q: 10.584533, mean_eps: 0.200000\n",
            " 1619/2000: episode: 99, duration: 0.260s, episode steps:  20, steps per second:  77, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.264492, mae: 5.913117, mean_q: 10.712034, mean_eps: 0.200000\n",
            " 1635/2000: episode: 100, duration: 0.202s, episode steps:  16, steps per second:  79, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.522635, mae: 5.863172, mean_q: 10.723749, mean_eps: 0.200000\n",
            " 1650/2000: episode: 101, duration: 0.189s, episode steps:  15, steps per second:  79, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 3.347624, mae: 6.003737, mean_q: 10.863968, mean_eps: 0.200000\n",
            " 1668/2000: episode: 102, duration: 0.221s, episode steps:  18, steps per second:  81, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 3.256568, mae: 5.994273, mean_q: 10.868850, mean_eps: 0.200000\n",
            " 1680/2000: episode: 103, duration: 0.151s, episode steps:  12, steps per second:  80, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 4.006192, mae: 6.087046, mean_q: 10.858679, mean_eps: 0.200000\n",
            " 1691/2000: episode: 104, duration: 0.165s, episode steps:  11, steps per second:  67, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 3.574475, mae: 6.078936, mean_q: 10.844148, mean_eps: 0.200000\n",
            " 1702/2000: episode: 105, duration: 0.161s, episode steps:  11, steps per second:  68, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 4.645473, mae: 6.195494, mean_q: 10.875956, mean_eps: 0.200000\n",
            " 1716/2000: episode: 106, duration: 0.225s, episode steps:  14, steps per second:  62, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 3.837772, mae: 6.064387, mean_q: 10.723358, mean_eps: 0.200000\n",
            " 1728/2000: episode: 107, duration: 0.228s, episode steps:  12, steps per second:  53, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 3.750779, mae: 6.092783, mean_q: 10.778248, mean_eps: 0.200000\n",
            " 1743/2000: episode: 108, duration: 0.289s, episode steps:  15, steps per second:  52, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 3.047955, mae: 6.064275, mean_q: 10.863603, mean_eps: 0.200000\n",
            " 1765/2000: episode: 109, duration: 0.387s, episode steps:  22, steps per second:  57, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.836753, mae: 6.063902, mean_q: 11.021248, mean_eps: 0.200000\n",
            " 1782/2000: episode: 110, duration: 0.301s, episode steps:  17, steps per second:  56, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.513440, mae: 6.066185, mean_q: 11.175306, mean_eps: 0.200000\n",
            " 1794/2000: episode: 111, duration: 0.228s, episode steps:  12, steps per second:  53, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 2.923676, mae: 6.143581, mean_q: 11.317560, mean_eps: 0.200000\n",
            " 1804/2000: episode: 112, duration: 0.188s, episode steps:  10, steps per second:  53, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 4.248616, mae: 6.245867, mean_q: 11.216942, mean_eps: 0.200000\n",
            " 1815/2000: episode: 113, duration: 0.205s, episode steps:  11, steps per second:  54, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 3.803688, mae: 6.251903, mean_q: 11.198321, mean_eps: 0.200000\n",
            " 1828/2000: episode: 114, duration: 0.165s, episode steps:  13, steps per second:  79, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 2.320123, mae: 6.104663, mean_q: 11.194500, mean_eps: 0.200000\n",
            " 1841/2000: episode: 115, duration: 0.161s, episode steps:  13, steps per second:  81, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 3.348012, mae: 6.232010, mean_q: 11.338559, mean_eps: 0.200000\n",
            " 1852/2000: episode: 116, duration: 0.144s, episode steps:  11, steps per second:  77, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 3.199270, mae: 6.238153, mean_q: 11.394917, mean_eps: 0.200000\n",
            " 1861/2000: episode: 117, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 2.694483, mae: 6.202222, mean_q: 11.431864, mean_eps: 0.200000\n",
            " 1872/2000: episode: 118, duration: 0.144s, episode steps:  11, steps per second:  76, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 3.080601, mae: 6.271128, mean_q: 11.473527, mean_eps: 0.200000\n",
            " 1885/2000: episode: 119, duration: 0.180s, episode steps:  13, steps per second:  72, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 3.294030, mae: 6.288364, mean_q: 11.490068, mean_eps: 0.200000\n",
            " 1902/2000: episode: 120, duration: 0.232s, episode steps:  17, steps per second:  73, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 3.138620, mae: 6.311239, mean_q: 11.517123, mean_eps: 0.200000\n",
            " 1920/2000: episode: 121, duration: 0.223s, episode steps:  18, steps per second:  81, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.028305, mae: 6.395246, mean_q: 11.456365, mean_eps: 0.200000\n",
            " 1936/2000: episode: 122, duration: 0.194s, episode steps:  16, steps per second:  82, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.655237, mae: 6.268836, mean_q: 11.387438, mean_eps: 0.200000\n",
            " 1960/2000: episode: 123, duration: 0.289s, episode steps:  24, steps per second:  83, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.217637, mae: 6.373848, mean_q: 11.678210, mean_eps: 0.200000\n",
            "done, took 38.035 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 93.000, steps: 93\n",
            "Episode 2: reward: 97.000, steps: 97\n",
            "Episode 3: reward: 94.000, steps: 94\n",
            "Episode 4: reward: 82.000, steps: 82\n",
            "Episode 5: reward: 102.000, steps: 102\n",
            "Episode 6: reward: 102.000, steps: 102\n",
            "Episode 7: reward: 104.000, steps: 104\n",
            "Episode 8: reward: 95.000, steps: 95\n",
            "Episode 9: reward: 90.000, steps: 90\n",
            "Episode 10: reward: 106.000, steps: 106\n",
            "Episode 11: reward: 102.000, steps: 102\n",
            "Episode 12: reward: 93.000, steps: 93\n",
            "Episode 13: reward: 96.000, steps: 96\n",
            "Episode 14: reward: 94.000, steps: 94\n",
            "Episode 15: reward: 82.000, steps: 82\n",
            "Episode 16: reward: 167.000, steps: 167\n",
            "Episode 17: reward: 94.000, steps: 94\n",
            "Episode 18: reward: 90.000, steps: 90\n",
            "Episode 19: reward: 106.000, steps: 106\n",
            "Episode 20: reward: 88.000, steps: 88\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb172685f90>"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    }
  ]
}