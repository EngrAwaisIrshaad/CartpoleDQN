{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPoleDQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Megacity1/CartpoleDQN/blob/main/Week%203%20Deep%20RL%202/CartPoleDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Deep Q Network (DQN) for CartPole Using Boltzmann Q Policy\n",
        "This exercise implements a DQN for CartPole using a Boltzmann Q policy for selecting the actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGsC7cJ5jNcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda73cbf-3020-40f4-e45e-7b87053448f1"
      },
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.24.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (13.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.44.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.10.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMIHLgQ3Z-lF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5a8608-456f-4127-ce8a-f551e5f4e4f3"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0AMLzq08ap0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b380e1-6c3d-4bce-8233-a8ac01deaf26"
      },
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "!pip install Adam\n",
        "#from keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Adam in /usr/local/lib/python3.7/dist-packages (0.0.0.dev0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll6bNdUm54WS"
      },
      "source": [
        "Implementation of DQN for CartPole, applying policy BoltzmannQPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCrPKNy40PC"
      },
      "source": [
        "##Implement DQN with BoltzmannGumbelQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efM9jkXr5A3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "21873ba7-f8ef-4f89-de6b-e71161a2a38f"
      },
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import BoltzmannGumbelQPolicy\n",
        "from rl.policy import LinearAnnealedPolicy\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n",
        "# setup experience replay buffer\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
        "                               attr='eps',            \n",
        "                               value_max=5.,\n",
        "                               value_min=.5, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=20)\n",
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "# add extra layers here\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=20,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy) \n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=8000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_49\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_47 (Flatten)        (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_95 (Dense)            (None, 32)                160       \n",
            "                                                                 \n",
            " dense_96 (Dense)            (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 226\n",
            "Trainable params: 226\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 8000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   10/8000: episode: 1, duration: 3.471s, episode steps:  10, steps per second:   3, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   43/8000: episode: 2, duration: 10.752s, episode steps:  33, steps per second:   3, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.554078, mae: 0.571355, mean_q: 0.111323, mean_eps: 4.291250\n",
            "   63/8000: episode: 3, duration: 0.346s, episode steps:  20, steps per second:  58, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.443952, mae: 0.534531, mean_q: 0.198704, mean_eps: 3.818750\n",
            "  119/8000: episode: 4, duration: 1.206s, episode steps:  56, steps per second:  46, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.329681, mae: 0.557252, mean_q: 0.460828, mean_eps: 2.963750\n",
            "  132/8000: episode: 5, duration: 0.279s, episode steps:  13, steps per second:  47, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.213958, mae: 0.614235, mean_q: 0.776670, mean_eps: 2.187500\n",
            "  142/8000: episode: 6, duration: 0.164s, episode steps:  10, steps per second:  61, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.168855, mae: 0.652696, mean_q: 0.938020, mean_eps: 1.928750\n",
            "  154/8000: episode: 7, duration: 0.245s, episode steps:  12, steps per second:  49, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.127946, mae: 0.687358, mean_q: 1.095032, mean_eps: 1.681250\n",
            "  172/8000: episode: 8, duration: 0.356s, episode steps:  18, steps per second:  51, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 0.107147, mae: 0.750422, mean_q: 1.281300, mean_eps: 1.343750\n",
            "  193/8000: episode: 9, duration: 0.349s, episode steps:  21, steps per second:  60, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.107758, mae: 0.817476, mean_q: 1.406366, mean_eps: 0.905000\n",
            "  217/8000: episode: 10, duration: 0.466s, episode steps:  24, steps per second:  51, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.094933, mae: 0.879176, mean_q: 1.558906, mean_eps: 0.526250\n",
            "  226/8000: episode: 11, duration: 0.216s, episode steps:   9, steps per second:  42, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.081771, mae: 0.941620, mean_q: 1.713215, mean_eps: 0.500000\n",
            "  237/8000: episode: 12, duration: 0.230s, episode steps:  11, steps per second:  48, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.100845, mae: 1.000760, mean_q: 1.802857, mean_eps: 0.500000\n",
            "  254/8000: episode: 13, duration: 0.317s, episode steps:  17, steps per second:  54, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.126203, mae: 1.066208, mean_q: 1.896423, mean_eps: 0.500000\n",
            "  265/8000: episode: 14, duration: 0.213s, episode steps:  11, steps per second:  52, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.107026, mae: 1.108966, mean_q: 1.965020, mean_eps: 0.500000\n",
            "  286/8000: episode: 15, duration: 0.393s, episode steps:  21, steps per second:  53, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.115593, mae: 1.174051, mean_q: 2.171941, mean_eps: 0.500000\n",
            "  296/8000: episode: 16, duration: 0.289s, episode steps:  10, steps per second:  35, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.120872, mae: 1.243638, mean_q: 2.336243, mean_eps: 0.500000\n",
            "  306/8000: episode: 17, duration: 0.293s, episode steps:  10, steps per second:  34, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.184900, mae: 1.320814, mean_q: 2.445969, mean_eps: 0.500000\n",
            "  315/8000: episode: 18, duration: 0.286s, episode steps:   9, steps per second:  31, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.134313, mae: 1.323815, mean_q: 2.429799, mean_eps: 0.500000\n",
            "  328/8000: episode: 19, duration: 0.387s, episode steps:  13, steps per second:  34, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.846 [0.000, 1.000],  loss: 0.173258, mae: 1.378768, mean_q: 2.575587, mean_eps: 0.500000\n",
            "  342/8000: episode: 20, duration: 0.361s, episode steps:  14, steps per second:  39, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.258081, mae: 1.451755, mean_q: 2.663418, mean_eps: 0.500000\n",
            "  352/8000: episode: 21, duration: 0.311s, episode steps:  10, steps per second:  32, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.228730, mae: 1.478087, mean_q: 2.629088, mean_eps: 0.500000\n",
            "  365/8000: episode: 22, duration: 0.344s, episode steps:  13, steps per second:  38, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.123285, mae: 1.479775, mean_q: 2.795063, mean_eps: 0.500000\n",
            "  375/8000: episode: 23, duration: 0.262s, episode steps:  10, steps per second:  38, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.153585, mae: 1.552129, mean_q: 3.038688, mean_eps: 0.500000\n",
            "  387/8000: episode: 24, duration: 0.334s, episode steps:  12, steps per second:  36, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.190745, mae: 1.623210, mean_q: 3.120770, mean_eps: 0.500000\n",
            "  397/8000: episode: 25, duration: 0.226s, episode steps:  10, steps per second:  44, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.180767, mae: 1.660515, mean_q: 3.165057, mean_eps: 0.500000\n",
            "  410/8000: episode: 26, duration: 0.301s, episode steps:  13, steps per second:  43, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.846 [0.000, 1.000],  loss: 0.240065, mae: 1.718514, mean_q: 3.250992, mean_eps: 0.500000\n",
            "  423/8000: episode: 27, duration: 0.372s, episode steps:  13, steps per second:  35, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.237727, mae: 1.754813, mean_q: 3.300614, mean_eps: 0.500000\n",
            "  436/8000: episode: 28, duration: 0.340s, episode steps:  13, steps per second:  38, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.192207, mae: 1.774376, mean_q: 3.443100, mean_eps: 0.500000\n",
            "  447/8000: episode: 29, duration: 0.323s, episode steps:  11, steps per second:  34, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.270020, mae: 1.855360, mean_q: 3.524688, mean_eps: 0.500000\n",
            "  458/8000: episode: 30, duration: 0.350s, episode steps:  11, steps per second:  31, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.228833, mae: 1.899255, mean_q: 3.563887, mean_eps: 0.500000\n",
            "  473/8000: episode: 31, duration: 0.423s, episode steps:  15, steps per second:  35, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 0.266250, mae: 1.942725, mean_q: 3.601620, mean_eps: 0.500000\n",
            "  484/8000: episode: 32, duration: 0.283s, episode steps:  11, steps per second:  39, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.294295, mae: 1.994754, mean_q: 3.705567, mean_eps: 0.500000\n",
            "  497/8000: episode: 33, duration: 0.275s, episode steps:  13, steps per second:  47, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.290475, mae: 2.043951, mean_q: 3.771757, mean_eps: 0.500000\n",
            "  507/8000: episode: 34, duration: 0.222s, episode steps:  10, steps per second:  45, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.305054, mae: 2.093574, mean_q: 3.839889, mean_eps: 0.500000\n",
            "  520/8000: episode: 35, duration: 0.289s, episode steps:  13, steps per second:  45, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.311307, mae: 2.129359, mean_q: 3.902585, mean_eps: 0.500000\n",
            "  529/8000: episode: 36, duration: 0.245s, episode steps:   9, steps per second:  37, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.297811, mae: 2.167029, mean_q: 3.949811, mean_eps: 0.500000\n",
            "  542/8000: episode: 37, duration: 0.375s, episode steps:  13, steps per second:  35, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.325296, mae: 2.197492, mean_q: 4.108254, mean_eps: 0.500000\n",
            "  557/8000: episode: 38, duration: 0.424s, episode steps:  15, steps per second:  35, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.294220, mae: 2.218613, mean_q: 4.182773, mean_eps: 0.500000\n",
            "  574/8000: episode: 39, duration: 0.463s, episode steps:  17, steps per second:  37, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.378504, mae: 2.311187, mean_q: 4.225516, mean_eps: 0.500000\n",
            "  587/8000: episode: 40, duration: 0.339s, episode steps:  13, steps per second:  38, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.283408, mae: 2.335314, mean_q: 4.258887, mean_eps: 0.500000\n",
            "  609/8000: episode: 41, duration: 0.389s, episode steps:  22, steps per second:  57, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.383646, mae: 2.429171, mean_q: 4.474068, mean_eps: 0.500000\n",
            "  639/8000: episode: 42, duration: 0.511s, episode steps:  30, steps per second:  59, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 0.294275, mae: 2.486949, mean_q: 4.618679, mean_eps: 0.500000\n",
            "  649/8000: episode: 43, duration: 0.236s, episode steps:  10, steps per second:  42, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.368066, mae: 2.553266, mean_q: 4.792148, mean_eps: 0.500000\n",
            "  659/8000: episode: 44, duration: 0.200s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.384830, mae: 2.582528, mean_q: 4.789184, mean_eps: 0.500000\n",
            "  672/8000: episode: 45, duration: 0.241s, episode steps:  13, steps per second:  54, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.353750, mae: 2.617370, mean_q: 4.812593, mean_eps: 0.500000\n",
            "  688/8000: episode: 46, duration: 0.296s, episode steps:  16, steps per second:  54, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.371669, mae: 2.678690, mean_q: 4.990218, mean_eps: 0.500000\n",
            "  715/8000: episode: 47, duration: 0.540s, episode steps:  27, steps per second:  50, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.420160, mae: 2.733334, mean_q: 5.033840, mean_eps: 0.500000\n",
            "  741/8000: episode: 48, duration: 0.498s, episode steps:  26, steps per second:  52, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.447567, mae: 2.833022, mean_q: 5.262584, mean_eps: 0.500000\n",
            "  886/8000: episode: 49, duration: 2.810s, episode steps: 145, steps per second:  52, episode reward: 145.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.418426, mae: 3.063812, mean_q: 5.736611, mean_eps: 0.500000\n",
            "  908/8000: episode: 50, duration: 0.632s, episode steps:  22, steps per second:  35, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.445731, mae: 3.324945, mean_q: 6.318944, mean_eps: 0.500000\n",
            " 1020/8000: episode: 51, duration: 2.420s, episode steps: 112, steps per second:  46, episode reward: 112.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 0.489402, mae: 3.617778, mean_q: 6.935340, mean_eps: 0.500000\n",
            " 1066/8000: episode: 52, duration: 1.126s, episode steps:  46, steps per second:  41, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 0.579252, mae: 3.953452, mean_q: 7.570109, mean_eps: 0.500000\n",
            " 1087/8000: episode: 53, duration: 0.540s, episode steps:  21, steps per second:  39, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.604715, mae: 4.070325, mean_q: 7.888361, mean_eps: 0.500000\n",
            " 1115/8000: episode: 54, duration: 0.753s, episode steps:  28, steps per second:  37, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.658989, mae: 4.176974, mean_q: 7.996961, mean_eps: 0.500000\n",
            " 1135/8000: episode: 55, duration: 0.557s, episode steps:  20, steps per second:  36, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.570814, mae: 4.278988, mean_q: 8.247115, mean_eps: 0.500000\n",
            " 1150/8000: episode: 56, duration: 0.413s, episode steps:  15, steps per second:  36, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.774082, mae: 4.391550, mean_q: 8.414588, mean_eps: 0.500000\n",
            " 1166/8000: episode: 57, duration: 0.413s, episode steps:  16, steps per second:  39, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.643192, mae: 4.399261, mean_q: 8.536163, mean_eps: 0.500000\n",
            " 1190/8000: episode: 58, duration: 0.614s, episode steps:  24, steps per second:  39, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.999359, mae: 4.523296, mean_q: 8.623393, mean_eps: 0.500000\n",
            " 1215/8000: episode: 59, duration: 0.636s, episode steps:  25, steps per second:  39, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 0.946653, mae: 4.665252, mean_q: 8.982222, mean_eps: 0.500000\n",
            " 1238/8000: episode: 60, duration: 0.615s, episode steps:  23, steps per second:  37, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 0.912032, mae: 4.678314, mean_q: 8.881889, mean_eps: 0.500000\n",
            " 1250/8000: episode: 61, duration: 0.225s, episode steps:  12, steps per second:  53, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.753952, mae: 4.816910, mean_q: 9.269919, mean_eps: 0.500000\n",
            " 1261/8000: episode: 62, duration: 0.183s, episode steps:  11, steps per second:  60, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 1.245540, mae: 4.880629, mean_q: 9.409927, mean_eps: 0.500000\n",
            " 1274/8000: episode: 63, duration: 0.267s, episode steps:  13, steps per second:  49, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 1.055062, mae: 4.935708, mean_q: 9.576185, mean_eps: 0.500000\n",
            " 1286/8000: episode: 64, duration: 0.260s, episode steps:  12, steps per second:  46, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.803522, mae: 4.925003, mean_q: 9.618798, mean_eps: 0.500000\n",
            " 1299/8000: episode: 65, duration: 0.342s, episode steps:  13, steps per second:  38, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.922578, mae: 4.999295, mean_q: 9.763605, mean_eps: 0.500000\n",
            " 1316/8000: episode: 66, duration: 0.437s, episode steps:  17, steps per second:  39, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 0.895553, mae: 5.066705, mean_q: 9.875965, mean_eps: 0.500000\n",
            " 1325/8000: episode: 67, duration: 0.279s, episode steps:   9, steps per second:  32, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 1.848966, mae: 5.153070, mean_q: 9.770375, mean_eps: 0.500000\n",
            " 1338/8000: episode: 68, duration: 0.343s, episode steps:  13, steps per second:  38, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.974942, mae: 5.173764, mean_q: 9.984076, mean_eps: 0.500000\n",
            " 1365/8000: episode: 69, duration: 0.776s, episode steps:  27, steps per second:  35, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.296 [0.000, 1.000],  loss: 1.688331, mae: 5.328619, mean_q: 10.080015, mean_eps: 0.500000\n",
            " 1380/8000: episode: 70, duration: 0.458s, episode steps:  15, steps per second:  33, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.559905, mae: 5.402124, mean_q: 10.397883, mean_eps: 0.500000\n",
            " 1405/8000: episode: 71, duration: 0.698s, episode steps:  25, steps per second:  36, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 1.870414, mae: 5.493102, mean_q: 10.376409, mean_eps: 0.500000\n",
            " 1440/8000: episode: 72, duration: 0.892s, episode steps:  35, steps per second:  39, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 1.798439, mae: 5.554018, mean_q: 10.433406, mean_eps: 0.500000\n",
            " 1454/8000: episode: 73, duration: 0.270s, episode steps:  14, steps per second:  52, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 2.362718, mae: 5.643909, mean_q: 10.416923, mean_eps: 0.500000\n",
            " 1471/8000: episode: 74, duration: 0.368s, episode steps:  17, steps per second:  46, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 1.524788, mae: 5.683261, mean_q: 10.772053, mean_eps: 0.500000\n",
            " 1484/8000: episode: 75, duration: 0.265s, episode steps:  13, steps per second:  49, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 2.160610, mae: 5.787737, mean_q: 10.947158, mean_eps: 0.500000\n",
            " 1504/8000: episode: 76, duration: 0.533s, episode steps:  20, steps per second:  38, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 1.856586, mae: 5.784541, mean_q: 10.937036, mean_eps: 0.500000\n",
            " 1523/8000: episode: 77, duration: 0.543s, episode steps:  19, steps per second:  35, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 1.656343, mae: 5.805008, mean_q: 11.005763, mean_eps: 0.500000\n",
            " 1534/8000: episode: 78, duration: 0.202s, episode steps:  11, steps per second:  54, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 1.993633, mae: 5.894107, mean_q: 11.116895, mean_eps: 0.500000\n",
            " 1546/8000: episode: 79, duration: 0.250s, episode steps:  12, steps per second:  48, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.374028, mae: 5.895301, mean_q: 11.277791, mean_eps: 0.500000\n",
            " 1569/8000: episode: 80, duration: 0.500s, episode steps:  23, steps per second:  46, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.225757, mae: 5.972905, mean_q: 11.535099, mean_eps: 0.500000\n",
            " 1584/8000: episode: 81, duration: 0.439s, episode steps:  15, steps per second:  34, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 2.231229, mae: 6.018090, mean_q: 11.340109, mean_eps: 0.500000\n",
            " 1607/8000: episode: 82, duration: 0.665s, episode steps:  23, steps per second:  35, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.348 [0.000, 1.000],  loss: 2.067595, mae: 6.075240, mean_q: 11.415123, mean_eps: 0.500000\n",
            " 1621/8000: episode: 83, duration: 0.375s, episode steps:  14, steps per second:  37, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 2.073282, mae: 6.100394, mean_q: 11.523027, mean_eps: 0.500000\n",
            " 1633/8000: episode: 84, duration: 0.412s, episode steps:  12, steps per second:  29, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 2.607040, mae: 6.226842, mean_q: 11.596173, mean_eps: 0.500000\n",
            " 1644/8000: episode: 85, duration: 0.314s, episode steps:  11, steps per second:  35, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 1.682781, mae: 6.169395, mean_q: 11.674713, mean_eps: 0.500000\n",
            " 1653/8000: episode: 86, duration: 0.728s, episode steps:   9, steps per second:  12, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 1.793831, mae: 6.179877, mean_q: 11.735933, mean_eps: 0.500000\n",
            " 1670/8000: episode: 87, duration: 0.618s, episode steps:  17, steps per second:  28, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1.513748, mae: 6.227090, mean_q: 11.931574, mean_eps: 0.500000\n",
            " 1696/8000: episode: 88, duration: 0.748s, episode steps:  26, steps per second:  35, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 2.062719, mae: 6.332249, mean_q: 12.045939, mean_eps: 0.500000\n",
            " 1723/8000: episode: 89, duration: 0.738s, episode steps:  27, steps per second:  37, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 2.137370, mae: 6.353633, mean_q: 11.929288, mean_eps: 0.500000\n",
            " 1740/8000: episode: 90, duration: 0.501s, episode steps:  17, steps per second:  34, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 2.446058, mae: 6.445390, mean_q: 12.166215, mean_eps: 0.500000\n",
            " 1755/8000: episode: 91, duration: 0.447s, episode steps:  15, steps per second:  34, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.264712, mae: 6.454019, mean_q: 12.134109, mean_eps: 0.500000\n",
            " 1775/8000: episode: 92, duration: 0.588s, episode steps:  20, steps per second:  34, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.834104, mae: 6.478831, mean_q: 12.343480, mean_eps: 0.500000\n",
            " 1851/8000: episode: 93, duration: 2.110s, episode steps:  76, steps per second:  36, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 2.629992, mae: 6.604086, mean_q: 12.320904, mean_eps: 0.500000\n",
            " 1870/8000: episode: 94, duration: 0.534s, episode steps:  19, steps per second:  36, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 2.326908, mae: 6.677788, mean_q: 12.628107, mean_eps: 0.500000\n",
            " 1900/8000: episode: 95, duration: 0.831s, episode steps:  30, steps per second:  36, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.612825, mae: 6.741147, mean_q: 12.649998, mean_eps: 0.500000\n",
            " 1973/8000: episode: 96, duration: 1.362s, episode steps:  73, steps per second:  54, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 2.125896, mae: 6.856611, mean_q: 13.029410, mean_eps: 0.500000\n",
            " 1989/8000: episode: 97, duration: 0.440s, episode steps:  16, steps per second:  36, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.563066, mae: 7.025699, mean_q: 13.333196, mean_eps: 0.500000\n",
            " 2016/8000: episode: 98, duration: 0.708s, episode steps:  27, steps per second:  38, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 2.347095, mae: 7.043842, mean_q: 13.424732, mean_eps: 0.500000\n",
            " 2062/8000: episode: 99, duration: 1.064s, episode steps:  46, steps per second:  43, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 2.563202, mae: 7.111556, mean_q: 13.489686, mean_eps: 0.500000\n",
            " 2101/8000: episode: 100, duration: 0.738s, episode steps:  39, steps per second:  53, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 2.518753, mae: 7.239783, mean_q: 13.785475, mean_eps: 0.500000\n",
            " 2171/8000: episode: 101, duration: 1.456s, episode steps:  70, steps per second:  48, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 2.225479, mae: 7.326867, mean_q: 13.982835, mean_eps: 0.500000\n",
            " 2264/8000: episode: 102, duration: 2.005s, episode steps:  93, steps per second:  46, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 2.425748, mae: 7.545020, mean_q: 14.441797, mean_eps: 0.500000\n",
            " 2288/8000: episode: 103, duration: 0.638s, episode steps:  24, steps per second:  38, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.414971, mae: 7.764538, mean_q: 14.614968, mean_eps: 0.500000\n",
            " 2334/8000: episode: 104, duration: 1.197s, episode steps:  46, steps per second:  38, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 2.932257, mae: 7.719282, mean_q: 14.654372, mean_eps: 0.500000\n",
            " 2399/8000: episode: 105, duration: 1.444s, episode steps:  65, steps per second:  45, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 2.515479, mae: 7.880134, mean_q: 15.045444, mean_eps: 0.500000\n",
            " 2432/8000: episode: 106, duration: 0.675s, episode steps:  33, steps per second:  49, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 3.356620, mae: 7.975450, mean_q: 15.074604, mean_eps: 0.500000\n",
            " 2469/8000: episode: 107, duration: 0.999s, episode steps:  37, steps per second:  37, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 2.685984, mae: 8.004339, mean_q: 15.397948, mean_eps: 0.500000\n",
            " 2576/8000: episode: 108, duration: 2.506s, episode steps: 107, steps per second:  43, episode reward: 107.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 2.673171, mae: 8.224616, mean_q: 15.812669, mean_eps: 0.500000\n",
            " 2622/8000: episode: 109, duration: 1.285s, episode steps:  46, steps per second:  36, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 2.652308, mae: 8.461184, mean_q: 16.326048, mean_eps: 0.500000\n",
            " 2678/8000: episode: 110, duration: 1.629s, episode steps:  56, steps per second:  34, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 3.057020, mae: 8.550663, mean_q: 16.351874, mean_eps: 0.500000\n",
            " 2716/8000: episode: 111, duration: 0.890s, episode steps:  38, steps per second:  43, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2.832807, mae: 8.683322, mean_q: 16.715113, mean_eps: 0.500000\n",
            " 2853/8000: episode: 112, duration: 3.092s, episode steps: 137, steps per second:  44, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 2.949749, mae: 8.826142, mean_q: 16.967974, mean_eps: 0.500000\n",
            " 3024/8000: episode: 113, duration: 4.048s, episode steps: 171, steps per second:  42, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 3.384868, mae: 9.186997, mean_q: 17.649949, mean_eps: 0.500000\n",
            " 3086/8000: episode: 114, duration: 1.302s, episode steps:  62, steps per second:  48, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.342199, mae: 9.411644, mean_q: 18.135617, mean_eps: 0.500000\n",
            " 3149/8000: episode: 115, duration: 1.527s, episode steps:  63, steps per second:  41, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 3.390428, mae: 9.587312, mean_q: 18.499213, mean_eps: 0.500000\n",
            " 3187/8000: episode: 116, duration: 0.654s, episode steps:  38, steps per second:  58, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.579598, mae: 9.705274, mean_q: 18.562748, mean_eps: 0.500000\n",
            " 3226/8000: episode: 117, duration: 0.716s, episode steps:  39, steps per second:  54, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 4.092345, mae: 9.656872, mean_q: 18.464421, mean_eps: 0.500000\n",
            " 3341/8000: episode: 118, duration: 2.434s, episode steps: 115, steps per second:  47, episode reward: 115.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 3.628137, mae: 9.912555, mean_q: 19.068667, mean_eps: 0.500000\n",
            " 3420/8000: episode: 119, duration: 1.553s, episode steps:  79, steps per second:  51, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 3.055682, mae: 10.123779, mean_q: 19.623299, mean_eps: 0.500000\n",
            " 3446/8000: episode: 120, duration: 0.543s, episode steps:  26, steps per second:  48, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 2.901319, mae: 10.106780, mean_q: 19.540729, mean_eps: 0.500000\n",
            " 3518/8000: episode: 121, duration: 1.331s, episode steps:  72, steps per second:  54, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 3.409003, mae: 10.310870, mean_q: 19.908930, mean_eps: 0.500000\n",
            " 3542/8000: episode: 122, duration: 0.438s, episode steps:  24, steps per second:  55, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.216359, mae: 10.377035, mean_q: 20.255492, mean_eps: 0.500000\n",
            " 3664/8000: episode: 123, duration: 2.372s, episode steps: 122, steps per second:  51, episode reward: 122.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 3.526499, mae: 10.553550, mean_q: 20.479691, mean_eps: 0.500000\n",
            " 3732/8000: episode: 124, duration: 1.881s, episode steps:  68, steps per second:  36, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 3.244011, mae: 10.781722, mean_q: 20.932275, mean_eps: 0.500000\n",
            " 3815/8000: episode: 125, duration: 1.892s, episode steps:  83, steps per second:  44, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 3.637296, mae: 10.881628, mean_q: 21.153597, mean_eps: 0.500000\n",
            " 3918/8000: episode: 126, duration: 1.939s, episode steps: 103, steps per second:  53, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 2.724499, mae: 11.101853, mean_q: 21.746227, mean_eps: 0.500000\n",
            " 4014/8000: episode: 127, duration: 2.189s, episode steps:  96, steps per second:  44, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 3.609891, mae: 11.373647, mean_q: 22.199349, mean_eps: 0.500000\n",
            " 4047/8000: episode: 128, duration: 1.104s, episode steps:  33, steps per second:  30, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.612477, mae: 11.514914, mean_q: 22.459970, mean_eps: 0.500000\n",
            " 4169/8000: episode: 129, duration: 3.102s, episode steps: 122, steps per second:  39, episode reward: 122.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 3.649892, mae: 11.709429, mean_q: 22.889769, mean_eps: 0.500000\n",
            " 4192/8000: episode: 130, duration: 0.576s, episode steps:  23, steps per second:  40, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 3.286457, mae: 11.842108, mean_q: 23.269033, mean_eps: 0.500000\n",
            " 4256/8000: episode: 131, duration: 1.530s, episode steps:  64, steps per second:  42, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.886291, mae: 11.918767, mean_q: 23.272874, mean_eps: 0.500000\n",
            " 4322/8000: episode: 132, duration: 1.805s, episode steps:  66, steps per second:  37, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.032439, mae: 12.149635, mean_q: 23.820881, mean_eps: 0.500000\n",
            " 4453/8000: episode: 133, duration: 3.055s, episode steps: 131, steps per second:  43, episode reward: 131.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 3.858082, mae: 12.361360, mean_q: 24.202063, mean_eps: 0.500000\n",
            " 4587/8000: episode: 134, duration: 2.888s, episode steps: 134, steps per second:  46, episode reward: 134.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 3.979309, mae: 12.681784, mean_q: 24.970898, mean_eps: 0.500000\n",
            " 4675/8000: episode: 135, duration: 2.020s, episode steps:  88, steps per second:  44, episode reward: 88.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 4.585627, mae: 12.928678, mean_q: 25.387072, mean_eps: 0.500000\n",
            " 4772/8000: episode: 136, duration: 2.262s, episode steps:  97, steps per second:  43, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 4.406670, mae: 13.138291, mean_q: 25.837794, mean_eps: 0.500000\n",
            " 4903/8000: episode: 137, duration: 2.970s, episode steps: 131, steps per second:  44, episode reward: 131.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 4.994717, mae: 13.385590, mean_q: 26.293329, mean_eps: 0.500000\n",
            " 5061/8000: episode: 138, duration: 3.439s, episode steps: 158, steps per second:  46, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.456 [0.000, 1.000],  loss: 4.800592, mae: 13.726468, mean_q: 27.014157, mean_eps: 0.500000\n",
            " 5080/8000: episode: 139, duration: 0.418s, episode steps:  19, steps per second:  45, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 4.720210, mae: 13.839292, mean_q: 27.279528, mean_eps: 0.500000\n",
            " 5132/8000: episode: 140, duration: 1.183s, episode steps:  52, steps per second:  44, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.611760, mae: 14.039271, mean_q: 27.585707, mean_eps: 0.500000\n",
            " 5274/8000: episode: 141, duration: 2.598s, episode steps: 142, steps per second:  55, episode reward: 142.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 4.771774, mae: 14.189575, mean_q: 27.959022, mean_eps: 0.500000\n",
            " 5338/8000: episode: 142, duration: 1.560s, episode steps:  64, steps per second:  41, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 4.912127, mae: 14.353708, mean_q: 28.354863, mean_eps: 0.500000\n",
            " 5451/8000: episode: 143, duration: 2.813s, episode steps: 113, steps per second:  40, episode reward: 113.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 5.353738, mae: 14.641095, mean_q: 28.836832, mean_eps: 0.500000\n",
            " 5592/8000: episode: 144, duration: 3.465s, episode steps: 141, steps per second:  41, episode reward: 141.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 5.107412, mae: 14.863650, mean_q: 29.385151, mean_eps: 0.500000\n",
            " 5639/8000: episode: 145, duration: 1.289s, episode steps:  47, steps per second:  36, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 5.008373, mae: 15.171365, mean_q: 30.085385, mean_eps: 0.500000\n",
            " 5829/8000: episode: 146, duration: 5.522s, episode steps: 190, steps per second:  34, episode reward: 190.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 5.803647, mae: 15.427624, mean_q: 30.468984, mean_eps: 0.500000\n",
            " 5911/8000: episode: 147, duration: 2.315s, episode steps:  82, steps per second:  35, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 6.190715, mae: 15.671861, mean_q: 30.956003, mean_eps: 0.500000\n",
            " 6111/8000: episode: 148, duration: 4.920s, episode steps: 200, steps per second:  41, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 4.763515, mae: 16.038188, mean_q: 31.941690, mean_eps: 0.500000\n",
            " 6253/8000: episode: 149, duration: 3.830s, episode steps: 142, steps per second:  37, episode reward: 142.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.822128, mae: 16.437596, mean_q: 32.581933, mean_eps: 0.500000\n",
            " 6391/8000: episode: 150, duration: 3.490s, episode steps: 138, steps per second:  40, episode reward: 138.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 5.776086, mae: 16.778645, mean_q: 33.352382, mean_eps: 0.500000\n",
            " 6442/8000: episode: 151, duration: 1.188s, episode steps:  51, steps per second:  43, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.451 [0.000, 1.000],  loss: 6.442624, mae: 17.033998, mean_q: 33.761136, mean_eps: 0.500000\n",
            " 6585/8000: episode: 152, duration: 3.243s, episode steps: 143, steps per second:  44, episode reward: 143.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 5.957673, mae: 17.236370, mean_q: 34.366520, mean_eps: 0.500000\n",
            " 6785/8000: episode: 153, duration: 4.268s, episode steps: 200, steps per second:  47, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.878185, mae: 17.650761, mean_q: 35.149156, mean_eps: 0.500000\n",
            " 6985/8000: episode: 154, duration: 3.948s, episode steps: 200, steps per second:  51, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 6.477140, mae: 18.105932, mean_q: 36.120307, mean_eps: 0.500000\n",
            " 7185/8000: episode: 155, duration: 4.728s, episode steps: 200, steps per second:  42, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 7.014077, mae: 18.652891, mean_q: 37.257574, mean_eps: 0.500000\n",
            " 7385/8000: episode: 156, duration: 4.670s, episode steps: 200, steps per second:  43, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 7.445869, mae: 19.159844, mean_q: 38.301768, mean_eps: 0.500000\n",
            " 7472/8000: episode: 157, duration: 1.810s, episode steps:  87, steps per second:  48, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 7.019931, mae: 19.559246, mean_q: 39.163911, mean_eps: 0.500000\n",
            " 7545/8000: episode: 158, duration: 1.864s, episode steps:  73, steps per second:  39, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 7.379682, mae: 19.792521, mean_q: 39.647688, mean_eps: 0.500000\n",
            " 7745/8000: episode: 159, duration: 4.046s, episode steps: 200, steps per second:  49, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 6.349385, mae: 20.131483, mean_q: 40.423190, mean_eps: 0.500000\n",
            " 7774/8000: episode: 160, duration: 0.561s, episode steps:  29, steps per second:  52, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.621 [0.000, 1.000],  loss: 8.704894, mae: 20.464289, mean_q: 41.020914, mean_eps: 0.500000\n",
            " 7974/8000: episode: 161, duration: 4.595s, episode steps: 200, steps per second:  44, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 8.930377, mae: 20.670398, mean_q: 41.266830, mean_eps: 0.500000\n",
            "done, took 200.479 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 200.000, steps: 200\n",
            "Episode 6: reward: 200.000, steps: 200\n",
            "Episode 7: reward: 200.000, steps: 200\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 200.000, steps: 200\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 200.000, steps: 200\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 200.000, steps: 200\n",
            "Episode 15: reward: 200.000, steps: 200\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 200.000, steps: 200\n",
            "Episode 18: reward: 200.000, steps: 200\n",
            "Episode 19: reward: 200.000, steps: 200\n",
            "Episode 20: reward: 200.000, steps: 200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb16fb0d650>"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    }
  ]
}