{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPoleDQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Megacity1/CartpoleDQN/blob/main/Week%203%20Deep%20RL%202/CartPoleDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Deep Q Network (DQN) for CartPole Using Boltzmann Q Policy\n",
        "This exercise implements a DQN for CartPole using a Boltzmann Q policy for selecting the actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGsC7cJ5jNcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda73cbf-3020-40f4-e45e-7b87053448f1"
      },
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.24.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (13.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.44.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.10.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMIHLgQ3Z-lF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5a8608-456f-4127-ce8a-f551e5f4e4f3"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0AMLzq08ap0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b380e1-6c3d-4bce-8233-a8ac01deaf26"
      },
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "!pip install Adam\n",
        "#from keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Adam in /usr/local/lib/python3.7/dist-packages (0.0.0.dev0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll6bNdUm54WS"
      },
      "source": [
        "Implementation of DQN for CartPole, applying policy BoltzmannQPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCrPKNy40PC"
      },
      "source": [
        "##Implement DQN with BoltzmannGumbelQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efM9jkXr5A3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ac8ee802-cf57-4cd5-fb6a-66405bfc9062"
      },
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import BoltzmannGumbelQPolicy\n",
        "from rl.policy import LinearAnnealedPolicy\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n",
        "# setup experience replay buffer\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
        "                               attr='eps',            \n",
        "                               value_max=5.,\n",
        "                               value_min=.5, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=200)\n",
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "# add extra layers here\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=20,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy) \n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=8000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_46\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_44 (Flatten)        (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_89 (Dense)            (None, 16)                80        \n",
            "                                                                 \n",
            " dense_90 (Dense)            (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 8000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   11/8000: episode: 1, duration: 4.027s, episode steps:  11, steps per second:   3, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   42/8000: episode: 2, duration: 14.159s, episode steps:  31, steps per second:   2, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.613 [0.000, 1.000],  loss: 0.552772, mae: 0.561661, mean_q: 0.009814, mean_eps: 4.302500\n",
            "   55/8000: episode: 3, duration: 0.408s, episode steps:  13, steps per second:  32, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.473358, mae: 0.518824, mean_q: 0.071483, mean_eps: 3.920000\n",
            "   74/8000: episode: 4, duration: 0.544s, episode steps:  19, steps per second:  35, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 0.436298, mae: 0.526123, mean_q: 0.164949, mean_eps: 3.560000\n",
            "   91/8000: episode: 5, duration: 0.478s, episode steps:  17, steps per second:  36, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.401781, mae: 0.537781, mean_q: 0.229113, mean_eps: 3.155000\n",
            "  105/8000: episode: 6, duration: 0.430s, episode steps:  14, steps per second:  33, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.379369, mae: 0.564876, mean_q: 0.311506, mean_eps: 2.806250\n",
            "  124/8000: episode: 7, duration: 0.528s, episode steps:  19, steps per second:  36, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.351620, mae: 0.578449, mean_q: 0.392738, mean_eps: 2.435000\n",
            "  138/8000: episode: 8, duration: 0.358s, episode steps:  14, steps per second:  39, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.327522, mae: 0.593352, mean_q: 0.449801, mean_eps: 2.063750\n",
            "  156/8000: episode: 9, duration: 0.487s, episode steps:  18, steps per second:  37, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 0.306793, mae: 0.620617, mean_q: 0.550649, mean_eps: 1.703750\n",
            "  176/8000: episode: 10, duration: 0.543s, episode steps:  20, steps per second:  37, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 0.276156, mae: 0.643539, mean_q: 0.675751, mean_eps: 1.276250\n",
            "  200/8000: episode: 11, duration: 1.101s, episode steps:  24, steps per second:  22, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.292 [0.000, 1.000],  loss: 0.256375, mae: 0.681062, mean_q: 0.776354, mean_eps: 0.781250\n",
            "  217/8000: episode: 12, duration: 0.624s, episode steps:  17, steps per second:  27, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: 0.234297, mae: 0.737841, mean_q: 0.966972, mean_eps: 0.500000\n",
            "  230/8000: episode: 13, duration: 0.369s, episode steps:  13, steps per second:  35, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.208736, mae: 0.774034, mean_q: 1.103334, mean_eps: 0.500000\n",
            "  240/8000: episode: 14, duration: 0.317s, episode steps:  10, steps per second:  32, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.216584, mae: 0.815829, mean_q: 1.217554, mean_eps: 0.500000\n",
            "  249/8000: episode: 15, duration: 0.295s, episode steps:   9, steps per second:  31, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.213640, mae: 0.864535, mean_q: 1.319024, mean_eps: 0.500000\n",
            "  265/8000: episode: 16, duration: 0.442s, episode steps:  16, steps per second:  36, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.197266, mae: 0.903016, mean_q: 1.421426, mean_eps: 0.500000\n",
            "  277/8000: episode: 17, duration: 0.402s, episode steps:  12, steps per second:  30, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.210228, mae: 0.963506, mean_q: 1.556423, mean_eps: 0.500000\n",
            "  298/8000: episode: 18, duration: 0.600s, episode steps:  21, steps per second:  35, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.381 [0.000, 1.000],  loss: 0.186047, mae: 1.003112, mean_q: 1.650902, mean_eps: 0.500000\n",
            "  308/8000: episode: 19, duration: 0.296s, episode steps:  10, steps per second:  34, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.218580, mae: 1.087005, mean_q: 1.857272, mean_eps: 0.500000\n",
            "  332/8000: episode: 20, duration: 0.662s, episode steps:  24, steps per second:  36, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.232366, mae: 1.161789, mean_q: 2.044477, mean_eps: 0.500000\n",
            "  346/8000: episode: 21, duration: 0.412s, episode steps:  14, steps per second:  34, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.263942, mae: 1.243550, mean_q: 2.182643, mean_eps: 0.500000\n",
            "  356/8000: episode: 22, duration: 0.301s, episode steps:  10, steps per second:  33, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.256358, mae: 1.235664, mean_q: 2.200195, mean_eps: 0.500000\n",
            "  367/8000: episode: 23, duration: 0.286s, episode steps:  11, steps per second:  38, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.304923, mae: 1.316944, mean_q: 2.364116, mean_eps: 0.500000\n",
            "  388/8000: episode: 24, duration: 0.585s, episode steps:  21, steps per second:  36, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.282060, mae: 1.357676, mean_q: 2.435040, mean_eps: 0.500000\n",
            "  400/8000: episode: 25, duration: 0.328s, episode steps:  12, steps per second:  37, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.306447, mae: 1.471630, mean_q: 2.713329, mean_eps: 0.500000\n",
            "  411/8000: episode: 26, duration: 0.287s, episode steps:  11, steps per second:  38, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.414428, mae: 1.525322, mean_q: 2.767723, mean_eps: 0.500000\n",
            "  423/8000: episode: 27, duration: 0.341s, episode steps:  12, steps per second:  35, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.295470, mae: 1.524807, mean_q: 2.807016, mean_eps: 0.500000\n",
            "  432/8000: episode: 28, duration: 0.235s, episode steps:   9, steps per second:  38, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.444307, mae: 1.632834, mean_q: 3.023374, mean_eps: 0.500000\n",
            "  444/8000: episode: 29, duration: 0.234s, episode steps:  12, steps per second:  51, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.450771, mae: 1.643404, mean_q: 3.034448, mean_eps: 0.500000\n",
            "  453/8000: episode: 30, duration: 0.162s, episode steps:   9, steps per second:  56, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.362598, mae: 1.661580, mean_q: 3.073919, mean_eps: 0.500000\n",
            "  462/8000: episode: 31, duration: 0.166s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.374900, mae: 1.658770, mean_q: 3.086311, mean_eps: 0.500000\n",
            "  470/8000: episode: 32, duration: 0.134s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.125 [0.000, 1.000],  loss: 0.514306, mae: 1.758113, mean_q: 3.274284, mean_eps: 0.500000\n",
            "  499/8000: episode: 33, duration: 0.431s, episode steps:  29, steps per second:  67, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.345 [0.000, 1.000],  loss: 0.351743, mae: 1.795071, mean_q: 3.407965, mean_eps: 0.500000\n",
            "  525/8000: episode: 34, duration: 0.682s, episode steps:  26, steps per second:  38, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.525119, mae: 1.925709, mean_q: 3.663238, mean_eps: 0.500000\n",
            "  537/8000: episode: 35, duration: 0.303s, episode steps:  12, steps per second:  40, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.514716, mae: 2.021623, mean_q: 3.776264, mean_eps: 0.500000\n",
            "  548/8000: episode: 36, duration: 0.261s, episode steps:  11, steps per second:  42, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.378601, mae: 1.980377, mean_q: 3.870676, mean_eps: 0.500000\n",
            "  568/8000: episode: 37, duration: 0.494s, episode steps:  20, steps per second:  40, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.468892, mae: 2.083745, mean_q: 3.987011, mean_eps: 0.500000\n",
            "  582/8000: episode: 38, duration: 0.328s, episode steps:  14, steps per second:  43, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.458900, mae: 2.137952, mean_q: 4.056448, mean_eps: 0.500000\n",
            "  593/8000: episode: 39, duration: 0.286s, episode steps:  11, steps per second:  38, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.563238, mae: 2.210857, mean_q: 4.199425, mean_eps: 0.500000\n",
            "  614/8000: episode: 40, duration: 0.483s, episode steps:  21, steps per second:  44, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.556014, mae: 2.279749, mean_q: 4.360636, mean_eps: 0.500000\n",
            "  636/8000: episode: 41, duration: 0.425s, episode steps:  22, steps per second:  52, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.499899, mae: 2.327046, mean_q: 4.451870, mean_eps: 0.500000\n",
            "  646/8000: episode: 42, duration: 0.181s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.489863, mae: 2.370203, mean_q: 4.632952, mean_eps: 0.500000\n",
            "  666/8000: episode: 43, duration: 0.315s, episode steps:  20, steps per second:  63, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.683670, mae: 2.464861, mean_q: 4.705684, mean_eps: 0.500000\n",
            "  676/8000: episode: 44, duration: 0.159s, episode steps:  10, steps per second:  63, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.565517, mae: 2.513454, mean_q: 4.790587, mean_eps: 0.500000\n",
            "  690/8000: episode: 45, duration: 0.206s, episode steps:  14, steps per second:  68, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.143 [0.000, 1.000],  loss: 0.684435, mae: 2.562776, mean_q: 4.855328, mean_eps: 0.500000\n",
            "  703/8000: episode: 46, duration: 0.207s, episode steps:  13, steps per second:  63, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.538634, mae: 2.594377, mean_q: 4.954551, mean_eps: 0.500000\n",
            "  712/8000: episode: 47, duration: 0.142s, episode steps:   9, steps per second:  63, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.501811, mae: 2.631722, mean_q: 5.057765, mean_eps: 0.500000\n",
            "  724/8000: episode: 48, duration: 0.194s, episode steps:  12, steps per second:  62, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.684508, mae: 2.712989, mean_q: 5.143840, mean_eps: 0.500000\n",
            "  736/8000: episode: 49, duration: 0.220s, episode steps:  12, steps per second:  55, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.549446, mae: 2.722865, mean_q: 5.220583, mean_eps: 0.500000\n",
            "  749/8000: episode: 50, duration: 0.214s, episode steps:  13, steps per second:  61, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.154 [0.000, 1.000],  loss: 0.585175, mae: 2.777808, mean_q: 5.293869, mean_eps: 0.500000\n",
            "  766/8000: episode: 51, duration: 0.400s, episode steps:  17, steps per second:  42, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 0.605289, mae: 2.831734, mean_q: 5.389682, mean_eps: 0.500000\n",
            "  779/8000: episode: 52, duration: 0.355s, episode steps:  13, steps per second:  37, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.492477, mae: 2.850669, mean_q: 5.519790, mean_eps: 0.500000\n",
            "  790/8000: episode: 53, duration: 0.308s, episode steps:  11, steps per second:  36, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.632893, mae: 2.917693, mean_q: 5.571397, mean_eps: 0.500000\n",
            "  801/8000: episode: 54, duration: 0.274s, episode steps:  11, steps per second:  40, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.512700, mae: 2.943009, mean_q: 5.638402, mean_eps: 0.500000\n",
            "  820/8000: episode: 55, duration: 0.490s, episode steps:  19, steps per second:  39, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 0.580128, mae: 3.005654, mean_q: 5.686560, mean_eps: 0.500000\n",
            "  831/8000: episode: 56, duration: 0.284s, episode steps:  11, steps per second:  39, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.766136, mae: 3.069599, mean_q: 5.746314, mean_eps: 0.500000\n",
            "  846/8000: episode: 57, duration: 0.397s, episode steps:  15, steps per second:  38, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.544480, mae: 3.049691, mean_q: 5.812460, mean_eps: 0.500000\n",
            "  855/8000: episode: 58, duration: 0.219s, episode steps:   9, steps per second:  41, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.628316, mae: 3.090106, mean_q: 5.908218, mean_eps: 0.500000\n",
            "  867/8000: episode: 59, duration: 0.300s, episode steps:  12, steps per second:  40, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.567397, mae: 3.152165, mean_q: 6.068284, mean_eps: 0.500000\n",
            "  892/8000: episode: 60, duration: 0.401s, episode steps:  25, steps per second:  62, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.588147, mae: 3.215198, mean_q: 6.005119, mean_eps: 0.500000\n",
            "  906/8000: episode: 61, duration: 0.228s, episode steps:  14, steps per second:  61, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.523836, mae: 3.253174, mean_q: 6.140788, mean_eps: 0.500000\n",
            "  922/8000: episode: 62, duration: 0.257s, episode steps:  16, steps per second:  62, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 0.554677, mae: 3.304292, mean_q: 6.327491, mean_eps: 0.500000\n",
            "  934/8000: episode: 63, duration: 0.209s, episode steps:  12, steps per second:  57, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.481355, mae: 3.342628, mean_q: 6.391203, mean_eps: 0.500000\n",
            "  951/8000: episode: 64, duration: 0.411s, episode steps:  17, steps per second:  41, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.235 [0.000, 1.000],  loss: 0.517651, mae: 3.389800, mean_q: 6.386929, mean_eps: 0.500000\n",
            "  964/8000: episode: 65, duration: 0.366s, episode steps:  13, steps per second:  35, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.550218, mae: 3.439490, mean_q: 6.430776, mean_eps: 0.500000\n",
            "  980/8000: episode: 66, duration: 0.365s, episode steps:  16, steps per second:  44, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.548428, mae: 3.500757, mean_q: 6.613409, mean_eps: 0.500000\n",
            "  991/8000: episode: 67, duration: 0.216s, episode steps:  11, steps per second:  51, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.410721, mae: 3.497584, mean_q: 6.627829, mean_eps: 0.500000\n",
            " 1008/8000: episode: 68, duration: 0.285s, episode steps:  17, steps per second:  60, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.617402, mae: 3.540554, mean_q: 6.646024, mean_eps: 0.500000\n",
            " 1026/8000: episode: 69, duration: 0.265s, episode steps:  18, steps per second:  68, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.539016, mae: 3.580287, mean_q: 6.757519, mean_eps: 0.500000\n",
            " 1038/8000: episode: 70, duration: 0.239s, episode steps:  12, steps per second:  50, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.584759, mae: 3.623042, mean_q: 6.822669, mean_eps: 0.500000\n",
            " 1050/8000: episode: 71, duration: 0.240s, episode steps:  12, steps per second:  50, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.569334, mae: 3.659934, mean_q: 6.868190, mean_eps: 0.500000\n",
            " 1069/8000: episode: 72, duration: 0.403s, episode steps:  19, steps per second:  47, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 0.431365, mae: 3.690555, mean_q: 7.016202, mean_eps: 0.500000\n",
            " 1079/8000: episode: 73, duration: 0.182s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.777001, mae: 3.709677, mean_q: 6.941114, mean_eps: 0.500000\n",
            " 1119/8000: episode: 74, duration: 0.831s, episode steps:  40, steps per second:  48, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 0.549467, mae: 3.796996, mean_q: 7.166203, mean_eps: 0.500000\n",
            " 1144/8000: episode: 75, duration: 0.686s, episode steps:  25, steps per second:  36, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 1.243461, mae: 3.925789, mean_q: 7.222218, mean_eps: 0.500000\n",
            " 1159/8000: episode: 76, duration: 0.428s, episode steps:  15, steps per second:  35, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.676525, mae: 3.886957, mean_q: 7.290924, mean_eps: 0.500000\n",
            " 1178/8000: episode: 77, duration: 0.371s, episode steps:  19, steps per second:  51, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.620951, mae: 3.941972, mean_q: 7.421289, mean_eps: 0.500000\n",
            " 1203/8000: episode: 78, duration: 0.415s, episode steps:  25, steps per second:  60, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 0.532403, mae: 3.971904, mean_q: 7.521791, mean_eps: 0.500000\n",
            " 1214/8000: episode: 79, duration: 0.195s, episode steps:  11, steps per second:  57, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.953108, mae: 4.098194, mean_q: 7.693158, mean_eps: 0.500000\n",
            " 1229/8000: episode: 80, duration: 0.237s, episode steps:  15, steps per second:  63, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.358534, mae: 4.059447, mean_q: 7.713686, mean_eps: 0.500000\n",
            " 1247/8000: episode: 81, duration: 0.300s, episode steps:  18, steps per second:  60, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.931955, mae: 4.115977, mean_q: 7.723806, mean_eps: 0.500000\n",
            " 1260/8000: episode: 82, duration: 0.238s, episode steps:  13, steps per second:  55, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.449585, mae: 4.082780, mean_q: 7.696291, mean_eps: 0.500000\n",
            " 1275/8000: episode: 83, duration: 0.263s, episode steps:  15, steps per second:  57, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.531781, mae: 4.185643, mean_q: 7.925038, mean_eps: 0.500000\n",
            " 1328/8000: episode: 84, duration: 1.190s, episode steps:  53, steps per second:  45, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.660 [0.000, 1.000],  loss: 0.808647, mae: 4.262148, mean_q: 8.027406, mean_eps: 0.500000\n",
            " 1337/8000: episode: 85, duration: 0.236s, episode steps:   9, steps per second:  38, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.556842, mae: 4.243300, mean_q: 8.037391, mean_eps: 0.500000\n",
            " 1349/8000: episode: 86, duration: 0.317s, episode steps:  12, steps per second:  38, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.924966, mae: 4.401945, mean_q: 8.324501, mean_eps: 0.500000\n",
            " 1360/8000: episode: 87, duration: 0.315s, episode steps:  11, steps per second:  35, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 1.213130, mae: 4.480566, mean_q: 8.323892, mean_eps: 0.500000\n",
            " 1381/8000: episode: 88, duration: 0.564s, episode steps:  21, steps per second:  37, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.522855, mae: 4.406936, mean_q: 8.326274, mean_eps: 0.500000\n",
            " 1393/8000: episode: 89, duration: 0.358s, episode steps:  12, steps per second:  34, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.962803, mae: 4.461822, mean_q: 8.457940, mean_eps: 0.500000\n",
            " 1404/8000: episode: 90, duration: 0.322s, episode steps:  11, steps per second:  34, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.658987, mae: 4.501172, mean_q: 8.563259, mean_eps: 0.500000\n",
            " 1447/8000: episode: 91, duration: 0.991s, episode steps:  43, steps per second:  43, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.628 [0.000, 1.000],  loss: 0.978075, mae: 4.616844, mean_q: 8.780211, mean_eps: 0.500000\n",
            " 1459/8000: episode: 92, duration: 0.191s, episode steps:  12, steps per second:  63, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.959179, mae: 4.682740, mean_q: 8.954683, mean_eps: 0.500000\n",
            " 1475/8000: episode: 93, duration: 0.297s, episode steps:  16, steps per second:  54, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 1.032169, mae: 4.795690, mean_q: 9.158376, mean_eps: 0.500000\n",
            " 1487/8000: episode: 94, duration: 0.254s, episode steps:  12, steps per second:  47, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 1.109284, mae: 4.798533, mean_q: 9.128671, mean_eps: 0.500000\n",
            " 1496/8000: episode: 95, duration: 0.195s, episode steps:   9, steps per second:  46, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 1.295697, mae: 4.846478, mean_q: 9.158266, mean_eps: 0.500000\n",
            " 1507/8000: episode: 96, duration: 0.287s, episode steps:  11, steps per second:  38, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 1.266025, mae: 4.914117, mean_q: 9.334545, mean_eps: 0.500000\n",
            " 1527/8000: episode: 97, duration: 0.514s, episode steps:  20, steps per second:  39, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.133158, mae: 4.897988, mean_q: 9.255441, mean_eps: 0.500000\n",
            " 1537/8000: episode: 98, duration: 0.296s, episode steps:  10, steps per second:  34, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 1.534620, mae: 5.011503, mean_q: 9.479995, mean_eps: 0.500000\n",
            " 1558/8000: episode: 99, duration: 0.373s, episode steps:  21, steps per second:  56, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.906111, mae: 5.115355, mean_q: 9.572415, mean_eps: 0.500000\n",
            " 1570/8000: episode: 100, duration: 0.262s, episode steps:  12, steps per second:  46, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 1.548957, mae: 5.079545, mean_q: 9.568703, mean_eps: 0.500000\n",
            " 1594/8000: episode: 101, duration: 0.439s, episode steps:  24, steps per second:  55, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.708 [0.000, 1.000],  loss: 1.139089, mae: 5.129355, mean_q: 9.727926, mean_eps: 0.500000\n",
            " 1642/8000: episode: 102, duration: 0.895s, episode steps:  48, steps per second:  54, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 1.233713, mae: 5.190090, mean_q: 9.842553, mean_eps: 0.500000\n",
            " 1657/8000: episode: 103, duration: 0.276s, episode steps:  15, steps per second:  54, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 2.206208, mae: 5.309196, mean_q: 9.944388, mean_eps: 0.500000\n",
            " 1669/8000: episode: 104, duration: 0.235s, episode steps:  12, steps per second:  51, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 2.353843, mae: 5.378617, mean_q: 10.014456, mean_eps: 0.500000\n",
            " 1690/8000: episode: 105, duration: 0.370s, episode steps:  21, steps per second:  57, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.896845, mae: 5.393793, mean_q: 10.042255, mean_eps: 0.500000\n",
            " 1709/8000: episode: 106, duration: 0.385s, episode steps:  19, steps per second:  49, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 1.958818, mae: 5.415093, mean_q: 10.049150, mean_eps: 0.500000\n",
            " 1744/8000: episode: 107, duration: 0.960s, episode steps:  35, steps per second:  36, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 1.875238, mae: 5.406031, mean_q: 10.032653, mean_eps: 0.500000\n",
            " 1753/8000: episode: 108, duration: 0.263s, episode steps:   9, steps per second:  34, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.987378, mae: 5.368466, mean_q: 10.183512, mean_eps: 0.500000\n",
            " 1781/8000: episode: 109, duration: 0.695s, episode steps:  28, steps per second:  40, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 1.223439, mae: 5.385392, mean_q: 10.182474, mean_eps: 0.500000\n",
            " 1796/8000: episode: 110, duration: 0.320s, episode steps:  15, steps per second:  47, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.459111, mae: 5.577689, mean_q: 10.637673, mean_eps: 0.500000\n",
            " 1835/8000: episode: 111, duration: 0.805s, episode steps:  39, steps per second:  48, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1.448328, mae: 5.569444, mean_q: 10.516868, mean_eps: 0.500000\n",
            " 1892/8000: episode: 112, duration: 0.920s, episode steps:  57, steps per second:  62, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.614 [0.000, 1.000],  loss: 1.505703, mae: 5.651450, mean_q: 10.645995, mean_eps: 0.500000\n",
            " 1912/8000: episode: 113, duration: 0.293s, episode steps:  20, steps per second:  68, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 1.729410, mae: 5.661633, mean_q: 10.568726, mean_eps: 0.500000\n",
            " 1924/8000: episode: 114, duration: 0.196s, episode steps:  12, steps per second:  61, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 1.814840, mae: 5.760471, mean_q: 10.783806, mean_eps: 0.500000\n",
            " 1936/8000: episode: 115, duration: 0.211s, episode steps:  12, steps per second:  57, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 1.855754, mae: 5.696986, mean_q: 10.683859, mean_eps: 0.500000\n",
            " 1957/8000: episode: 116, duration: 0.420s, episode steps:  21, steps per second:  50, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.360217, mae: 5.816493, mean_q: 11.031944, mean_eps: 0.500000\n",
            " 1971/8000: episode: 117, duration: 0.369s, episode steps:  14, steps per second:  38, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.857 [0.000, 1.000],  loss: 1.286503, mae: 5.786845, mean_q: 10.946182, mean_eps: 0.500000\n",
            " 2003/8000: episode: 118, duration: 0.712s, episode steps:  32, steps per second:  45, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 1.527288, mae: 5.876895, mean_q: 11.140423, mean_eps: 0.500000\n",
            " 2055/8000: episode: 119, duration: 0.968s, episode steps:  52, steps per second:  54, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1.493032, mae: 5.889110, mean_q: 11.172755, mean_eps: 0.500000\n",
            " 2080/8000: episode: 120, duration: 0.692s, episode steps:  25, steps per second:  36, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 1.466197, mae: 6.010251, mean_q: 11.359791, mean_eps: 0.500000\n",
            " 2151/8000: episode: 121, duration: 1.811s, episode steps:  71, steps per second:  39, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 1.371622, mae: 6.072749, mean_q: 11.559842, mean_eps: 0.500000\n",
            " 2170/8000: episode: 122, duration: 0.470s, episode steps:  19, steps per second:  40, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.682885, mae: 6.184221, mean_q: 11.664184, mean_eps: 0.500000\n",
            " 2190/8000: episode: 123, duration: 0.321s, episode steps:  20, steps per second:  62, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 1.713534, mae: 6.199190, mean_q: 11.703710, mean_eps: 0.500000\n",
            " 2214/8000: episode: 124, duration: 0.387s, episode steps:  24, steps per second:  62, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 1.664017, mae: 6.257294, mean_q: 11.814370, mean_eps: 0.500000\n",
            " 2232/8000: episode: 125, duration: 0.323s, episode steps:  18, steps per second:  56, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 1.468839, mae: 6.324197, mean_q: 11.984008, mean_eps: 0.500000\n",
            " 2268/8000: episode: 126, duration: 0.913s, episode steps:  36, steps per second:  39, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.203572, mae: 6.286520, mean_q: 11.968063, mean_eps: 0.500000\n",
            " 2294/8000: episode: 127, duration: 0.632s, episode steps:  26, steps per second:  41, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.117892, mae: 6.396900, mean_q: 12.309107, mean_eps: 0.500000\n",
            " 2375/8000: episode: 128, duration: 1.908s, episode steps:  81, steps per second:  42, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.291449, mae: 6.454877, mean_q: 12.339543, mean_eps: 0.500000\n",
            " 2456/8000: episode: 129, duration: 1.972s, episode steps:  81, steps per second:  41, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 1.211941, mae: 6.572407, mean_q: 12.588273, mean_eps: 0.500000\n",
            " 2498/8000: episode: 130, duration: 1.083s, episode steps:  42, steps per second:  39, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.588680, mae: 6.730537, mean_q: 12.860620, mean_eps: 0.500000\n",
            " 2547/8000: episode: 131, duration: 0.962s, episode steps:  49, steps per second:  51, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 1.330534, mae: 6.751094, mean_q: 12.932219, mean_eps: 0.500000\n",
            " 2610/8000: episode: 132, duration: 1.003s, episode steps:  63, steps per second:  63, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 1.467268, mae: 6.959941, mean_q: 13.334037, mean_eps: 0.500000\n",
            " 2642/8000: episode: 133, duration: 0.550s, episode steps:  32, steps per second:  58, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 1.819245, mae: 7.130520, mean_q: 13.629243, mean_eps: 0.500000\n",
            " 2688/8000: episode: 134, duration: 0.704s, episode steps:  46, steps per second:  65, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 1.800606, mae: 7.167329, mean_q: 13.755034, mean_eps: 0.500000\n",
            " 2740/8000: episode: 135, duration: 1.150s, episode steps:  52, steps per second:  45, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.860876, mae: 7.226513, mean_q: 13.865346, mean_eps: 0.500000\n",
            " 2826/8000: episode: 136, duration: 1.849s, episode steps:  86, steps per second:  47, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.461038, mae: 7.270271, mean_q: 13.997471, mean_eps: 0.500000\n",
            " 2895/8000: episode: 137, duration: 1.545s, episode steps:  69, steps per second:  45, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 1.288637, mae: 7.504538, mean_q: 14.556084, mean_eps: 0.500000\n",
            " 2962/8000: episode: 138, duration: 1.332s, episode steps:  67, steps per second:  50, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.370994, mae: 7.702361, mean_q: 15.001311, mean_eps: 0.500000\n",
            " 3006/8000: episode: 139, duration: 1.241s, episode steps:  44, steps per second:  35, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 1.451705, mae: 7.682912, mean_q: 14.812983, mean_eps: 0.500000\n",
            " 3044/8000: episode: 140, duration: 1.039s, episode steps:  38, steps per second:  37, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 1.909149, mae: 7.817285, mean_q: 15.050830, mean_eps: 0.500000\n",
            " 3135/8000: episode: 141, duration: 1.714s, episode steps:  91, steps per second:  53, episode reward: 91.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 1.503732, mae: 7.988216, mean_q: 15.503798, mean_eps: 0.500000\n",
            " 3193/8000: episode: 142, duration: 1.336s, episode steps:  58, steps per second:  43, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 2.227845, mae: 8.142004, mean_q: 15.690227, mean_eps: 0.500000\n",
            " 3244/8000: episode: 143, duration: 1.193s, episode steps:  51, steps per second:  43, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.451 [0.000, 1.000],  loss: 1.637775, mae: 8.190398, mean_q: 15.859980, mean_eps: 0.500000\n",
            " 3292/8000: episode: 144, duration: 0.958s, episode steps:  48, steps per second:  50, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 2.255431, mae: 8.416108, mean_q: 16.233961, mean_eps: 0.500000\n",
            " 3327/8000: episode: 145, duration: 0.806s, episode steps:  35, steps per second:  43, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 1.573391, mae: 8.501826, mean_q: 16.503788, mean_eps: 0.500000\n",
            " 3370/8000: episode: 146, duration: 0.899s, episode steps:  43, steps per second:  48, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 1.918425, mae: 8.629835, mean_q: 16.763550, mean_eps: 0.500000\n",
            " 3428/8000: episode: 147, duration: 0.941s, episode steps:  58, steps per second:  62, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 2.273367, mae: 8.671181, mean_q: 16.829276, mean_eps: 0.500000\n",
            " 3490/8000: episode: 148, duration: 1.018s, episode steps:  62, steps per second:  61, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.137259, mae: 8.787793, mean_q: 17.060909, mean_eps: 0.500000\n",
            " 3566/8000: episode: 149, duration: 1.715s, episode steps:  76, steps per second:  44, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.888808, mae: 8.961494, mean_q: 17.391885, mean_eps: 0.500000\n",
            " 3662/8000: episode: 150, duration: 2.132s, episode steps:  96, steps per second:  45, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 1.581009, mae: 9.163906, mean_q: 17.970297, mean_eps: 0.500000\n",
            " 3734/8000: episode: 151, duration: 1.255s, episode steps:  72, steps per second:  57, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 1.980700, mae: 9.415938, mean_q: 18.444761, mean_eps: 0.500000\n",
            " 3774/8000: episode: 152, duration: 0.711s, episode steps:  40, steps per second:  56, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.939600, mae: 9.393804, mean_q: 18.408502, mean_eps: 0.500000\n",
            " 3918/8000: episode: 153, duration: 2.560s, episode steps: 144, steps per second:  56, episode reward: 144.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 2.462423, mae: 9.775563, mean_q: 19.127069, mean_eps: 0.500000\n",
            " 3976/8000: episode: 154, duration: 1.274s, episode steps:  58, steps per second:  46, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 2.941374, mae: 10.008620, mean_q: 19.586696, mean_eps: 0.500000\n",
            " 4097/8000: episode: 155, duration: 2.318s, episode steps: 121, steps per second:  52, episode reward: 121.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 2.809477, mae: 10.184392, mean_q: 19.904014, mean_eps: 0.500000\n",
            " 4133/8000: episode: 156, duration: 0.801s, episode steps:  36, steps per second:  45, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 2.667166, mae: 10.325695, mean_q: 20.228336, mean_eps: 0.500000\n",
            " 4194/8000: episode: 157, duration: 1.083s, episode steps:  61, steps per second:  56, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 3.188660, mae: 10.433237, mean_q: 20.320512, mean_eps: 0.500000\n",
            " 4248/8000: episode: 158, duration: 1.803s, episode steps:  54, steps per second:  30, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 2.531975, mae: 10.560546, mean_q: 20.643232, mean_eps: 0.500000\n",
            " 4310/8000: episode: 159, duration: 1.942s, episode steps:  62, steps per second:  32, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.630918, mae: 10.772689, mean_q: 21.051962, mean_eps: 0.500000\n",
            " 4394/8000: episode: 160, duration: 1.668s, episode steps:  84, steps per second:  50, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 2.181657, mae: 10.768217, mean_q: 21.259892, mean_eps: 0.500000\n",
            " 4422/8000: episode: 161, duration: 0.500s, episode steps:  28, steps per second:  56, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.201676, mae: 11.119514, mean_q: 21.729291, mean_eps: 0.500000\n",
            " 4514/8000: episode: 162, duration: 1.859s, episode steps:  92, steps per second:  50, episode reward: 92.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 3.623774, mae: 11.128446, mean_q: 21.825590, mean_eps: 0.500000\n",
            " 4550/8000: episode: 163, duration: 0.577s, episode steps:  36, steps per second:  62, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 2.930987, mae: 11.282893, mean_q: 22.176227, mean_eps: 0.500000\n",
            " 4610/8000: episode: 164, duration: 0.995s, episode steps:  60, steps per second:  60, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 3.554927, mae: 11.353850, mean_q: 22.307867, mean_eps: 0.500000\n",
            " 4667/8000: episode: 165, duration: 0.917s, episode steps:  57, steps per second:  62, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 5.203132, mae: 11.515595, mean_q: 22.488533, mean_eps: 0.500000\n",
            " 4761/8000: episode: 166, duration: 2.281s, episode steps:  94, steps per second:  41, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 2.864384, mae: 11.633669, mean_q: 22.961281, mean_eps: 0.500000\n",
            " 4840/8000: episode: 167, duration: 1.675s, episode steps:  79, steps per second:  47, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 2.814310, mae: 11.906462, mean_q: 23.429453, mean_eps: 0.500000\n",
            " 4957/8000: episode: 168, duration: 1.924s, episode steps: 117, steps per second:  61, episode reward: 117.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 3.383366, mae: 12.077512, mean_q: 23.794297, mean_eps: 0.500000\n",
            " 5017/8000: episode: 169, duration: 0.964s, episode steps:  60, steps per second:  62, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 5.060006, mae: 12.298344, mean_q: 24.150220, mean_eps: 0.500000\n",
            " 5132/8000: episode: 170, duration: 1.980s, episode steps: 115, steps per second:  58, episode reward: 115.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 3.352312, mae: 12.378886, mean_q: 24.456706, mean_eps: 0.500000\n",
            " 5224/8000: episode: 171, duration: 1.756s, episode steps:  92, steps per second:  52, episode reward: 92.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 3.296544, mae: 12.722040, mean_q: 25.170173, mean_eps: 0.500000\n",
            " 5283/8000: episode: 172, duration: 1.785s, episode steps:  59, steps per second:  33, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 3.781384, mae: 12.760811, mean_q: 25.201407, mean_eps: 0.500000\n",
            " 5297/8000: episode: 173, duration: 0.308s, episode steps:  14, steps per second:  45, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.151876, mae: 12.788924, mean_q: 25.203814, mean_eps: 0.500000\n",
            " 5402/8000: episode: 174, duration: 2.552s, episode steps: 105, steps per second:  41, episode reward: 105.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 3.082635, mae: 13.061684, mean_q: 25.847467, mean_eps: 0.500000\n",
            " 5445/8000: episode: 175, duration: 0.919s, episode steps:  43, steps per second:  47, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 4.303926, mae: 13.171508, mean_q: 26.034007, mean_eps: 0.500000\n",
            " 5596/8000: episode: 176, duration: 3.483s, episode steps: 151, steps per second:  43, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 4.342716, mae: 13.293008, mean_q: 26.304207, mean_eps: 0.500000\n",
            " 5722/8000: episode: 177, duration: 3.055s, episode steps: 126, steps per second:  41, episode reward: 126.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 3.804729, mae: 13.646330, mean_q: 26.997252, mean_eps: 0.500000\n",
            " 5792/8000: episode: 178, duration: 1.485s, episode steps:  70, steps per second:  47, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 3.249521, mae: 13.968120, mean_q: 27.757557, mean_eps: 0.500000\n",
            " 5884/8000: episode: 179, duration: 2.436s, episode steps:  92, steps per second:  38, episode reward: 92.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.554 [0.000, 1.000],  loss: 4.537950, mae: 14.066290, mean_q: 27.821527, mean_eps: 0.500000\n",
            " 5905/8000: episode: 180, duration: 0.454s, episode steps:  21, steps per second:  46, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 7.586402, mae: 14.183565, mean_q: 28.005804, mean_eps: 0.500000\n",
            " 5933/8000: episode: 181, duration: 0.567s, episode steps:  28, steps per second:  49, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 3.158623, mae: 14.194546, mean_q: 28.259953, mean_eps: 0.500000\n",
            " 5952/8000: episode: 182, duration: 0.352s, episode steps:  19, steps per second:  54, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 1.333202, mae: 14.127122, mean_q: 28.406747, mean_eps: 0.500000\n",
            " 6066/8000: episode: 183, duration: 2.004s, episode steps: 114, steps per second:  57, episode reward: 114.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.800108, mae: 14.399689, mean_q: 28.425338, mean_eps: 0.500000\n",
            " 6162/8000: episode: 184, duration: 1.764s, episode steps:  96, steps per second:  54, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 4.402103, mae: 14.641714, mean_q: 29.012225, mean_eps: 0.500000\n",
            " 6303/8000: episode: 185, duration: 3.690s, episode steps: 141, steps per second:  38, episode reward: 141.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 4.339196, mae: 14.922566, mean_q: 29.599013, mean_eps: 0.500000\n",
            " 6413/8000: episode: 186, duration: 2.337s, episode steps: 110, steps per second:  47, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.105759, mae: 15.129131, mean_q: 30.013888, mean_eps: 0.500000\n",
            " 6466/8000: episode: 187, duration: 1.550s, episode steps:  53, steps per second:  34, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.453 [0.000, 1.000],  loss: 4.754487, mae: 15.323497, mean_q: 30.513235, mean_eps: 0.500000\n",
            " 6488/8000: episode: 188, duration: 0.723s, episode steps:  22, steps per second:  30, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.394853, mae: 15.432224, mean_q: 30.804875, mean_eps: 0.500000\n",
            " 6560/8000: episode: 189, duration: 1.425s, episode steps:  72, steps per second:  51, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 5.287622, mae: 15.588759, mean_q: 30.892790, mean_eps: 0.500000\n",
            " 6663/8000: episode: 190, duration: 1.861s, episode steps: 103, steps per second:  55, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  loss: 5.583631, mae: 15.613194, mean_q: 30.915296, mean_eps: 0.500000\n",
            " 6702/8000: episode: 191, duration: 0.953s, episode steps:  39, steps per second:  41, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 4.854326, mae: 15.760375, mean_q: 31.304312, mean_eps: 0.500000\n",
            " 6836/8000: episode: 192, duration: 2.557s, episode steps: 134, steps per second:  52, episode reward: 134.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 5.380269, mae: 15.968728, mean_q: 31.698839, mean_eps: 0.500000\n",
            " 6865/8000: episode: 193, duration: 0.432s, episode steps:  29, steps per second:  67, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 6.597020, mae: 15.851373, mean_q: 31.349829, mean_eps: 0.500000\n",
            " 6908/8000: episode: 194, duration: 0.654s, episode steps:  43, steps per second:  66, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 5.802247, mae: 16.250957, mean_q: 32.219812, mean_eps: 0.500000\n",
            " 6957/8000: episode: 195, duration: 1.054s, episode steps:  49, steps per second:  46, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 7.647055, mae: 16.298075, mean_q: 32.150374, mean_eps: 0.500000\n",
            " 7026/8000: episode: 196, duration: 1.587s, episode steps:  69, steps per second:  43, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 4.660565, mae: 16.482252, mean_q: 32.718085, mean_eps: 0.500000\n",
            " 7055/8000: episode: 197, duration: 0.484s, episode steps:  29, steps per second:  60, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.586 [0.000, 1.000],  loss: 3.963427, mae: 16.493319, mean_q: 32.820682, mean_eps: 0.500000\n",
            " 7167/8000: episode: 198, duration: 2.867s, episode steps: 112, steps per second:  39, episode reward: 112.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.807549, mae: 16.650972, mean_q: 32.951966, mean_eps: 0.500000\n",
            " 7196/8000: episode: 199, duration: 0.691s, episode steps:  29, steps per second:  42, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.414 [0.000, 1.000],  loss: 6.530134, mae: 16.905708, mean_q: 33.497087, mean_eps: 0.500000\n",
            " 7251/8000: episode: 200, duration: 0.892s, episode steps:  55, steps per second:  62, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 5.748670, mae: 16.605335, mean_q: 32.944310, mean_eps: 0.500000\n",
            " 7310/8000: episode: 201, duration: 1.072s, episode steps:  59, steps per second:  55, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.441 [0.000, 1.000],  loss: 3.535741, mae: 16.943108, mean_q: 33.810491, mean_eps: 0.500000\n",
            " 7332/8000: episode: 202, duration: 0.360s, episode steps:  22, steps per second:  61, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 5.921909, mae: 17.196861, mean_q: 34.163733, mean_eps: 0.500000\n",
            " 7460/8000: episode: 203, duration: 2.866s, episode steps: 128, steps per second:  45, episode reward: 128.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 5.471659, mae: 17.262458, mean_q: 34.327953, mean_eps: 0.500000\n",
            " 7575/8000: episode: 204, duration: 2.479s, episode steps: 115, steps per second:  46, episode reward: 115.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 6.624914, mae: 17.440929, mean_q: 34.570645, mean_eps: 0.500000\n",
            " 7643/8000: episode: 205, duration: 1.095s, episode steps:  68, steps per second:  62, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.660629, mae: 17.627129, mean_q: 34.929191, mean_eps: 0.500000\n",
            " 7804/8000: episode: 206, duration: 2.699s, episode steps: 161, steps per second:  60, episode reward: 161.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 6.123967, mae: 17.775552, mean_q: 35.314300, mean_eps: 0.500000\n",
            " 7945/8000: episode: 207, duration: 3.292s, episode steps: 141, steps per second:  43, episode reward: 141.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 5.247855, mae: 18.058829, mean_q: 36.037255, mean_eps: 0.500000\n",
            "done, took 189.266 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 162.000, steps: 162\n",
            "Episode 4: reward: 196.000, steps: 196\n",
            "Episode 5: reward: 200.000, steps: 200\n",
            "Episode 6: reward: 200.000, steps: 200\n",
            "Episode 7: reward: 156.000, steps: 156\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 192.000, steps: 192\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 178.000, steps: 178\n",
            "Episode 12: reward: 188.000, steps: 188\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 158.000, steps: 158\n",
            "Episode 15: reward: 186.000, steps: 186\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 200.000, steps: 200\n",
            "Episode 18: reward: 180.000, steps: 180\n",
            "Episode 19: reward: 154.000, steps: 154\n",
            "Episode 20: reward: 187.000, steps: 187\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb17067ee90>"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    }
  ]
}