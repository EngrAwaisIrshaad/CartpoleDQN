{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPoleDQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Megacity1/CartpoleDQN/blob/main/Week%203%20Deep%20RL%202/CartPoleDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Deep Q Network (DQN) for CartPole Using Boltzmann Q Policy\n",
        "This exercise implements a DQN for CartPole using a Boltzmann Q policy for selecting the actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGsC7cJ5jNcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda73cbf-3020-40f4-e45e-7b87053448f1"
      },
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.24.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (13.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.44.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.10.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMIHLgQ3Z-lF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5a8608-456f-4127-ce8a-f551e5f4e4f3"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0AMLzq08ap0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b380e1-6c3d-4bce-8233-a8ac01deaf26"
      },
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "!pip install Adam\n",
        "#from keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Adam in /usr/local/lib/python3.7/dist-packages (0.0.0.dev0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll6bNdUm54WS"
      },
      "source": [
        "Implementation of DQN for CartPole, applying policy BoltzmannQPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCrPKNy40PC"
      },
      "source": [
        "##Implement DQN with BoltzmannGumbelQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efM9jkXr5A3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3024c135-7898-4147-8745-0e7a0ad5828f"
      },
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import BoltzmannGumbelQPolicy\n",
        "from rl.policy import LinearAnnealedPolicy\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n",
        "# setup experience replay buffer\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
        "                               attr='eps',            \n",
        "                               value_max=5.,\n",
        "                               value_min=.5, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=20)\n",
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "# add extra layers here\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=20,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy) \n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=8000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_53\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_51 (Flatten)        (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_103 (Dense)           (None, 16)                80        \n",
            "                                                                 \n",
            " dense_104 (Dense)           (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 8000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   21/8000: episode: 1, duration: 15.117s, episode steps:  21, steps per second:   1, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   34/8000: episode: 2, duration: 0.423s, episode steps:  13, steps per second:  31, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.645355, mae: 0.651954, mean_q: 0.316936, mean_eps: 0.500000\n",
            "   43/8000: episode: 3, duration: 0.205s, episode steps:   9, steps per second:  44, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.599904, mae: 0.656075, mean_q: 0.351273, mean_eps: 0.500000\n",
            "   58/8000: episode: 4, duration: 0.296s, episode steps:  15, steps per second:  51, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.507050, mae: 0.619159, mean_q: 0.422958, mean_eps: 0.500000\n",
            "   70/8000: episode: 5, duration: 0.254s, episode steps:  12, steps per second:  47, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.452489, mae: 0.582281, mean_q: 0.495403, mean_eps: 0.500000\n",
            "   84/8000: episode: 6, duration: 0.289s, episode steps:  14, steps per second:  48, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.214 [0.000, 1.000],  loss: 0.417402, mae: 0.565390, mean_q: 0.583919, mean_eps: 0.500000\n",
            "   93/8000: episode: 7, duration: 0.185s, episode steps:   9, steps per second:  49, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.402502, mae: 0.568615, mean_q: 0.680846, mean_eps: 0.500000\n",
            "  114/8000: episode: 8, duration: 0.439s, episode steps:  21, steps per second:  48, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.381 [0.000, 1.000],  loss: 0.373810, mae: 0.555731, mean_q: 0.723232, mean_eps: 0.500000\n",
            "  125/8000: episode: 9, duration: 0.256s, episode steps:  11, steps per second:  43, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.365909, mae: 0.599402, mean_q: 0.809608, mean_eps: 0.500000\n",
            "  139/8000: episode: 10, duration: 0.302s, episode steps:  14, steps per second:  46, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.214 [0.000, 1.000],  loss: 0.362893, mae: 0.639621, mean_q: 0.881010, mean_eps: 0.500000\n",
            "  149/8000: episode: 11, duration: 0.224s, episode steps:  10, steps per second:  45, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.332611, mae: 0.645994, mean_q: 0.936550, mean_eps: 0.500000\n",
            "  159/8000: episode: 12, duration: 0.198s, episode steps:  10, steps per second:  51, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.324900, mae: 0.667592, mean_q: 1.035743, mean_eps: 0.500000\n",
            "  169/8000: episode: 13, duration: 0.173s, episode steps:  10, steps per second:  58, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.320597, mae: 0.697979, mean_q: 1.152494, mean_eps: 0.500000\n",
            "  185/8000: episode: 14, duration: 0.311s, episode steps:  16, steps per second:  51, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.296266, mae: 0.719271, mean_q: 1.261964, mean_eps: 0.500000\n",
            "  198/8000: episode: 15, duration: 0.274s, episode steps:  13, steps per second:  47, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.312305, mae: 0.768688, mean_q: 1.341459, mean_eps: 0.500000\n",
            "  216/8000: episode: 16, duration: 0.432s, episode steps:  18, steps per second:  42, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.278 [0.000, 1.000],  loss: 0.293985, mae: 0.816597, mean_q: 1.405762, mean_eps: 0.500000\n",
            "  230/8000: episode: 17, duration: 0.296s, episode steps:  14, steps per second:  47, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.298119, mae: 0.859801, mean_q: 1.492931, mean_eps: 0.500000\n",
            "  241/8000: episode: 18, duration: 0.223s, episode steps:  11, steps per second:  49, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.275810, mae: 0.870409, mean_q: 1.569649, mean_eps: 0.500000\n",
            "  250/8000: episode: 19, duration: 0.186s, episode steps:   9, steps per second:  48, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.275162, mae: 0.909307, mean_q: 1.661881, mean_eps: 0.500000\n",
            "  262/8000: episode: 20, duration: 0.251s, episode steps:  12, steps per second:  48, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.284921, mae: 0.965237, mean_q: 1.831321, mean_eps: 0.500000\n",
            "  280/8000: episode: 21, duration: 0.339s, episode steps:  18, steps per second:  53, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.356710, mae: 1.043613, mean_q: 1.895064, mean_eps: 0.500000\n",
            "  293/8000: episode: 22, duration: 0.305s, episode steps:  13, steps per second:  43, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.342914, mae: 1.120152, mean_q: 1.901436, mean_eps: 0.500000\n",
            "  303/8000: episode: 23, duration: 0.234s, episode steps:  10, steps per second:  43, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.344435, mae: 1.127380, mean_q: 1.908512, mean_eps: 0.500000\n",
            "  314/8000: episode: 24, duration: 0.228s, episode steps:  11, steps per second:  48, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.309591, mae: 1.160205, mean_q: 1.995691, mean_eps: 0.500000\n",
            "  323/8000: episode: 25, duration: 0.178s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.366999, mae: 1.256275, mean_q: 2.188298, mean_eps: 0.500000\n",
            "  339/8000: episode: 26, duration: 0.300s, episode steps:  16, steps per second:  53, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.349182, mae: 1.274245, mean_q: 2.232293, mean_eps: 0.500000\n",
            "  350/8000: episode: 27, duration: 0.230s, episode steps:  11, steps per second:  48, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.355814, mae: 1.312919, mean_q: 2.321932, mean_eps: 0.500000\n",
            "  364/8000: episode: 28, duration: 0.245s, episode steps:  14, steps per second:  57, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.315620, mae: 1.340198, mean_q: 2.392061, mean_eps: 0.500000\n",
            "  379/8000: episode: 29, duration: 0.268s, episode steps:  15, steps per second:  56, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.351307, mae: 1.364589, mean_q: 2.513544, mean_eps: 0.500000\n",
            "  391/8000: episode: 30, duration: 0.233s, episode steps:  12, steps per second:  51, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.380452, mae: 1.436586, mean_q: 2.620457, mean_eps: 0.500000\n",
            "  403/8000: episode: 31, duration: 0.280s, episode steps:  12, steps per second:  43, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.311898, mae: 1.426824, mean_q: 2.682762, mean_eps: 0.500000\n",
            "  414/8000: episode: 32, duration: 0.265s, episode steps:  11, steps per second:  41, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.334207, mae: 1.484467, mean_q: 2.811188, mean_eps: 0.500000\n",
            "  439/8000: episode: 33, duration: 0.523s, episode steps:  25, steps per second:  48, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.360 [0.000, 1.000],  loss: 0.389822, mae: 1.572197, mean_q: 2.909999, mean_eps: 0.500000\n",
            "  449/8000: episode: 34, duration: 0.234s, episode steps:  10, steps per second:  43, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.428514, mae: 1.613668, mean_q: 3.002056, mean_eps: 0.500000\n",
            "  463/8000: episode: 35, duration: 0.331s, episode steps:  14, steps per second:  42, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.410821, mae: 1.648494, mean_q: 3.061202, mean_eps: 0.500000\n",
            "  475/8000: episode: 36, duration: 0.237s, episode steps:  12, steps per second:  51, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.398551, mae: 1.720508, mean_q: 3.149125, mean_eps: 0.500000\n",
            "  488/8000: episode: 37, duration: 0.300s, episode steps:  13, steps per second:  43, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.520045, mae: 1.800896, mean_q: 3.335868, mean_eps: 0.500000\n",
            "  499/8000: episode: 38, duration: 0.257s, episode steps:  11, steps per second:  43, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.422361, mae: 1.760230, mean_q: 3.293676, mean_eps: 0.500000\n",
            "  508/8000: episode: 39, duration: 0.228s, episode steps:   9, steps per second:  39, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.366114, mae: 1.782207, mean_q: 3.408918, mean_eps: 0.500000\n",
            "  523/8000: episode: 40, duration: 0.410s, episode steps:  15, steps per second:  37, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.380001, mae: 1.820465, mean_q: 3.517312, mean_eps: 0.500000\n",
            "  538/8000: episode: 41, duration: 0.358s, episode steps:  15, steps per second:  42, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 0.417070, mae: 1.861422, mean_q: 3.707311, mean_eps: 0.500000\n",
            "  552/8000: episode: 42, duration: 0.265s, episode steps:  14, steps per second:  53, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.476246, mae: 1.972236, mean_q: 3.734501, mean_eps: 0.500000\n",
            "  566/8000: episode: 43, duration: 0.271s, episode steps:  14, steps per second:  52, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.451934, mae: 1.981268, mean_q: 3.833288, mean_eps: 0.500000\n",
            "  577/8000: episode: 44, duration: 0.245s, episode steps:  11, steps per second:  45, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.567202, mae: 2.079418, mean_q: 3.970520, mean_eps: 0.500000\n",
            "  592/8000: episode: 45, duration: 0.274s, episode steps:  15, steps per second:  55, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.486974, mae: 2.083877, mean_q: 4.063550, mean_eps: 0.500000\n",
            "  602/8000: episode: 46, duration: 0.173s, episode steps:  10, steps per second:  58, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.572314, mae: 2.139280, mean_q: 4.152272, mean_eps: 0.500000\n",
            "  611/8000: episode: 47, duration: 0.166s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.383911, mae: 2.100388, mean_q: 4.187374, mean_eps: 0.500000\n",
            "  623/8000: episode: 48, duration: 0.250s, episode steps:  12, steps per second:  48, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.593291, mae: 2.233258, mean_q: 4.292826, mean_eps: 0.500000\n",
            "  634/8000: episode: 49, duration: 0.190s, episode steps:  11, steps per second:  58, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.503988, mae: 2.217627, mean_q: 4.305689, mean_eps: 0.500000\n",
            "  648/8000: episode: 50, duration: 0.306s, episode steps:  14, steps per second:  46, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.612780, mae: 2.306986, mean_q: 4.338123, mean_eps: 0.500000\n",
            "  662/8000: episode: 51, duration: 0.250s, episode steps:  14, steps per second:  56, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.556109, mae: 2.328900, mean_q: 4.441559, mean_eps: 0.500000\n",
            "  671/8000: episode: 52, duration: 0.165s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.606748, mae: 2.405105, mean_q: 4.545536, mean_eps: 0.500000\n",
            "  681/8000: episode: 53, duration: 0.177s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.523715, mae: 2.407629, mean_q: 4.598449, mean_eps: 0.500000\n",
            "  690/8000: episode: 54, duration: 0.161s, episode steps:   9, steps per second:  56, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.549297, mae: 2.417179, mean_q: 4.632345, mean_eps: 0.500000\n",
            "  707/8000: episode: 55, duration: 0.296s, episode steps:  17, steps per second:  57, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: 0.660748, mae: 2.499633, mean_q: 4.666302, mean_eps: 0.500000\n",
            "  719/8000: episode: 56, duration: 0.236s, episode steps:  12, steps per second:  51, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.527301, mae: 2.504403, mean_q: 4.773488, mean_eps: 0.500000\n",
            "  732/8000: episode: 57, duration: 0.246s, episode steps:  13, steps per second:  53, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.548414, mae: 2.561661, mean_q: 4.895375, mean_eps: 0.500000\n",
            "  749/8000: episode: 58, duration: 0.326s, episode steps:  17, steps per second:  52, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: 0.463268, mae: 2.574157, mean_q: 5.094946, mean_eps: 0.500000\n",
            "  774/8000: episode: 59, duration: 0.490s, episode steps:  25, steps per second:  51, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.577243, mae: 2.680629, mean_q: 5.186049, mean_eps: 0.500000\n",
            "  784/8000: episode: 60, duration: 0.195s, episode steps:  10, steps per second:  51, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.456271, mae: 2.717114, mean_q: 5.265428, mean_eps: 0.500000\n",
            "  800/8000: episode: 61, duration: 0.296s, episode steps:  16, steps per second:  54, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.623619, mae: 2.810737, mean_q: 5.351788, mean_eps: 0.500000\n",
            "  815/8000: episode: 62, duration: 0.259s, episode steps:  15, steps per second:  58, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.590839, mae: 2.857198, mean_q: 5.359912, mean_eps: 0.500000\n",
            "  828/8000: episode: 63, duration: 0.250s, episode steps:  13, steps per second:  52, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.677692, mae: 2.903323, mean_q: 5.430944, mean_eps: 0.500000\n",
            "  839/8000: episode: 64, duration: 0.192s, episode steps:  11, steps per second:  57, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.512216, mae: 2.902347, mean_q: 5.506547, mean_eps: 0.500000\n",
            "  854/8000: episode: 65, duration: 0.264s, episode steps:  15, steps per second:  57, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.643861, mae: 2.980644, mean_q: 5.611653, mean_eps: 0.500000\n",
            "  868/8000: episode: 66, duration: 0.275s, episode steps:  14, steps per second:  51, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.560350, mae: 3.005718, mean_q: 5.633292, mean_eps: 0.500000\n",
            "  883/8000: episode: 67, duration: 0.283s, episode steps:  15, steps per second:  53, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.565329, mae: 3.019593, mean_q: 5.688731, mean_eps: 0.500000\n",
            "  902/8000: episode: 68, duration: 0.386s, episode steps:  19, steps per second:  49, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.547098, mae: 3.091000, mean_q: 5.771505, mean_eps: 0.500000\n",
            "  936/8000: episode: 69, duration: 0.614s, episode steps:  34, steps per second:  55, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.559 [0.000, 1.000],  loss: 0.585707, mae: 3.161349, mean_q: 5.899306, mean_eps: 0.500000\n",
            "  987/8000: episode: 70, duration: 1.236s, episode steps:  51, steps per second:  41, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 0.591247, mae: 3.252629, mean_q: 6.059987, mean_eps: 0.500000\n",
            "  998/8000: episode: 71, duration: 0.220s, episode steps:  11, steps per second:  50, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.592677, mae: 3.318357, mean_q: 6.267890, mean_eps: 0.500000\n",
            " 1013/8000: episode: 72, duration: 0.282s, episode steps:  15, steps per second:  53, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.497781, mae: 3.354656, mean_q: 6.351439, mean_eps: 0.500000\n",
            " 1031/8000: episode: 73, duration: 0.362s, episode steps:  18, steps per second:  50, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 0.698390, mae: 3.425583, mean_q: 6.332954, mean_eps: 0.500000\n",
            " 1045/8000: episode: 74, duration: 0.337s, episode steps:  14, steps per second:  42, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.456233, mae: 3.405644, mean_q: 6.379872, mean_eps: 0.500000\n",
            " 1082/8000: episode: 75, duration: 0.664s, episode steps:  37, steps per second:  56, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.568 [0.000, 1.000],  loss: 0.514253, mae: 3.514456, mean_q: 6.580441, mean_eps: 0.500000\n",
            " 1110/8000: episode: 76, duration: 0.518s, episode steps:  28, steps per second:  54, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 0.610172, mae: 3.628049, mean_q: 6.821069, mean_eps: 0.500000\n",
            " 1124/8000: episode: 77, duration: 0.270s, episode steps:  14, steps per second:  52, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.857 [0.000, 1.000],  loss: 0.752098, mae: 3.708340, mean_q: 6.841429, mean_eps: 0.500000\n",
            " 1153/8000: episode: 78, duration: 0.528s, episode steps:  29, steps per second:  55, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.379 [0.000, 1.000],  loss: 0.588577, mae: 3.738957, mean_q: 6.963629, mean_eps: 0.500000\n",
            " 1163/8000: episode: 79, duration: 0.178s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.631079, mae: 3.827155, mean_q: 7.216892, mean_eps: 0.500000\n",
            " 1174/8000: episode: 80, duration: 0.212s, episode steps:  11, steps per second:  52, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.730082, mae: 3.830713, mean_q: 7.223451, mean_eps: 0.500000\n",
            " 1208/8000: episode: 81, duration: 0.585s, episode steps:  34, steps per second:  58, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.676 [0.000, 1.000],  loss: 0.628346, mae: 3.896793, mean_q: 7.378837, mean_eps: 0.500000\n",
            " 1219/8000: episode: 82, duration: 0.242s, episode steps:  11, steps per second:  46, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.722552, mae: 4.009504, mean_q: 7.693488, mean_eps: 0.500000\n",
            " 1237/8000: episode: 83, duration: 0.354s, episode steps:  18, steps per second:  51, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.722 [0.000, 1.000],  loss: 0.897572, mae: 4.137316, mean_q: 7.841705, mean_eps: 0.500000\n",
            " 1249/8000: episode: 84, duration: 0.263s, episode steps:  12, steps per second:  46, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 1.242882, mae: 4.234142, mean_q: 7.902425, mean_eps: 0.500000\n",
            " 1257/8000: episode: 85, duration: 0.150s, episode steps:   8, steps per second:  53, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.895022, mae: 4.321888, mean_q: 8.143932, mean_eps: 0.500000\n",
            " 1268/8000: episode: 86, duration: 0.194s, episode steps:  11, steps per second:  57, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 1.168960, mae: 4.322625, mean_q: 8.099456, mean_eps: 0.500000\n",
            " 1278/8000: episode: 87, duration: 0.198s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 1.290814, mae: 4.283089, mean_q: 8.061952, mean_eps: 0.500000\n",
            " 1291/8000: episode: 88, duration: 0.263s, episode steps:  13, steps per second:  49, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 2.099434, mae: 4.434338, mean_q: 8.192640, mean_eps: 0.500000\n",
            " 1303/8000: episode: 89, duration: 0.229s, episode steps:  12, steps per second:  52, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 1.449760, mae: 4.413485, mean_q: 8.178473, mean_eps: 0.500000\n",
            " 1319/8000: episode: 90, duration: 0.306s, episode steps:  16, steps per second:  52, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 1.346436, mae: 4.467176, mean_q: 8.244961, mean_eps: 0.500000\n",
            " 1338/8000: episode: 91, duration: 0.377s, episode steps:  19, steps per second:  50, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 1.213827, mae: 4.511657, mean_q: 8.446326, mean_eps: 0.500000\n",
            " 1361/8000: episode: 92, duration: 0.423s, episode steps:  23, steps per second:  54, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.783 [0.000, 1.000],  loss: 2.092091, mae: 4.693558, mean_q: 8.735645, mean_eps: 0.500000\n",
            " 1375/8000: episode: 93, duration: 0.243s, episode steps:  14, steps per second:  58, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 2.083749, mae: 4.770406, mean_q: 8.802650, mean_eps: 0.500000\n",
            " 1389/8000: episode: 94, duration: 0.263s, episode steps:  14, steps per second:  53, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 1.614502, mae: 4.685154, mean_q: 8.648595, mean_eps: 0.500000\n",
            " 1406/8000: episode: 95, duration: 0.328s, episode steps:  17, steps per second:  52, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 1.166543, mae: 4.699010, mean_q: 8.799285, mean_eps: 0.500000\n",
            " 1416/8000: episode: 96, duration: 0.196s, episode steps:  10, steps per second:  51, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 2.547765, mae: 4.909686, mean_q: 8.916183, mean_eps: 0.500000\n",
            " 1426/8000: episode: 97, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.345449, mae: 4.825491, mean_q: 8.937858, mean_eps: 0.500000\n",
            " 1437/8000: episode: 98, duration: 0.227s, episode steps:  11, steps per second:  48, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 2.201235, mae: 4.932331, mean_q: 9.025094, mean_eps: 0.500000\n",
            " 1450/8000: episode: 99, duration: 0.267s, episode steps:  13, steps per second:  49, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 1.049872, mae: 4.860484, mean_q: 9.026803, mean_eps: 0.500000\n",
            " 1480/8000: episode: 100, duration: 0.531s, episode steps:  30, steps per second:  57, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.567 [0.000, 1.000],  loss: 2.193601, mae: 5.044793, mean_q: 9.231860, mean_eps: 0.500000\n",
            " 1493/8000: episode: 101, duration: 0.249s, episode steps:  13, steps per second:  52, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1.567932, mae: 4.924818, mean_q: 9.143800, mean_eps: 0.500000\n",
            " 1505/8000: episode: 102, duration: 0.246s, episode steps:  12, steps per second:  49, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 2.629305, mae: 5.118261, mean_q: 9.374072, mean_eps: 0.500000\n",
            " 1564/8000: episode: 103, duration: 1.081s, episode steps:  59, steps per second:  55, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 2.023718, mae: 5.144650, mean_q: 9.437682, mean_eps: 0.500000\n",
            " 1575/8000: episode: 104, duration: 0.218s, episode steps:  11, steps per second:  51, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 2.533764, mae: 5.296897, mean_q: 9.773261, mean_eps: 0.500000\n",
            " 1587/8000: episode: 105, duration: 0.259s, episode steps:  12, steps per second:  46, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 2.405703, mae: 5.349916, mean_q: 9.795928, mean_eps: 0.500000\n",
            " 1596/8000: episode: 106, duration: 0.181s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.719630, mae: 5.340050, mean_q: 9.902015, mean_eps: 0.500000\n",
            " 1611/8000: episode: 107, duration: 0.310s, episode steps:  15, steps per second:  48, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 2.757761, mae: 5.391841, mean_q: 9.914919, mean_eps: 0.500000\n",
            " 1621/8000: episode: 108, duration: 0.181s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 2.015618, mae: 5.360217, mean_q: 10.021061, mean_eps: 0.500000\n",
            " 1633/8000: episode: 109, duration: 0.253s, episode steps:  12, steps per second:  47, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 3.100901, mae: 5.530356, mean_q: 10.148443, mean_eps: 0.500000\n",
            " 1648/8000: episode: 110, duration: 0.315s, episode steps:  15, steps per second:  48, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 1.804890, mae: 5.429753, mean_q: 10.107153, mean_eps: 0.500000\n",
            " 1663/8000: episode: 111, duration: 0.306s, episode steps:  15, steps per second:  49, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 3.236921, mae: 5.607054, mean_q: 10.236076, mean_eps: 0.500000\n",
            " 1673/8000: episode: 112, duration: 0.192s, episode steps:  10, steps per second:  52, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 2.393451, mae: 5.538827, mean_q: 10.150035, mean_eps: 0.500000\n",
            " 1686/8000: episode: 113, duration: 0.275s, episode steps:  13, steps per second:  47, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 1.859411, mae: 5.405589, mean_q: 9.966337, mean_eps: 0.500000\n",
            " 1695/8000: episode: 114, duration: 0.203s, episode steps:   9, steps per second:  44, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 2.891891, mae: 5.689003, mean_q: 10.366566, mean_eps: 0.500000\n",
            " 1720/8000: episode: 115, duration: 0.492s, episode steps:  25, steps per second:  51, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.640 [0.000, 1.000],  loss: 2.431307, mae: 5.626189, mean_q: 10.337093, mean_eps: 0.500000\n",
            " 1730/8000: episode: 116, duration: 0.195s, episode steps:  10, steps per second:  51, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 1.839719, mae: 5.617328, mean_q: 10.475714, mean_eps: 0.500000\n",
            " 1746/8000: episode: 117, duration: 0.311s, episode steps:  16, steps per second:  51, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 3.337528, mae: 5.774731, mean_q: 10.461833, mean_eps: 0.500000\n",
            " 1757/8000: episode: 118, duration: 0.216s, episode steps:  11, steps per second:  51, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 2.002308, mae: 5.662697, mean_q: 10.402326, mean_eps: 0.500000\n",
            " 1768/8000: episode: 119, duration: 0.194s, episode steps:  11, steps per second:  57, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 1.504474, mae: 5.563711, mean_q: 10.445997, mean_eps: 0.500000\n",
            " 1778/8000: episode: 120, duration: 0.187s, episode steps:  10, steps per second:  54, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.705430, mae: 5.651745, mean_q: 10.668316, mean_eps: 0.500000\n",
            " 1788/8000: episode: 121, duration: 0.193s, episode steps:  10, steps per second:  52, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 1.599359, mae: 5.652198, mean_q: 10.672148, mean_eps: 0.500000\n",
            " 1807/8000: episode: 122, duration: 0.364s, episode steps:  19, steps per second:  52, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.684 [0.000, 1.000],  loss: 1.827200, mae: 5.757825, mean_q: 10.821646, mean_eps: 0.500000\n",
            " 1819/8000: episode: 123, duration: 0.232s, episode steps:  12, steps per second:  52, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 2.403861, mae: 5.836484, mean_q: 10.884432, mean_eps: 0.500000\n",
            " 1851/8000: episode: 124, duration: 0.625s, episode steps:  32, steps per second:  51, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 2.771894, mae: 5.965994, mean_q: 10.907444, mean_eps: 0.500000\n",
            " 1906/8000: episode: 125, duration: 1.393s, episode steps:  55, steps per second:  39, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 2.216126, mae: 5.938781, mean_q: 10.955334, mean_eps: 0.500000\n",
            " 1919/8000: episode: 126, duration: 0.332s, episode steps:  13, steps per second:  39, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 2.530871, mae: 5.972928, mean_q: 11.051139, mean_eps: 0.500000\n",
            " 1973/8000: episode: 127, duration: 1.066s, episode steps:  54, steps per second:  51, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 2.254175, mae: 6.008235, mean_q: 11.145059, mean_eps: 0.500000\n",
            " 1989/8000: episode: 128, duration: 0.319s, episode steps:  16, steps per second:  50, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.498607, mae: 6.050834, mean_q: 11.192729, mean_eps: 0.500000\n",
            " 2002/8000: episode: 129, duration: 0.302s, episode steps:  13, steps per second:  43, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 3.171983, mae: 6.133644, mean_q: 11.259084, mean_eps: 0.500000\n",
            " 2014/8000: episode: 130, duration: 0.279s, episode steps:  12, steps per second:  43, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 2.336192, mae: 6.041338, mean_q: 11.175090, mean_eps: 0.500000\n",
            " 2025/8000: episode: 131, duration: 0.279s, episode steps:  11, steps per second:  39, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 2.334849, mae: 6.074810, mean_q: 11.340137, mean_eps: 0.500000\n",
            " 2035/8000: episode: 132, duration: 0.600s, episode steps:  10, steps per second:  17, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 3.389541, mae: 6.276497, mean_q: 11.578307, mean_eps: 0.500000\n",
            " 2057/8000: episode: 133, duration: 0.483s, episode steps:  22, steps per second:  46, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 2.741336, mae: 6.166294, mean_q: 11.263341, mean_eps: 0.500000\n",
            " 2071/8000: episode: 134, duration: 0.637s, episode steps:  14, steps per second:  22, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 2.592750, mae: 6.198086, mean_q: 11.404295, mean_eps: 0.500000\n",
            " 2079/8000: episode: 135, duration: 0.146s, episode steps:   8, steps per second:  55, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.875 [0.000, 1.000],  loss: 2.419490, mae: 6.196326, mean_q: 11.376320, mean_eps: 0.500000\n",
            " 2097/8000: episode: 136, duration: 0.362s, episode steps:  18, steps per second:  50, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.722 [0.000, 1.000],  loss: 2.964948, mae: 6.222287, mean_q: 11.381263, mean_eps: 0.500000\n",
            " 2108/8000: episode: 137, duration: 0.224s, episode steps:  11, steps per second:  49, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 2.130676, mae: 6.144054, mean_q: 11.452384, mean_eps: 0.500000\n",
            " 2148/8000: episode: 138, duration: 0.727s, episode steps:  40, steps per second:  55, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.650 [0.000, 1.000],  loss: 2.256262, mae: 6.249858, mean_q: 11.729984, mean_eps: 0.500000\n",
            " 2159/8000: episode: 139, duration: 0.209s, episode steps:  11, steps per second:  53, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 2.560772, mae: 6.257345, mean_q: 11.490892, mean_eps: 0.500000\n",
            " 2168/8000: episode: 140, duration: 0.150s, episode steps:   9, steps per second:  60, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 2.230113, mae: 6.306692, mean_q: 11.652991, mean_eps: 0.500000\n",
            " 2254/8000: episode: 141, duration: 1.384s, episode steps:  86, steps per second:  62, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 2.282826, mae: 6.344221, mean_q: 11.800267, mean_eps: 0.500000\n",
            " 2290/8000: episode: 142, duration: 0.619s, episode steps:  36, steps per second:  58, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 2.470459, mae: 6.412862, mean_q: 11.843013, mean_eps: 0.500000\n",
            " 2339/8000: episode: 143, duration: 0.934s, episode steps:  49, steps per second:  52, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 2.267209, mae: 6.414338, mean_q: 11.975953, mean_eps: 0.500000\n",
            " 2431/8000: episode: 144, duration: 1.560s, episode steps:  92, steps per second:  59, episode reward: 92.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.454403, mae: 6.588967, mean_q: 12.297820, mean_eps: 0.500000\n",
            " 2453/8000: episode: 145, duration: 0.398s, episode steps:  22, steps per second:  55, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 1.619459, mae: 6.580971, mean_q: 12.444216, mean_eps: 0.500000\n",
            " 2502/8000: episode: 146, duration: 0.801s, episode steps:  49, steps per second:  61, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 2.050305, mae: 6.712902, mean_q: 12.716529, mean_eps: 0.500000\n",
            " 2515/8000: episode: 147, duration: 0.244s, episode steps:  13, steps per second:  53, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 2.064038, mae: 6.762989, mean_q: 12.748837, mean_eps: 0.500000\n",
            " 2535/8000: episode: 148, duration: 0.343s, episode steps:  20, steps per second:  58, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 2.665664, mae: 6.839497, mean_q: 12.776787, mean_eps: 0.500000\n",
            " 2616/8000: episode: 149, duration: 1.386s, episode steps:  81, steps per second:  58, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 2.143204, mae: 6.897046, mean_q: 12.992497, mean_eps: 0.500000\n",
            " 2692/8000: episode: 150, duration: 1.224s, episode steps:  76, steps per second:  62, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2.295795, mae: 6.991527, mean_q: 13.187875, mean_eps: 0.500000\n",
            " 2708/8000: episode: 151, duration: 0.266s, episode steps:  16, steps per second:  60, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 2.962934, mae: 7.187081, mean_q: 13.442891, mean_eps: 0.500000\n",
            " 2791/8000: episode: 152, duration: 1.856s, episode steps:  83, steps per second:  45, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 2.550898, mae: 7.119506, mean_q: 13.359094, mean_eps: 0.500000\n",
            " 2805/8000: episode: 153, duration: 0.310s, episode steps:  14, steps per second:  45, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 2.676385, mae: 7.202717, mean_q: 13.451512, mean_eps: 0.500000\n",
            " 2818/8000: episode: 154, duration: 0.264s, episode steps:  13, steps per second:  49, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 2.758317, mae: 7.232821, mean_q: 13.615178, mean_eps: 0.500000\n",
            " 2866/8000: episode: 155, duration: 0.856s, episode steps:  48, steps per second:  56, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 1.932015, mae: 7.265278, mean_q: 13.862665, mean_eps: 0.500000\n",
            " 2915/8000: episode: 156, duration: 0.848s, episode steps:  49, steps per second:  58, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.551 [0.000, 1.000],  loss: 2.379076, mae: 7.481348, mean_q: 14.258378, mean_eps: 0.500000\n",
            " 2986/8000: episode: 157, duration: 1.249s, episode steps:  71, steps per second:  57, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 1.848719, mae: 7.516559, mean_q: 14.383828, mean_eps: 0.500000\n",
            " 3008/8000: episode: 158, duration: 0.403s, episode steps:  22, steps per second:  55, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.854855, mae: 7.788397, mean_q: 14.847713, mean_eps: 0.500000\n",
            " 3050/8000: episode: 159, duration: 0.921s, episode steps:  42, steps per second:  46, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.539047, mae: 7.716452, mean_q: 14.641121, mean_eps: 0.500000\n",
            " 3068/8000: episode: 160, duration: 0.481s, episode steps:  18, steps per second:  37, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 2.475349, mae: 7.847140, mean_q: 14.933043, mean_eps: 0.500000\n",
            " 3105/8000: episode: 161, duration: 1.028s, episode steps:  37, steps per second:  36, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 2.213707, mae: 7.870382, mean_q: 15.036103, mean_eps: 0.500000\n",
            " 3140/8000: episode: 162, duration: 0.648s, episode steps:  35, steps per second:  54, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 2.586331, mae: 7.945403, mean_q: 15.139305, mean_eps: 0.500000\n",
            " 3178/8000: episode: 163, duration: 0.677s, episode steps:  38, steps per second:  56, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  loss: 3.227851, mae: 8.034010, mean_q: 15.233190, mean_eps: 0.500000\n",
            " 3284/8000: episode: 164, duration: 1.817s, episode steps: 106, steps per second:  58, episode reward: 106.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 2.846841, mae: 8.160179, mean_q: 15.537349, mean_eps: 0.500000\n",
            " 3335/8000: episode: 165, duration: 0.890s, episode steps:  51, steps per second:  57, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 2.516090, mae: 8.312170, mean_q: 15.951919, mean_eps: 0.500000\n",
            " 3370/8000: episode: 166, duration: 0.631s, episode steps:  35, steps per second:  55, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 2.943486, mae: 8.386067, mean_q: 16.026145, mean_eps: 0.500000\n",
            " 3379/8000: episode: 167, duration: 0.185s, episode steps:   9, steps per second:  49, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 1.774533, mae: 8.465723, mean_q: 16.329071, mean_eps: 0.500000\n",
            " 3453/8000: episode: 168, duration: 1.705s, episode steps:  74, steps per second:  43, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 2.831092, mae: 8.577850, mean_q: 16.406457, mean_eps: 0.500000\n",
            " 3470/8000: episode: 169, duration: 0.360s, episode steps:  17, steps per second:  47, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 3.189466, mae: 8.647379, mean_q: 16.547716, mean_eps: 0.500000\n",
            " 3508/8000: episode: 170, duration: 0.679s, episode steps:  38, steps per second:  56, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  loss: 2.968689, mae: 8.753464, mean_q: 16.766499, mean_eps: 0.500000\n",
            " 3550/8000: episode: 171, duration: 0.746s, episode steps:  42, steps per second:  56, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.595 [0.000, 1.000],  loss: 2.353971, mae: 8.726760, mean_q: 16.814228, mean_eps: 0.500000\n",
            " 3625/8000: episode: 172, duration: 1.347s, episode steps:  75, steps per second:  56, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 3.000393, mae: 8.900185, mean_q: 17.094059, mean_eps: 0.500000\n",
            " 3666/8000: episode: 173, duration: 0.705s, episode steps:  41, steps per second:  58, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 2.402947, mae: 9.069720, mean_q: 17.517252, mean_eps: 0.500000\n",
            " 3728/8000: episode: 174, duration: 1.394s, episode steps:  62, steps per second:  44, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.323032, mae: 9.207013, mean_q: 17.669018, mean_eps: 0.500000\n",
            " 3797/8000: episode: 175, duration: 1.144s, episode steps:  69, steps per second:  60, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 3.160524, mae: 9.271859, mean_q: 17.797994, mean_eps: 0.500000\n",
            " 3841/8000: episode: 176, duration: 0.779s, episode steps:  44, steps per second:  57, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 2.996947, mae: 9.444197, mean_q: 18.267644, mean_eps: 0.500000\n",
            " 3881/8000: episode: 177, duration: 0.641s, episode steps:  40, steps per second:  62, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 3.521350, mae: 9.494860, mean_q: 18.266607, mean_eps: 0.500000\n",
            " 3929/8000: episode: 178, duration: 0.750s, episode steps:  48, steps per second:  64, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 3.484203, mae: 9.536502, mean_q: 18.322504, mean_eps: 0.500000\n",
            " 3974/8000: episode: 179, duration: 0.732s, episode steps:  45, steps per second:  61, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 3.947191, mae: 9.653311, mean_q: 18.521263, mean_eps: 0.500000\n",
            " 4032/8000: episode: 180, duration: 0.953s, episode steps:  58, steps per second:  61, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 3.237431, mae: 9.729515, mean_q: 18.800460, mean_eps: 0.500000\n",
            " 4133/8000: episode: 181, duration: 1.541s, episode steps: 101, steps per second:  66, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 3.149592, mae: 9.926908, mean_q: 19.234501, mean_eps: 0.500000\n",
            " 4165/8000: episode: 182, duration: 0.474s, episode steps:  32, steps per second:  67, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 2.989781, mae: 10.126987, mean_q: 19.731949, mean_eps: 0.500000\n",
            " 4183/8000: episode: 183, duration: 0.313s, episode steps:  18, steps per second:  57, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 3.636245, mae: 10.113609, mean_q: 19.502762, mean_eps: 0.500000\n",
            " 4289/8000: episode: 184, duration: 1.630s, episode steps: 106, steps per second:  65, episode reward: 106.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 4.131774, mae: 10.276400, mean_q: 19.795564, mean_eps: 0.500000\n",
            " 4337/8000: episode: 185, duration: 0.780s, episode steps:  48, steps per second:  62, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 4.193874, mae: 10.398372, mean_q: 20.038927, mean_eps: 0.500000\n",
            " 4375/8000: episode: 186, duration: 0.603s, episode steps:  38, steps per second:  63, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.374466, mae: 10.525215, mean_q: 20.257377, mean_eps: 0.500000\n",
            " 4431/8000: episode: 187, duration: 0.913s, episode steps:  56, steps per second:  61, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 3.873654, mae: 10.524020, mean_q: 20.345255, mean_eps: 0.500000\n",
            " 4468/8000: episode: 188, duration: 0.631s, episode steps:  37, steps per second:  59, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 2.430002, mae: 10.566857, mean_q: 20.727675, mean_eps: 0.500000\n",
            " 4496/8000: episode: 189, duration: 0.469s, episode steps:  28, steps per second:  60, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 3.215976, mae: 10.745000, mean_q: 20.938338, mean_eps: 0.500000\n",
            " 4524/8000: episode: 190, duration: 0.444s, episode steps:  28, steps per second:  63, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 4.220834, mae: 10.864915, mean_q: 21.125039, mean_eps: 0.500000\n",
            " 4573/8000: episode: 191, duration: 0.796s, episode steps:  49, steps per second:  62, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 3.909007, mae: 10.909900, mean_q: 21.086898, mean_eps: 0.500000\n",
            " 4614/8000: episode: 192, duration: 0.626s, episode steps:  41, steps per second:  65, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 4.123347, mae: 10.921096, mean_q: 21.145709, mean_eps: 0.500000\n",
            " 4650/8000: episode: 193, duration: 0.556s, episode steps:  36, steps per second:  65, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 4.426659, mae: 11.016720, mean_q: 21.344978, mean_eps: 0.500000\n",
            " 4690/8000: episode: 194, duration: 0.635s, episode steps:  40, steps per second:  63, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 4.922632, mae: 11.125199, mean_q: 21.524975, mean_eps: 0.500000\n",
            " 4754/8000: episode: 195, duration: 0.984s, episode steps:  64, steps per second:  65, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 4.556166, mae: 11.228869, mean_q: 21.711335, mean_eps: 0.500000\n",
            " 4769/8000: episode: 196, duration: 0.289s, episode steps:  15, steps per second:  52, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 5.623861, mae: 11.399940, mean_q: 22.003578, mean_eps: 0.500000\n",
            " 4819/8000: episode: 197, duration: 0.805s, episode steps:  50, steps per second:  62, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 4.720941, mae: 11.346496, mean_q: 21.930615, mean_eps: 0.500000\n",
            " 4885/8000: episode: 198, duration: 1.014s, episode steps:  66, steps per second:  65, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 4.072561, mae: 11.496221, mean_q: 22.368863, mean_eps: 0.500000\n",
            " 4946/8000: episode: 199, duration: 1.014s, episode steps:  61, steps per second:  60, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 4.850222, mae: 11.558406, mean_q: 22.432836, mean_eps: 0.500000\n",
            " 4989/8000: episode: 200, duration: 0.674s, episode steps:  43, steps per second:  64, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 4.604344, mae: 11.700411, mean_q: 22.754337, mean_eps: 0.500000\n",
            " 5048/8000: episode: 201, duration: 0.929s, episode steps:  59, steps per second:  64, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 4.645642, mae: 11.779737, mean_q: 22.967670, mean_eps: 0.500000\n",
            " 5091/8000: episode: 202, duration: 0.720s, episode steps:  43, steps per second:  60, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 4.648335, mae: 11.852052, mean_q: 23.026029, mean_eps: 0.500000\n",
            " 5128/8000: episode: 203, duration: 0.567s, episode steps:  37, steps per second:  65, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 5.262654, mae: 11.972268, mean_q: 23.235545, mean_eps: 0.500000\n",
            " 5211/8000: episode: 204, duration: 1.371s, episode steps:  83, steps per second:  61, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 4.938809, mae: 12.041335, mean_q: 23.352013, mean_eps: 0.500000\n",
            " 5285/8000: episode: 205, duration: 1.217s, episode steps:  74, steps per second:  61, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 4.673742, mae: 12.138170, mean_q: 23.594626, mean_eps: 0.500000\n",
            " 5381/8000: episode: 206, duration: 1.555s, episode steps:  96, steps per second:  62, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 5.933247, mae: 12.294438, mean_q: 23.808794, mean_eps: 0.500000\n",
            " 5475/8000: episode: 207, duration: 1.545s, episode steps:  94, steps per second:  61, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 4.934672, mae: 12.415005, mean_q: 24.128205, mean_eps: 0.500000\n",
            " 5549/8000: episode: 208, duration: 1.224s, episode steps:  74, steps per second:  60, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 4.105166, mae: 12.523665, mean_q: 24.411453, mean_eps: 0.500000\n",
            " 5669/8000: episode: 209, duration: 1.917s, episode steps: 120, steps per second:  63, episode reward: 120.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 5.411604, mae: 12.739161, mean_q: 24.752833, mean_eps: 0.500000\n",
            " 5770/8000: episode: 210, duration: 1.633s, episode steps: 101, steps per second:  62, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 5.358608, mae: 12.955262, mean_q: 25.165320, mean_eps: 0.500000\n",
            " 5802/8000: episode: 211, duration: 0.509s, episode steps:  32, steps per second:  63, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 7.473105, mae: 13.070880, mean_q: 25.216224, mean_eps: 0.500000\n",
            " 5873/8000: episode: 212, duration: 1.093s, episode steps:  71, steps per second:  65, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 5.270332, mae: 13.029395, mean_q: 25.301723, mean_eps: 0.500000\n",
            " 5939/8000: episode: 213, duration: 1.017s, episode steps:  66, steps per second:  65, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 5.742024, mae: 13.169599, mean_q: 25.584518, mean_eps: 0.500000\n",
            " 6040/8000: episode: 214, duration: 1.513s, episode steps: 101, steps per second:  67, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 5.041111, mae: 13.235241, mean_q: 25.705838, mean_eps: 0.500000\n",
            " 6122/8000: episode: 215, duration: 1.236s, episode steps:  82, steps per second:  66, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 6.163272, mae: 13.367119, mean_q: 25.949495, mean_eps: 0.500000\n",
            " 6212/8000: episode: 216, duration: 1.379s, episode steps:  90, steps per second:  65, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 5.208299, mae: 13.542579, mean_q: 26.404982, mean_eps: 0.500000\n",
            " 6275/8000: episode: 217, duration: 0.983s, episode steps:  63, steps per second:  64, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 4.633209, mae: 13.571980, mean_q: 26.624715, mean_eps: 0.500000\n",
            " 6424/8000: episode: 218, duration: 2.432s, episode steps: 149, steps per second:  61, episode reward: 149.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 5.965285, mae: 13.818571, mean_q: 26.988531, mean_eps: 0.500000\n",
            " 6466/8000: episode: 219, duration: 0.691s, episode steps:  42, steps per second:  61, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 5.265083, mae: 13.834478, mean_q: 27.125581, mean_eps: 0.500000\n",
            " 6508/8000: episode: 220, duration: 0.708s, episode steps:  42, steps per second:  59, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 5.237142, mae: 13.954542, mean_q: 27.404456, mean_eps: 0.500000\n",
            " 6542/8000: episode: 221, duration: 0.570s, episode steps:  34, steps per second:  60, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 6.629131, mae: 14.195505, mean_q: 27.766616, mean_eps: 0.500000\n",
            " 6597/8000: episode: 222, duration: 0.880s, episode steps:  55, steps per second:  63, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.564 [0.000, 1.000],  loss: 7.132666, mae: 14.149615, mean_q: 27.441308, mean_eps: 0.500000\n",
            " 6707/8000: episode: 223, duration: 1.706s, episode steps: 110, steps per second:  64, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 5.730569, mae: 14.221326, mean_q: 27.785234, mean_eps: 0.500000\n",
            " 6788/8000: episode: 224, duration: 1.234s, episode steps:  81, steps per second:  66, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 4.968822, mae: 14.331350, mean_q: 28.132871, mean_eps: 0.500000\n",
            " 6845/8000: episode: 225, duration: 0.924s, episode steps:  57, steps per second:  62, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  loss: 5.954170, mae: 14.558097, mean_q: 28.497148, mean_eps: 0.500000\n",
            " 6919/8000: episode: 226, duration: 1.148s, episode steps:  74, steps per second:  64, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 5.756351, mae: 14.705457, mean_q: 28.880329, mean_eps: 0.500000\n",
            " 6966/8000: episode: 227, duration: 0.738s, episode steps:  47, steps per second:  64, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 5.528163, mae: 14.707975, mean_q: 28.841486, mean_eps: 0.500000\n",
            " 7019/8000: episode: 228, duration: 0.902s, episode steps:  53, steps per second:  59, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 5.779489, mae: 14.717951, mean_q: 28.922140, mean_eps: 0.500000\n",
            " 7084/8000: episode: 229, duration: 1.048s, episode steps:  65, steps per second:  62, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 5.810475, mae: 14.914004, mean_q: 29.341846, mean_eps: 0.500000\n",
            " 7173/8000: episode: 230, duration: 1.378s, episode steps:  89, steps per second:  65, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 6.003603, mae: 15.090287, mean_q: 29.663111, mean_eps: 0.500000\n",
            " 7237/8000: episode: 231, duration: 1.017s, episode steps:  64, steps per second:  63, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 6.708437, mae: 15.162433, mean_q: 29.763736, mean_eps: 0.500000\n",
            " 7305/8000: episode: 232, duration: 1.086s, episode steps:  68, steps per second:  63, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.456 [0.000, 1.000],  loss: 7.703642, mae: 15.262323, mean_q: 29.814801, mean_eps: 0.500000\n",
            " 7322/8000: episode: 233, duration: 0.305s, episode steps:  17, steps per second:  56, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 6.642480, mae: 15.296524, mean_q: 29.929310, mean_eps: 0.500000\n",
            " 7384/8000: episode: 234, duration: 0.983s, episode steps:  62, steps per second:  63, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 5.420017, mae: 15.336951, mean_q: 30.094432, mean_eps: 0.500000\n",
            " 7439/8000: episode: 235, duration: 0.873s, episode steps:  55, steps per second:  63, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 5.705228, mae: 15.389335, mean_q: 30.307713, mean_eps: 0.500000\n",
            " 7523/8000: episode: 236, duration: 1.325s, episode steps:  84, steps per second:  63, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 5.636863, mae: 15.456381, mean_q: 30.452638, mean_eps: 0.500000\n",
            " 7677/8000: episode: 237, duration: 2.575s, episode steps: 154, steps per second:  60, episode reward: 154.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 7.270073, mae: 15.736927, mean_q: 30.888521, mean_eps: 0.500000\n",
            " 7833/8000: episode: 238, duration: 2.557s, episode steps: 156, steps per second:  61, episode reward: 156.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 5.404850, mae: 16.000637, mean_q: 31.634722, mean_eps: 0.500000\n",
            " 7903/8000: episode: 239, duration: 1.060s, episode steps:  70, steps per second:  66, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 8.084210, mae: 16.261160, mean_q: 31.838217, mean_eps: 0.500000\n",
            "done, took 156.998 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 132.000, steps: 132\n",
            "Episode 5: reward: 200.000, steps: 200\n",
            "Episode 6: reward: 127.000, steps: 127\n",
            "Episode 7: reward: 155.000, steps: 155\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 200.000, steps: 200\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 200.000, steps: 200\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 200.000, steps: 200\n",
            "Episode 15: reward: 200.000, steps: 200\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 200.000, steps: 200\n",
            "Episode 18: reward: 200.000, steps: 200\n",
            "Episode 19: reward: 200.000, steps: 200\n",
            "Episode 20: reward: 200.000, steps: 200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb16f7d8c10>"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    }
  ]
}