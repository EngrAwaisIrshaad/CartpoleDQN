{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPoleDQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Megacity1/CartpoleDQN/blob/main/Week%203%20Deep%20RL%202/CartPoleDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Deep Q Network (DQN) for CartPole Using Boltzmann Q Policy\n",
        "This exercise implements a DQN for CartPole using a Boltzmann Q policy for selecting the actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGsC7cJ5jNcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda73cbf-3020-40f4-e45e-7b87053448f1"
      },
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.24.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (13.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.44.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.10.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMIHLgQ3Z-lF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5a8608-456f-4127-ce8a-f551e5f4e4f3"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0AMLzq08ap0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b380e1-6c3d-4bce-8233-a8ac01deaf26"
      },
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "!pip install Adam\n",
        "#from keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Adam in /usr/local/lib/python3.7/dist-packages (0.0.0.dev0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll6bNdUm54WS"
      },
      "source": [
        "Implementation of DQN for CartPole, applying policy BoltzmannQPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCrPKNy40PC"
      },
      "source": [
        "##Implement DQN with BoltzmannGumbelQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efM9jkXr5A3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1591671f-82c0-44b8-97ad-78032a8f72f4"
      },
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import BoltzmannGumbelQPolicy\n",
        "from rl.policy import LinearAnnealedPolicy\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n",
        "# setup experience replay buffer\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
        "                               attr='eps',            \n",
        "                               value_max=10.,\n",
        "                               value_min=.10, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=20)\n",
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "# add extra layers here\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=20,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy) \n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=8000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_42\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_40 (Flatten)        (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_81 (Dense)            (None, 16)                80        \n",
            "                                                                 \n",
            " dense_82 (Dense)            (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 8000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   22/8000: episode: 1, duration: 12.536s, episode steps:  22, steps per second:   2, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.682 [0.000, 1.000],  loss: 0.493213, mae: 0.529277, mean_q: -0.056252, mean_eps: 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   32/8000: episode: 2, duration: 0.212s, episode steps:  10, steps per second:  47, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.454913, mae: 0.499706, mean_q: -0.004301, mean_eps: 0.100000\n",
            "   42/8000: episode: 3, duration: 0.227s, episode steps:  10, steps per second:  44, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.394185, mae: 0.462674, mean_q: 0.065166, mean_eps: 0.100000\n",
            "   53/8000: episode: 4, duration: 0.207s, episode steps:  11, steps per second:  53, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.343291, mae: 0.444124, mean_q: 0.145824, mean_eps: 0.100000\n",
            "   64/8000: episode: 5, duration: 0.208s, episode steps:  11, steps per second:  53, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.300927, mae: 0.434711, mean_q: 0.229726, mean_eps: 0.100000\n",
            "   74/8000: episode: 6, duration: 0.202s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.260282, mae: 0.431767, mean_q: 0.322046, mean_eps: 0.100000\n",
            "   85/8000: episode: 7, duration: 0.209s, episode steps:  11, steps per second:  53, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.232007, mae: 0.432740, mean_q: 0.410795, mean_eps: 0.100000\n",
            "   95/8000: episode: 8, duration: 0.179s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.204443, mae: 0.429671, mean_q: 0.514877, mean_eps: 0.100000\n",
            "  105/8000: episode: 9, duration: 0.177s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.198044, mae: 0.449696, mean_q: 0.581877, mean_eps: 0.100000\n",
            "  114/8000: episode: 10, duration: 0.147s, episode steps:   9, steps per second:  61, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.194953, mae: 0.481057, mean_q: 0.657106, mean_eps: 0.100000\n",
            "  124/8000: episode: 11, duration: 0.192s, episode steps:  10, steps per second:  52, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.178288, mae: 0.497638, mean_q: 0.757511, mean_eps: 0.100000\n",
            "  133/8000: episode: 12, duration: 0.165s, episode steps:   9, steps per second:  55, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.172955, mae: 0.524236, mean_q: 0.862490, mean_eps: 0.100000\n",
            "  143/8000: episode: 13, duration: 0.182s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.163248, mae: 0.541302, mean_q: 0.896962, mean_eps: 0.100000\n",
            "  152/8000: episode: 14, duration: 0.178s, episode steps:   9, steps per second:  51, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.161501, mae: 0.574162, mean_q: 1.014306, mean_eps: 0.100000\n",
            "  163/8000: episode: 15, duration: 0.237s, episode steps:  11, steps per second:  46, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.180006, mae: 0.632821, mean_q: 1.115920, mean_eps: 0.100000\n",
            "  172/8000: episode: 16, duration: 0.227s, episode steps:   9, steps per second:  40, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.160275, mae: 0.645574, mean_q: 1.188830, mean_eps: 0.100000\n",
            "  184/8000: episode: 17, duration: 0.346s, episode steps:  12, steps per second:  35, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.180713, mae: 0.703243, mean_q: 1.267012, mean_eps: 0.100000\n",
            "  193/8000: episode: 18, duration: 0.247s, episode steps:   9, steps per second:  36, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.177766, mae: 0.715980, mean_q: 1.250349, mean_eps: 0.100000\n",
            "  203/8000: episode: 19, duration: 0.274s, episode steps:  10, steps per second:  36, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.193603, mae: 0.776294, mean_q: 1.392066, mean_eps: 0.100000\n",
            "  214/8000: episode: 20, duration: 0.301s, episode steps:  11, steps per second:  37, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.210243, mae: 0.833838, mean_q: 1.465343, mean_eps: 0.100000\n",
            "  224/8000: episode: 21, duration: 0.252s, episode steps:  10, steps per second:  40, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.206895, mae: 0.855189, mean_q: 1.442416, mean_eps: 0.100000\n",
            "  232/8000: episode: 22, duration: 0.199s, episode steps:   8, steps per second:  40, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.207428, mae: 0.895634, mean_q: 1.534741, mean_eps: 0.100000\n",
            "  242/8000: episode: 23, duration: 0.250s, episode steps:  10, steps per second:  40, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.229889, mae: 0.940525, mean_q: 1.604058, mean_eps: 0.100000\n",
            "  251/8000: episode: 24, duration: 0.243s, episode steps:   9, steps per second:  37, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.243431, mae: 0.980895, mean_q: 1.643015, mean_eps: 0.100000\n",
            "  261/8000: episode: 25, duration: 0.265s, episode steps:  10, steps per second:  38, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.226168, mae: 1.012905, mean_q: 1.737149, mean_eps: 0.100000\n",
            "  274/8000: episode: 26, duration: 0.314s, episode steps:  13, steps per second:  41, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.846 [0.000, 1.000],  loss: 0.192309, mae: 1.015434, mean_q: 1.776531, mean_eps: 0.100000\n",
            "  282/8000: episode: 27, duration: 0.166s, episode steps:   8, steps per second:  48, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.209871, mae: 1.070779, mean_q: 1.874285, mean_eps: 0.100000\n",
            "  290/8000: episode: 28, duration: 0.176s, episode steps:   8, steps per second:  45, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.218915, mae: 1.116964, mean_q: 1.940763, mean_eps: 0.100000\n",
            "  301/8000: episode: 29, duration: 0.228s, episode steps:  11, steps per second:  48, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.230659, mae: 1.164639, mean_q: 1.985264, mean_eps: 0.100000\n",
            "  310/8000: episode: 30, duration: 0.181s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.219509, mae: 1.189015, mean_q: 2.034183, mean_eps: 0.100000\n",
            "  322/8000: episode: 31, duration: 0.230s, episode steps:  12, steps per second:  52, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.232602, mae: 1.204042, mean_q: 2.092421, mean_eps: 0.100000\n",
            "  331/8000: episode: 32, duration: 0.183s, episode steps:   9, steps per second:  49, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.226937, mae: 1.238446, mean_q: 2.182694, mean_eps: 0.100000\n",
            "  343/8000: episode: 33, duration: 0.252s, episode steps:  12, steps per second:  48, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.238130, mae: 1.279895, mean_q: 2.243112, mean_eps: 0.100000\n",
            "  352/8000: episode: 34, duration: 0.163s, episode steps:   9, steps per second:  55, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.270135, mae: 1.297527, mean_q: 2.261536, mean_eps: 0.100000\n",
            "  361/8000: episode: 35, duration: 0.191s, episode steps:   9, steps per second:  47, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.215787, mae: 1.297909, mean_q: 2.351784, mean_eps: 0.100000\n",
            "  370/8000: episode: 36, duration: 0.177s, episode steps:   9, steps per second:  51, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.203068, mae: 1.283886, mean_q: 2.416403, mean_eps: 0.100000\n",
            "  380/8000: episode: 37, duration: 0.191s, episode steps:  10, steps per second:  52, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.237485, mae: 1.323168, mean_q: 2.469885, mean_eps: 0.100000\n",
            "  390/8000: episode: 38, duration: 0.193s, episode steps:  10, steps per second:  52, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.286459, mae: 1.373184, mean_q: 2.585810, mean_eps: 0.100000\n",
            "  401/8000: episode: 39, duration: 0.226s, episode steps:  11, steps per second:  49, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.312736, mae: 1.376337, mean_q: 2.633307, mean_eps: 0.100000\n",
            "  411/8000: episode: 40, duration: 0.187s, episode steps:  10, steps per second:  54, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.246772, mae: 1.304334, mean_q: 2.714429, mean_eps: 0.100000\n",
            "  420/8000: episode: 41, duration: 0.180s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.279798, mae: 1.314869, mean_q: 2.798143, mean_eps: 0.100000\n",
            "  429/8000: episode: 42, duration: 0.184s, episode steps:   9, steps per second:  49, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.232163, mae: 1.280278, mean_q: 2.810456, mean_eps: 0.100000\n",
            "  439/8000: episode: 43, duration: 0.250s, episode steps:  10, steps per second:  40, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.280509, mae: 1.342386, mean_q: 2.862686, mean_eps: 0.100000\n",
            "  449/8000: episode: 44, duration: 0.261s, episode steps:  10, steps per second:  38, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.303296, mae: 1.429769, mean_q: 2.926803, mean_eps: 0.100000\n",
            "  458/8000: episode: 45, duration: 0.201s, episode steps:   9, steps per second:  45, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.271406, mae: 1.408778, mean_q: 2.919533, mean_eps: 0.100000\n",
            "  469/8000: episode: 46, duration: 0.263s, episode steps:  11, steps per second:  42, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.298616, mae: 1.479454, mean_q: 3.002274, mean_eps: 0.100000\n",
            "  479/8000: episode: 47, duration: 0.257s, episode steps:  10, steps per second:  39, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.330305, mae: 1.514105, mean_q: 3.086799, mean_eps: 0.100000\n",
            "  489/8000: episode: 48, duration: 0.275s, episode steps:  10, steps per second:  36, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.248699, mae: 1.514380, mean_q: 3.164260, mean_eps: 0.100000\n",
            "  498/8000: episode: 49, duration: 0.206s, episode steps:   9, steps per second:  44, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.297993, mae: 1.501771, mean_q: 3.240640, mean_eps: 0.100000\n",
            "  509/8000: episode: 50, duration: 0.219s, episode steps:  11, steps per second:  50, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.335390, mae: 1.558389, mean_q: 3.270811, mean_eps: 0.100000\n",
            "  522/8000: episode: 51, duration: 0.304s, episode steps:  13, steps per second:  43, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.359605, mae: 1.593824, mean_q: 3.269763, mean_eps: 0.100000\n",
            "  532/8000: episode: 52, duration: 0.261s, episode steps:  10, steps per second:  38, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.320496, mae: 1.584904, mean_q: 3.375853, mean_eps: 0.100000\n",
            "  543/8000: episode: 53, duration: 0.249s, episode steps:  11, steps per second:  44, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.364528, mae: 1.598410, mean_q: 3.475176, mean_eps: 0.100000\n",
            "  552/8000: episode: 54, duration: 0.223s, episode steps:   9, steps per second:  40, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.259011, mae: 1.562138, mean_q: 3.495874, mean_eps: 0.100000\n",
            "  561/8000: episode: 55, duration: 0.201s, episode steps:   9, steps per second:  45, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.342546, mae: 1.604351, mean_q: 3.580497, mean_eps: 0.100000\n",
            "  569/8000: episode: 56, duration: 0.174s, episode steps:   8, steps per second:  46, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.378370, mae: 1.629848, mean_q: 3.619945, mean_eps: 0.100000\n",
            "  578/8000: episode: 57, duration: 0.221s, episode steps:   9, steps per second:  41, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.335948, mae: 1.673147, mean_q: 3.629583, mean_eps: 0.100000\n",
            "  590/8000: episode: 58, duration: 0.236s, episode steps:  12, steps per second:  51, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 0.358872, mae: 1.704269, mean_q: 3.734240, mean_eps: 0.100000\n",
            "  599/8000: episode: 59, duration: 0.192s, episode steps:   9, steps per second:  47, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.332012, mae: 1.764101, mean_q: 3.800317, mean_eps: 0.100000\n",
            "  611/8000: episode: 60, duration: 0.232s, episode steps:  12, steps per second:  52, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.350038, mae: 1.762630, mean_q: 3.848855, mean_eps: 0.100000\n",
            "  620/8000: episode: 61, duration: 0.139s, episode steps:   9, steps per second:  65, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.287036, mae: 1.761566, mean_q: 3.913748, mean_eps: 0.100000\n",
            "  631/8000: episode: 62, duration: 0.200s, episode steps:  11, steps per second:  55, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.359897, mae: 1.784503, mean_q: 3.946263, mean_eps: 0.100000\n",
            "  640/8000: episode: 63, duration: 0.156s, episode steps:   9, steps per second:  58, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.412707, mae: 1.845117, mean_q: 4.031307, mean_eps: 0.100000\n",
            "  649/8000: episode: 64, duration: 0.156s, episode steps:   9, steps per second:  58, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.349644, mae: 1.842380, mean_q: 4.022219, mean_eps: 0.100000\n",
            "  658/8000: episode: 65, duration: 0.164s, episode steps:   9, steps per second:  55, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.321473, mae: 1.837707, mean_q: 4.096370, mean_eps: 0.100000\n",
            "  667/8000: episode: 66, duration: 0.171s, episode steps:   9, steps per second:  53, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.400732, mae: 1.900035, mean_q: 4.154961, mean_eps: 0.100000\n",
            "  676/8000: episode: 67, duration: 0.160s, episode steps:   9, steps per second:  56, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.420283, mae: 1.939131, mean_q: 4.116183, mean_eps: 0.100000\n",
            "  685/8000: episode: 68, duration: 0.156s, episode steps:   9, steps per second:  58, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.358087, mae: 1.932225, mean_q: 4.157751, mean_eps: 0.100000\n",
            "  695/8000: episode: 69, duration: 0.164s, episode steps:  10, steps per second:  61, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.384539, mae: 1.951001, mean_q: 4.321688, mean_eps: 0.100000\n",
            "  704/8000: episode: 70, duration: 0.164s, episode steps:   9, steps per second:  55, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.372435, mae: 2.000155, mean_q: 4.334282, mean_eps: 0.100000\n",
            "  714/8000: episode: 71, duration: 0.172s, episode steps:  10, steps per second:  58, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.417657, mae: 2.016781, mean_q: 4.372246, mean_eps: 0.100000\n",
            "  725/8000: episode: 72, duration: 0.185s, episode steps:  11, steps per second:  59, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.348212, mae: 1.991849, mean_q: 4.357251, mean_eps: 0.100000\n",
            "  735/8000: episode: 73, duration: 0.172s, episode steps:  10, steps per second:  58, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.414689, mae: 2.043038, mean_q: 4.467695, mean_eps: 0.100000\n",
            "  744/8000: episode: 74, duration: 0.169s, episode steps:   9, steps per second:  53, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.309173, mae: 2.026134, mean_q: 4.551637, mean_eps: 0.100000\n",
            "  755/8000: episode: 75, duration: 0.191s, episode steps:  11, steps per second:  58, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.351131, mae: 2.041693, mean_q: 4.579386, mean_eps: 0.100000\n",
            "  763/8000: episode: 76, duration: 0.155s, episode steps:   8, steps per second:  51, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.367558, mae: 2.101829, mean_q: 4.648863, mean_eps: 0.100000\n",
            "  773/8000: episode: 77, duration: 0.172s, episode steps:  10, steps per second:  58, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.337125, mae: 2.053477, mean_q: 4.685270, mean_eps: 0.100000\n",
            "  781/8000: episode: 78, duration: 0.166s, episode steps:   8, steps per second:  48, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.467957, mae: 2.109386, mean_q: 4.500497, mean_eps: 0.100000\n",
            "  791/8000: episode: 79, duration: 0.177s, episode steps:  10, steps per second:  57, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.344170, mae: 2.089007, mean_q: 4.689345, mean_eps: 0.100000\n",
            "  802/8000: episode: 80, duration: 0.218s, episode steps:  11, steps per second:  50, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.321779, mae: 2.104240, mean_q: 4.883143, mean_eps: 0.100000\n",
            "  814/8000: episode: 81, duration: 0.191s, episode steps:  12, steps per second:  63, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.332628, mae: 2.099934, mean_q: 4.844249, mean_eps: 0.100000\n",
            "  823/8000: episode: 82, duration: 0.175s, episode steps:   9, steps per second:  52, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.387180, mae: 2.178472, mean_q: 4.757052, mean_eps: 0.100000\n",
            "  832/8000: episode: 83, duration: 0.162s, episode steps:   9, steps per second:  56, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.339744, mae: 2.213916, mean_q: 4.870568, mean_eps: 0.100000\n",
            "  842/8000: episode: 84, duration: 0.163s, episode steps:  10, steps per second:  61, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.273387, mae: 2.219066, mean_q: 5.022868, mean_eps: 0.100000\n",
            "  853/8000: episode: 85, duration: 0.210s, episode steps:  11, steps per second:  52, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.311303, mae: 2.256820, mean_q: 5.015563, mean_eps: 0.100000\n",
            "  863/8000: episode: 86, duration: 0.165s, episode steps:  10, steps per second:  61, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.329110, mae: 2.292022, mean_q: 5.028986, mean_eps: 0.100000\n",
            "  871/8000: episode: 87, duration: 0.161s, episode steps:   8, steps per second:  50, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.875 [0.000, 1.000],  loss: 0.378335, mae: 2.300572, mean_q: 4.926621, mean_eps: 0.100000\n",
            "  879/8000: episode: 88, duration: 0.153s, episode steps:   8, steps per second:  52, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.346655, mae: 2.285497, mean_q: 4.918025, mean_eps: 0.100000\n",
            "  892/8000: episode: 89, duration: 0.224s, episode steps:  13, steps per second:  58, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.327052, mae: 2.274675, mean_q: 5.047594, mean_eps: 0.100000\n",
            "  903/8000: episode: 90, duration: 0.242s, episode steps:  11, steps per second:  45, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.283937, mae: 2.267158, mean_q: 4.994716, mean_eps: 0.100000\n",
            "  912/8000: episode: 91, duration: 0.205s, episode steps:   9, steps per second:  44, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.326993, mae: 2.310864, mean_q: 5.158221, mean_eps: 0.100000\n",
            "  922/8000: episode: 92, duration: 0.228s, episode steps:  10, steps per second:  44, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.347195, mae: 2.336023, mean_q: 5.237509, mean_eps: 0.100000\n",
            "  934/8000: episode: 93, duration: 0.253s, episode steps:  12, steps per second:  47, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.360933, mae: 2.370193, mean_q: 5.052526, mean_eps: 0.100000\n",
            "  944/8000: episode: 94, duration: 0.192s, episode steps:  10, steps per second:  52, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.318186, mae: 2.401726, mean_q: 5.316404, mean_eps: 0.100000\n",
            "  954/8000: episode: 95, duration: 0.210s, episode steps:  10, steps per second:  48, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.304417, mae: 2.370469, mean_q: 5.215685, mean_eps: 0.100000\n",
            "  963/8000: episode: 96, duration: 0.187s, episode steps:   9, steps per second:  48, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.294840, mae: 2.449651, mean_q: 5.353998, mean_eps: 0.100000\n",
            "  974/8000: episode: 97, duration: 0.222s, episode steps:  11, steps per second:  50, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.269737, mae: 2.479592, mean_q: 5.458591, mean_eps: 0.100000\n",
            "  985/8000: episode: 98, duration: 0.207s, episode steps:  11, steps per second:  53, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.343518, mae: 2.522996, mean_q: 5.266336, mean_eps: 0.100000\n",
            "  993/8000: episode: 99, duration: 0.155s, episode steps:   8, steps per second:  52, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.875 [0.000, 1.000],  loss: 0.272852, mae: 2.465382, mean_q: 5.181637, mean_eps: 0.100000\n",
            " 1005/8000: episode: 100, duration: 0.261s, episode steps:  12, steps per second:  46, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.245813, mae: 2.528180, mean_q: 5.439083, mean_eps: 0.100000\n",
            " 1014/8000: episode: 101, duration: 0.160s, episode steps:   9, steps per second:  56, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.259321, mae: 2.563572, mean_q: 5.476855, mean_eps: 0.100000\n",
            " 1024/8000: episode: 102, duration: 0.183s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.245941, mae: 2.600186, mean_q: 5.541127, mean_eps: 0.100000\n",
            " 1037/8000: episode: 103, duration: 0.275s, episode steps:  13, steps per second:  47, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.281713, mae: 2.570567, mean_q: 5.362263, mean_eps: 0.100000\n",
            " 1046/8000: episode: 104, duration: 0.186s, episode steps:   9, steps per second:  48, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.224669, mae: 2.595991, mean_q: 5.481509, mean_eps: 0.100000\n",
            " 1056/8000: episode: 105, duration: 0.248s, episode steps:  10, steps per second:  40, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.316194, mae: 2.676473, mean_q: 5.625728, mean_eps: 0.100000\n",
            " 1064/8000: episode: 106, duration: 0.179s, episode steps:   8, steps per second:  45, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.875 [0.000, 1.000],  loss: 0.233079, mae: 2.728931, mean_q: 5.795599, mean_eps: 0.100000\n",
            " 1075/8000: episode: 107, duration: 0.221s, episode steps:  11, steps per second:  50, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.233500, mae: 2.680964, mean_q: 5.668476, mean_eps: 0.100000\n",
            " 1084/8000: episode: 108, duration: 0.188s, episode steps:   9, steps per second:  48, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.275390, mae: 2.792120, mean_q: 5.899206, mean_eps: 0.100000\n",
            " 1093/8000: episode: 109, duration: 0.166s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.305783, mae: 2.749407, mean_q: 5.725158, mean_eps: 0.100000\n",
            " 1104/8000: episode: 110, duration: 0.220s, episode steps:  11, steps per second:  50, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.253277, mae: 2.729099, mean_q: 5.679354, mean_eps: 0.100000\n",
            " 1114/8000: episode: 111, duration: 0.195s, episode steps:  10, steps per second:  51, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.310470, mae: 2.745722, mean_q: 5.683383, mean_eps: 0.100000\n",
            " 1125/8000: episode: 112, duration: 0.216s, episode steps:  11, steps per second:  51, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.271573, mae: 2.731544, mean_q: 5.623795, mean_eps: 0.100000\n",
            " 1136/8000: episode: 113, duration: 0.218s, episode steps:  11, steps per second:  51, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.220548, mae: 2.769844, mean_q: 5.881076, mean_eps: 0.100000\n",
            " 1145/8000: episode: 114, duration: 0.142s, episode steps:   9, steps per second:  64, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.262203, mae: 2.797043, mean_q: 5.851697, mean_eps: 0.100000\n",
            " 1156/8000: episode: 115, duration: 0.215s, episode steps:  11, steps per second:  51, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.293548, mae: 2.867838, mean_q: 5.891123, mean_eps: 0.100000\n",
            " 1166/8000: episode: 116, duration: 0.195s, episode steps:  10, steps per second:  51, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.245696, mae: 2.857278, mean_q: 5.858612, mean_eps: 0.100000\n",
            " 1176/8000: episode: 117, duration: 0.188s, episode steps:  10, steps per second:  53, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.211020, mae: 2.860082, mean_q: 5.896492, mean_eps: 0.100000\n",
            " 1184/8000: episode: 118, duration: 0.134s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.183642, mae: 2.913033, mean_q: 5.992189, mean_eps: 0.100000\n",
            " 1192/8000: episode: 119, duration: 0.137s, episode steps:   8, steps per second:  58, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.189516, mae: 2.888334, mean_q: 5.894731, mean_eps: 0.100000\n",
            " 1204/8000: episode: 120, duration: 0.179s, episode steps:  12, steps per second:  67, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.274472, mae: 2.996575, mean_q: 6.047855, mean_eps: 0.100000\n",
            " 1217/8000: episode: 121, duration: 0.223s, episode steps:  13, steps per second:  58, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.277697, mae: 2.898733, mean_q: 5.784141, mean_eps: 0.100000\n",
            " 1227/8000: episode: 122, duration: 0.205s, episode steps:  10, steps per second:  49, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.219608, mae: 2.923234, mean_q: 5.908673, mean_eps: 0.100000\n",
            " 1239/8000: episode: 123, duration: 0.196s, episode steps:  12, steps per second:  61, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.214127, mae: 3.001190, mean_q: 6.103622, mean_eps: 0.100000\n",
            " 1252/8000: episode: 124, duration: 0.228s, episode steps:  13, steps per second:  57, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.290384, mae: 2.975771, mean_q: 5.922048, mean_eps: 0.100000\n",
            " 1261/8000: episode: 125, duration: 0.178s, episode steps:   9, steps per second:  51, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.242278, mae: 2.928760, mean_q: 5.822333, mean_eps: 0.100000\n",
            " 1270/8000: episode: 126, duration: 0.148s, episode steps:   9, steps per second:  61, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.233301, mae: 3.018818, mean_q: 6.029452, mean_eps: 0.100000\n",
            " 1280/8000: episode: 127, duration: 0.158s, episode steps:  10, steps per second:  63, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.242483, mae: 3.041914, mean_q: 6.009772, mean_eps: 0.100000\n",
            " 1293/8000: episode: 128, duration: 0.203s, episode steps:  13, steps per second:  64, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.200643, mae: 3.151795, mean_q: 6.258869, mean_eps: 0.100000\n",
            " 1304/8000: episode: 129, duration: 0.183s, episode steps:  11, steps per second:  60, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.214074, mae: 3.139931, mean_q: 6.191063, mean_eps: 0.100000\n",
            " 1314/8000: episode: 130, duration: 0.161s, episode steps:  10, steps per second:  62, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.209727, mae: 3.109023, mean_q: 6.118369, mean_eps: 0.100000\n",
            " 1326/8000: episode: 131, duration: 0.192s, episode steps:  12, steps per second:  62, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.260287, mae: 3.213550, mean_q: 6.280741, mean_eps: 0.100000\n",
            " 1340/8000: episode: 132, duration: 0.229s, episode steps:  14, steps per second:  61, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.264648, mae: 3.168778, mean_q: 6.160781, mean_eps: 0.100000\n",
            " 1351/8000: episode: 133, duration: 0.185s, episode steps:  11, steps per second:  59, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.235311, mae: 3.176568, mean_q: 6.179208, mean_eps: 0.100000\n",
            " 1364/8000: episode: 134, duration: 0.213s, episode steps:  13, steps per second:  61, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.212466, mae: 3.217870, mean_q: 6.290605, mean_eps: 0.100000\n",
            " 1374/8000: episode: 135, duration: 0.150s, episode steps:  10, steps per second:  67, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.325478, mae: 3.159901, mean_q: 6.069199, mean_eps: 0.100000\n",
            " 1385/8000: episode: 136, duration: 0.186s, episode steps:  11, steps per second:  59, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.212297, mae: 3.245729, mean_q: 6.318531, mean_eps: 0.100000\n",
            " 1403/8000: episode: 137, duration: 0.289s, episode steps:  18, steps per second:  62, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.225840, mae: 3.312922, mean_q: 6.438921, mean_eps: 0.100000\n",
            " 1426/8000: episode: 138, duration: 0.348s, episode steps:  23, steps per second:  66, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.270400, mae: 3.374170, mean_q: 6.483056, mean_eps: 0.100000\n",
            " 1441/8000: episode: 139, duration: 0.240s, episode steps:  15, steps per second:  62, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.250299, mae: 3.457736, mean_q: 6.680563, mean_eps: 0.100000\n",
            " 1464/8000: episode: 140, duration: 0.378s, episode steps:  23, steps per second:  61, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.218629, mae: 3.394501, mean_q: 6.531177, mean_eps: 0.100000\n",
            " 1492/8000: episode: 141, duration: 0.414s, episode steps:  28, steps per second:  68, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 0.287427, mae: 3.483201, mean_q: 6.648421, mean_eps: 0.100000\n",
            " 1507/8000: episode: 142, duration: 0.248s, episode steps:  15, steps per second:  60, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.278718, mae: 3.492759, mean_q: 6.648559, mean_eps: 0.100000\n",
            " 1536/8000: episode: 143, duration: 0.452s, episode steps:  29, steps per second:  64, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 0.251495, mae: 3.520397, mean_q: 6.738907, mean_eps: 0.100000\n",
            " 1598/8000: episode: 144, duration: 0.930s, episode steps:  62, steps per second:  67, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 0.230082, mae: 3.678964, mean_q: 7.104682, mean_eps: 0.100000\n",
            " 1621/8000: episode: 145, duration: 0.343s, episode steps:  23, steps per second:  67, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.391995, mae: 3.822351, mean_q: 7.339220, mean_eps: 0.100000\n",
            " 1645/8000: episode: 146, duration: 0.359s, episode steps:  24, steps per second:  67, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.212761, mae: 3.919206, mean_q: 7.606081, mean_eps: 0.100000\n",
            " 1662/8000: episode: 147, duration: 0.251s, episode steps:  17, steps per second:  68, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.532259, mae: 4.071120, mean_q: 7.803243, mean_eps: 0.100000\n",
            " 1676/8000: episode: 148, duration: 0.201s, episode steps:  14, steps per second:  70, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.596653, mae: 4.097120, mean_q: 7.876824, mean_eps: 0.100000\n",
            " 1691/8000: episode: 149, duration: 0.242s, episode steps:  15, steps per second:  62, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.592843, mae: 3.972190, mean_q: 7.579439, mean_eps: 0.100000\n",
            " 1700/8000: episode: 150, duration: 0.138s, episode steps:   9, steps per second:  65, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.797484, mae: 4.252465, mean_q: 8.153374, mean_eps: 0.100000\n",
            " 1709/8000: episode: 151, duration: 0.151s, episode steps:   9, steps per second:  59, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.603083, mae: 4.163659, mean_q: 8.019305, mean_eps: 0.100000\n",
            " 1719/8000: episode: 152, duration: 0.143s, episode steps:  10, steps per second:  70, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.216091, mae: 4.146853, mean_q: 8.050984, mean_eps: 0.100000\n",
            " 1728/8000: episode: 153, duration: 0.154s, episode steps:   9, steps per second:  58, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.245456, mae: 4.206682, mean_q: 8.190583, mean_eps: 0.100000\n",
            " 1736/8000: episode: 154, duration: 0.126s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.058259, mae: 4.342261, mean_q: 8.354020, mean_eps: 0.100000\n",
            " 1746/8000: episode: 155, duration: 0.170s, episode steps:  10, steps per second:  59, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.880294, mae: 4.417916, mean_q: 8.568082, mean_eps: 0.100000\n",
            " 1757/8000: episode: 156, duration: 0.199s, episode steps:  11, steps per second:  55, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.905066, mae: 4.363001, mean_q: 8.349381, mean_eps: 0.100000\n",
            " 1766/8000: episode: 157, duration: 0.146s, episode steps:   9, steps per second:  62, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.568901, mae: 4.527149, mean_q: 8.647257, mean_eps: 0.100000\n",
            " 1776/8000: episode: 158, duration: 0.166s, episode steps:  10, steps per second:  60, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.764305, mae: 4.458831, mean_q: 8.675244, mean_eps: 0.100000\n",
            " 1788/8000: episode: 159, duration: 0.206s, episode steps:  12, steps per second:  58, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.802506, mae: 4.476124, mean_q: 8.725758, mean_eps: 0.100000\n",
            " 1803/8000: episode: 160, duration: 0.267s, episode steps:  15, steps per second:  56, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 1.683680, mae: 4.697762, mean_q: 9.005424, mean_eps: 0.100000\n",
            " 1812/8000: episode: 161, duration: 0.166s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.644766, mae: 4.647952, mean_q: 8.927247, mean_eps: 0.100000\n",
            " 1822/8000: episode: 162, duration: 0.195s, episode steps:  10, steps per second:  51, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.686494, mae: 4.699329, mean_q: 9.015999, mean_eps: 0.100000\n",
            " 1832/8000: episode: 163, duration: 0.162s, episode steps:  10, steps per second:  62, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.319841, mae: 4.813227, mean_q: 9.175972, mean_eps: 0.100000\n",
            " 1842/8000: episode: 164, duration: 0.168s, episode steps:  10, steps per second:  60, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.698092, mae: 4.689731, mean_q: 8.923826, mean_eps: 0.100000\n",
            " 1852/8000: episode: 165, duration: 0.164s, episode steps:  10, steps per second:  61, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 1.387204, mae: 4.808691, mean_q: 9.117268, mean_eps: 0.100000\n",
            " 1863/8000: episode: 166, duration: 0.161s, episode steps:  11, steps per second:  68, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 1.360518, mae: 4.958605, mean_q: 9.472390, mean_eps: 0.100000\n",
            " 1878/8000: episode: 167, duration: 0.229s, episode steps:  15, steps per second:  66, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 2.006206, mae: 5.036950, mean_q: 9.521055, mean_eps: 0.100000\n",
            " 1887/8000: episode: 168, duration: 0.127s, episode steps:   9, steps per second:  71, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.983470, mae: 4.957402, mean_q: 9.445933, mean_eps: 0.100000\n",
            " 1897/8000: episode: 169, duration: 0.148s, episode steps:  10, steps per second:  67, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 2.743418, mae: 5.072136, mean_q: 9.448706, mean_eps: 0.100000\n",
            " 1906/8000: episode: 170, duration: 0.135s, episode steps:   9, steps per second:  66, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 1.206326, mae: 4.955991, mean_q: 9.281886, mean_eps: 0.100000\n",
            " 1916/8000: episode: 171, duration: 0.165s, episode steps:  10, steps per second:  60, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.913429, mae: 5.196388, mean_q: 9.723743, mean_eps: 0.100000\n",
            " 1925/8000: episode: 172, duration: 0.135s, episode steps:   9, steps per second:  66, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.305633, mae: 4.886976, mean_q: 9.331490, mean_eps: 0.100000\n",
            " 1934/8000: episode: 173, duration: 0.151s, episode steps:   9, steps per second:  60, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.049734, mae: 5.005075, mean_q: 9.517600, mean_eps: 0.100000\n",
            " 1943/8000: episode: 174, duration: 0.132s, episode steps:   9, steps per second:  68, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 2.296868, mae: 5.208043, mean_q: 9.748240, mean_eps: 0.100000\n",
            " 1954/8000: episode: 175, duration: 0.192s, episode steps:  11, steps per second:  57, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 1.430260, mae: 5.127538, mean_q: 9.728024, mean_eps: 0.100000\n",
            " 1967/8000: episode: 176, duration: 0.212s, episode steps:  13, steps per second:  61, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 2.035032, mae: 5.162133, mean_q: 9.777555, mean_eps: 0.100000\n",
            " 1979/8000: episode: 177, duration: 0.197s, episode steps:  12, steps per second:  61, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 1.339892, mae: 5.174120, mean_q: 9.806154, mean_eps: 0.100000\n",
            " 1989/8000: episode: 178, duration: 0.165s, episode steps:  10, steps per second:  61, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 2.276830, mae: 5.381513, mean_q: 10.092523, mean_eps: 0.100000\n",
            " 2001/8000: episode: 179, duration: 0.193s, episode steps:  12, steps per second:  62, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 2.516789, mae: 5.418984, mean_q: 10.100661, mean_eps: 0.100000\n",
            " 2012/8000: episode: 180, duration: 0.167s, episode steps:  11, steps per second:  66, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 1.782638, mae: 5.315860, mean_q: 10.021721, mean_eps: 0.100000\n",
            " 2025/8000: episode: 181, duration: 0.232s, episode steps:  13, steps per second:  56, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 1.714177, mae: 5.397347, mean_q: 10.262165, mean_eps: 0.100000\n",
            " 2036/8000: episode: 182, duration: 0.199s, episode steps:  11, steps per second:  55, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 1.916450, mae: 5.339439, mean_q: 10.028305, mean_eps: 0.100000\n",
            " 2049/8000: episode: 183, duration: 0.203s, episode steps:  13, steps per second:  64, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 1.964606, mae: 5.482416, mean_q: 10.225543, mean_eps: 0.100000\n",
            " 2060/8000: episode: 184, duration: 0.174s, episode steps:  11, steps per second:  63, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 3.238810, mae: 5.756354, mean_q: 10.638807, mean_eps: 0.100000\n",
            " 2076/8000: episode: 185, duration: 0.253s, episode steps:  16, steps per second:  63, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 1.375817, mae: 5.321593, mean_q: 10.007314, mean_eps: 0.100000\n",
            " 2093/8000: episode: 186, duration: 0.249s, episode steps:  17, steps per second:  68, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 2.635306, mae: 5.343512, mean_q: 9.843146, mean_eps: 0.100000\n",
            " 2143/8000: episode: 187, duration: 0.719s, episode steps:  50, steps per second:  70, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 1.910782, mae: 5.449539, mean_q: 10.158030, mean_eps: 0.100000\n",
            " 2159/8000: episode: 188, duration: 0.253s, episode steps:  16, steps per second:  63, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 1.948103, mae: 5.484856, mean_q: 10.260828, mean_eps: 0.100000\n",
            " 2177/8000: episode: 189, duration: 0.291s, episode steps:  18, steps per second:  62, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 2.968186, mae: 5.625933, mean_q: 10.417056, mean_eps: 0.100000\n",
            " 2188/8000: episode: 190, duration: 0.171s, episode steps:  11, steps per second:  64, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 2.403863, mae: 5.572533, mean_q: 10.315861, mean_eps: 0.100000\n",
            " 2205/8000: episode: 191, duration: 0.297s, episode steps:  17, steps per second:  57, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 1.447127, mae: 5.586777, mean_q: 10.513029, mean_eps: 0.100000\n",
            " 2218/8000: episode: 192, duration: 0.224s, episode steps:  13, steps per second:  58, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 1.871386, mae: 5.433600, mean_q: 10.088445, mean_eps: 0.100000\n",
            " 2232/8000: episode: 193, duration: 0.238s, episode steps:  14, steps per second:  59, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 2.006460, mae: 5.516229, mean_q: 10.324303, mean_eps: 0.100000\n",
            " 2253/8000: episode: 194, duration: 0.311s, episode steps:  21, steps per second:  68, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 2.263709, mae: 5.555262, mean_q: 10.408176, mean_eps: 0.100000\n",
            " 2267/8000: episode: 195, duration: 0.218s, episode steps:  14, steps per second:  64, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 1.800442, mae: 5.524960, mean_q: 10.306496, mean_eps: 0.100000\n",
            " 2286/8000: episode: 196, duration: 0.287s, episode steps:  19, steps per second:  66, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 2.641786, mae: 5.689489, mean_q: 10.484045, mean_eps: 0.100000\n",
            " 2338/8000: episode: 197, duration: 0.746s, episode steps:  52, steps per second:  70, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 2.272626, mae: 5.654148, mean_q: 10.485327, mean_eps: 0.100000\n",
            " 2406/8000: episode: 198, duration: 0.989s, episode steps:  68, steps per second:  69, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 1.605813, mae: 5.715435, mean_q: 10.766093, mean_eps: 0.100000\n",
            " 2470/8000: episode: 199, duration: 0.911s, episode steps:  64, steps per second:  70, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 1.715300, mae: 5.772824, mean_q: 10.921883, mean_eps: 0.100000\n",
            " 2550/8000: episode: 200, duration: 1.186s, episode steps:  80, steps per second:  67, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 1.602693, mae: 5.807373, mean_q: 10.981522, mean_eps: 0.100000\n",
            " 2600/8000: episode: 201, duration: 0.726s, episode steps:  50, steps per second:  69, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 1.432334, mae: 5.875613, mean_q: 11.157045, mean_eps: 0.100000\n",
            " 2637/8000: episode: 202, duration: 0.518s, episode steps:  37, steps per second:  71, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 1.376301, mae: 5.979671, mean_q: 11.366326, mean_eps: 0.100000\n",
            " 2707/8000: episode: 203, duration: 1.030s, episode steps:  70, steps per second:  68, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1.567781, mae: 6.014406, mean_q: 11.411489, mean_eps: 0.100000\n",
            " 2783/8000: episode: 204, duration: 1.119s, episode steps:  76, steps per second:  68, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 1.522215, mae: 6.128088, mean_q: 11.620090, mean_eps: 0.100000\n",
            " 2905/8000: episode: 205, duration: 1.826s, episode steps: 122, steps per second:  67, episode reward: 122.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 1.338248, mae: 6.187694, mean_q: 11.807208, mean_eps: 0.100000\n",
            " 2970/8000: episode: 206, duration: 0.958s, episode steps:  65, steps per second:  68, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 1.302780, mae: 6.341320, mean_q: 12.127086, mean_eps: 0.100000\n",
            " 3048/8000: episode: 207, duration: 1.234s, episode steps:  78, steps per second:  63, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 1.668846, mae: 6.526822, mean_q: 12.440387, mean_eps: 0.100000\n",
            " 3142/8000: episode: 208, duration: 1.412s, episode steps:  94, steps per second:  67, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 1.317900, mae: 6.705022, mean_q: 12.898154, mean_eps: 0.100000\n",
            " 3248/8000: episode: 209, duration: 1.559s, episode steps: 106, steps per second:  68, episode reward: 106.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 1.486537, mae: 6.887768, mean_q: 13.228367, mean_eps: 0.100000\n",
            " 3311/8000: episode: 210, duration: 0.880s, episode steps:  63, steps per second:  72, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 1.333978, mae: 6.991061, mean_q: 13.465829, mean_eps: 0.100000\n",
            " 3377/8000: episode: 211, duration: 0.954s, episode steps:  66, steps per second:  69, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1.650149, mae: 7.171309, mean_q: 13.812318, mean_eps: 0.100000\n",
            " 3478/8000: episode: 212, duration: 1.451s, episode steps: 101, steps per second:  70, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 1.619559, mae: 7.286529, mean_q: 14.013464, mean_eps: 0.100000\n",
            " 3558/8000: episode: 213, duration: 1.195s, episode steps:  80, steps per second:  67, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 1.558940, mae: 7.429954, mean_q: 14.352508, mean_eps: 0.100000\n",
            " 3637/8000: episode: 214, duration: 1.201s, episode steps:  79, steps per second:  66, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 1.447361, mae: 7.601410, mean_q: 14.689573, mean_eps: 0.100000\n",
            " 3833/8000: episode: 215, duration: 3.176s, episode steps: 196, steps per second:  62, episode reward: 196.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 1.694461, mae: 7.872319, mean_q: 15.227691, mean_eps: 0.100000\n",
            " 3906/8000: episode: 216, duration: 1.485s, episode steps:  73, steps per second:  49, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 1.950837, mae: 8.148139, mean_q: 15.712448, mean_eps: 0.100000\n",
            " 3986/8000: episode: 217, duration: 1.523s, episode steps:  80, steps per second:  53, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 2.150118, mae: 8.366215, mean_q: 16.152044, mean_eps: 0.100000\n",
            " 4037/8000: episode: 218, duration: 1.195s, episode steps:  51, steps per second:  43, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 2.597221, mae: 8.448948, mean_q: 16.210585, mean_eps: 0.100000\n",
            " 4096/8000: episode: 219, duration: 1.239s, episode steps:  59, steps per second:  48, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 1.787058, mae: 8.510084, mean_q: 16.484655, mean_eps: 0.100000\n",
            " 4195/8000: episode: 220, duration: 1.436s, episode steps:  99, steps per second:  69, episode reward: 99.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 2.321507, mae: 8.616350, mean_q: 16.649618, mean_eps: 0.100000\n",
            " 4304/8000: episode: 221, duration: 1.571s, episode steps: 109, steps per second:  69, episode reward: 109.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 1.969704, mae: 8.756612, mean_q: 16.968182, mean_eps: 0.100000\n",
            " 4361/8000: episode: 222, duration: 0.907s, episode steps:  57, steps per second:  63, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.456 [0.000, 1.000],  loss: 2.124859, mae: 9.015280, mean_q: 17.489100, mean_eps: 0.100000\n",
            " 4435/8000: episode: 223, duration: 1.079s, episode steps:  74, steps per second:  69, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 1.830461, mae: 9.036882, mean_q: 17.579213, mean_eps: 0.100000\n",
            " 4532/8000: episode: 224, duration: 1.451s, episode steps:  97, steps per second:  67, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 2.222583, mae: 9.344671, mean_q: 18.196015, mean_eps: 0.100000\n",
            " 4598/8000: episode: 225, duration: 0.954s, episode steps:  66, steps per second:  69, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 1.367927, mae: 9.462659, mean_q: 18.536671, mean_eps: 0.100000\n",
            " 4687/8000: episode: 226, duration: 1.220s, episode steps:  89, steps per second:  73, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 2.028225, mae: 9.685058, mean_q: 18.898847, mean_eps: 0.100000\n",
            " 4844/8000: episode: 227, duration: 2.175s, episode steps: 157, steps per second:  72, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 2.138998, mae: 9.857562, mean_q: 19.293741, mean_eps: 0.100000\n",
            " 4910/8000: episode: 228, duration: 1.122s, episode steps:  66, steps per second:  59, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 2.793843, mae: 10.136159, mean_q: 19.752051, mean_eps: 0.100000\n",
            " 5004/8000: episode: 229, duration: 1.474s, episode steps:  94, steps per second:  64, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 2.682963, mae: 10.211048, mean_q: 19.897899, mean_eps: 0.100000\n",
            " 5096/8000: episode: 230, duration: 1.340s, episode steps:  92, steps per second:  69, episode reward: 92.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.985852, mae: 10.375216, mean_q: 20.390976, mean_eps: 0.100000\n",
            " 5153/8000: episode: 231, duration: 0.841s, episode steps:  57, steps per second:  68, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 3.108891, mae: 10.539680, mean_q: 20.607955, mean_eps: 0.100000\n",
            " 5277/8000: episode: 232, duration: 1.644s, episode steps: 124, steps per second:  75, episode reward: 124.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 2.991388, mae: 10.655966, mean_q: 20.728020, mean_eps: 0.100000\n",
            " 5366/8000: episode: 233, duration: 1.216s, episode steps:  89, steps per second:  73, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 2.926436, mae: 10.903886, mean_q: 21.321403, mean_eps: 0.100000\n",
            " 5432/8000: episode: 234, duration: 0.901s, episode steps:  66, steps per second:  73, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 3.980141, mae: 11.065374, mean_q: 21.510801, mean_eps: 0.100000\n",
            " 5521/8000: episode: 235, duration: 1.223s, episode steps:  89, steps per second:  73, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 3.041420, mae: 11.176146, mean_q: 21.818981, mean_eps: 0.100000\n",
            " 5575/8000: episode: 236, duration: 0.740s, episode steps:  54, steps per second:  73, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 2.855050, mae: 11.334415, mean_q: 22.144899, mean_eps: 0.100000\n",
            " 5726/8000: episode: 237, duration: 2.042s, episode steps: 151, steps per second:  74, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 3.035710, mae: 11.501603, mean_q: 22.472819, mean_eps: 0.100000\n",
            " 5889/8000: episode: 238, duration: 2.594s, episode steps: 163, steps per second:  63, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 3.236309, mae: 11.801957, mean_q: 23.148439, mean_eps: 0.100000\n",
            " 5976/8000: episode: 239, duration: 1.233s, episode steps:  87, steps per second:  71, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 3.914914, mae: 12.166629, mean_q: 23.766452, mean_eps: 0.100000\n",
            " 6075/8000: episode: 240, duration: 1.413s, episode steps:  99, steps per second:  70, episode reward: 99.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 3.382374, mae: 12.297404, mean_q: 24.114562, mean_eps: 0.100000\n",
            " 6129/8000: episode: 241, duration: 0.746s, episode steps:  54, steps per second:  72, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 4.201767, mae: 12.281732, mean_q: 24.049974, mean_eps: 0.100000\n",
            " 6200/8000: episode: 242, duration: 1.041s, episode steps:  71, steps per second:  68, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 3.644348, mae: 12.421102, mean_q: 24.333397, mean_eps: 0.100000\n",
            " 6293/8000: episode: 243, duration: 1.262s, episode steps:  93, steps per second:  74, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 3.999131, mae: 12.600141, mean_q: 24.583280, mean_eps: 0.100000\n",
            " 6450/8000: episode: 244, duration: 2.110s, episode steps: 157, steps per second:  74, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 3.592566, mae: 12.739131, mean_q: 25.028965, mean_eps: 0.100000\n",
            " 6553/8000: episode: 245, duration: 1.397s, episode steps: 103, steps per second:  74, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 4.059478, mae: 13.073416, mean_q: 25.648954, mean_eps: 0.100000\n",
            " 6670/8000: episode: 246, duration: 1.595s, episode steps: 117, steps per second:  73, episode reward: 117.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 3.860132, mae: 13.298456, mean_q: 26.093060, mean_eps: 0.100000\n",
            " 6797/8000: episode: 247, duration: 1.711s, episode steps: 127, steps per second:  74, episode reward: 127.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 3.792066, mae: 13.464670, mean_q: 26.435023, mean_eps: 0.100000\n",
            " 6874/8000: episode: 248, duration: 1.088s, episode steps:  77, steps per second:  71, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 3.719630, mae: 13.592917, mean_q: 26.771827, mean_eps: 0.100000\n",
            " 7020/8000: episode: 249, duration: 2.160s, episode steps: 146, steps per second:  68, episode reward: 146.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 3.830100, mae: 13.712357, mean_q: 26.893493, mean_eps: 0.100000\n",
            " 7145/8000: episode: 250, duration: 1.867s, episode steps: 125, steps per second:  67, episode reward: 125.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 4.624759, mae: 14.030423, mean_q: 27.523308, mean_eps: 0.100000\n",
            " 7225/8000: episode: 251, duration: 1.130s, episode steps:  80, steps per second:  71, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 3.271732, mae: 14.119563, mean_q: 27.795468, mean_eps: 0.100000\n",
            " 7321/8000: episode: 252, duration: 1.330s, episode steps:  96, steps per second:  72, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 5.468982, mae: 14.213879, mean_q: 27.776414, mean_eps: 0.100000\n",
            " 7515/8000: episode: 253, duration: 2.999s, episode steps: 194, steps per second:  65, episode reward: 194.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 4.071373, mae: 14.345879, mean_q: 28.165618, mean_eps: 0.100000\n",
            " 7631/8000: episode: 254, duration: 2.238s, episode steps: 116, steps per second:  52, episode reward: 116.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 4.097192, mae: 14.588483, mean_q: 28.708551, mean_eps: 0.100000\n",
            " 7826/8000: episode: 255, duration: 2.882s, episode steps: 195, steps per second:  68, episode reward: 195.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 4.378797, mae: 14.803857, mean_q: 29.134130, mean_eps: 0.100000\n",
            " 7902/8000: episode: 256, duration: 1.432s, episode steps:  76, steps per second:  53, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.539 [0.000, 1.000],  loss: 4.645990, mae: 15.032597, mean_q: 29.696330, mean_eps: 0.100000\n",
            "done, took 140.060 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 164.000, steps: 164\n",
            "Episode 2: reward: 189.000, steps: 189\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 126.000, steps: 126\n",
            "Episode 5: reward: 200.000, steps: 200\n",
            "Episode 6: reward: 88.000, steps: 88\n",
            "Episode 7: reward: 120.000, steps: 120\n",
            "Episode 8: reward: 109.000, steps: 109\n",
            "Episode 9: reward: 104.000, steps: 104\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 100.000, steps: 100\n",
            "Episode 12: reward: 200.000, steps: 200\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 87.000, steps: 87\n",
            "Episode 15: reward: 161.000, steps: 161\n",
            "Episode 16: reward: 128.000, steps: 128\n",
            "Episode 17: reward: 200.000, steps: 200\n",
            "Episode 18: reward: 200.000, steps: 200\n",
            "Episode 19: reward: 180.000, steps: 180\n",
            "Episode 20: reward: 200.000, steps: 200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb171203c50>"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    }
  ]
}