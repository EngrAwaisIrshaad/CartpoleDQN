{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPoleDQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Megacity1/CartpoleDQN/blob/main/Week%203%20Deep%20RL%202/CartPoleDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Deep Q Network (DQN) for CartPole Using Boltzmann Q Policy\n",
        "This exercise implements a DQN for CartPole using a Boltzmann Q policy for selecting the actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGsC7cJ5jNcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda73cbf-3020-40f4-e45e-7b87053448f1"
      },
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.24.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (13.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.44.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.10.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMIHLgQ3Z-lF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5a8608-456f-4127-ce8a-f551e5f4e4f3"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0AMLzq08ap0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b380e1-6c3d-4bce-8233-a8ac01deaf26"
      },
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "!pip install Adam\n",
        "#from keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Adam in /usr/local/lib/python3.7/dist-packages (0.0.0.dev0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll6bNdUm54WS"
      },
      "source": [
        "Implementation of DQN for CartPole, applying policy BoltzmannQPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCrPKNy40PC"
      },
      "source": [
        "##Implement DQN with BoltzmannGumbelQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efM9jkXr5A3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "72c713b0-94da-4b27-c0e4-134418ffc84a"
      },
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import BoltzmannGumbelQPolicy\n",
        "from rl.policy import LinearAnnealedPolicy\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n",
        "# setup experience replay buffer\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
        "                               attr='eps',            \n",
        "                               value_max=10.,\n",
        "                               value_min=.10, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=20)\n",
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "# add extra layers here\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=20,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy) \n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=8000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_38\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_36 (Flatten)        (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_73 (Dense)            (None, 16)                80        \n",
            "                                                                 \n",
            " dense_74 (Dense)            (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 8000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   21/8000: episode: 1, duration: 8.688s, episode steps:  21, steps per second:   2, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   31/8000: episode: 2, duration: 0.148s, episode steps:  10, steps per second:  68, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.563174, mae: 0.606853, mean_q: 0.425691, mean_eps: 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   41/8000: episode: 3, duration: 0.141s, episode steps:  10, steps per second:  71, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.486645, mae: 0.611458, mean_q: 0.609697, mean_eps: 0.100000\n",
            "   51/8000: episode: 4, duration: 0.142s, episode steps:  10, steps per second:  70, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.440622, mae: 0.633399, mean_q: 0.767325, mean_eps: 0.100000\n",
            "   61/8000: episode: 5, duration: 0.161s, episode steps:  10, steps per second:  62, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.392721, mae: 0.646818, mean_q: 0.979382, mean_eps: 0.100000\n",
            "   72/8000: episode: 6, duration: 0.179s, episode steps:  11, steps per second:  62, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.367628, mae: 0.659089, mean_q: 1.098519, mean_eps: 0.100000\n",
            "   80/8000: episode: 7, duration: 0.126s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.390672, mae: 0.702268, mean_q: 1.265706, mean_eps: 0.100000\n",
            "   89/8000: episode: 8, duration: 0.140s, episode steps:   9, steps per second:  64, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.359542, mae: 0.698084, mean_q: 1.292046, mean_eps: 0.100000\n",
            "   99/8000: episode: 9, duration: 0.165s, episode steps:  10, steps per second:  61, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.372615, mae: 0.747595, mean_q: 1.396966, mean_eps: 0.100000\n",
            "  110/8000: episode: 10, duration: 0.150s, episode steps:  11, steps per second:  73, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.420429, mae: 0.796092, mean_q: 1.506403, mean_eps: 0.100000\n",
            "  120/8000: episode: 11, duration: 0.138s, episode steps:  10, steps per second:  72, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.403291, mae: 0.827591, mean_q: 1.562427, mean_eps: 0.100000\n",
            "  128/8000: episode: 12, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.388702, mae: 0.859775, mean_q: 1.653798, mean_eps: 0.100000\n",
            "  138/8000: episode: 13, duration: 0.144s, episode steps:  10, steps per second:  70, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.388157, mae: 0.889756, mean_q: 1.721525, mean_eps: 0.100000\n",
            "  146/8000: episode: 14, duration: 0.119s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.504244, mae: 0.986281, mean_q: 1.845831, mean_eps: 0.100000\n",
            "  156/8000: episode: 15, duration: 0.137s, episode steps:  10, steps per second:  73, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.363252, mae: 0.954608, mean_q: 1.758479, mean_eps: 0.100000\n",
            "  168/8000: episode: 16, duration: 0.166s, episode steps:  12, steps per second:  72, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.083 [0.000, 1.000],  loss: 0.364390, mae: 0.972163, mean_q: 1.853395, mean_eps: 0.100000\n",
            "  178/8000: episode: 17, duration: 0.146s, episode steps:  10, steps per second:  69, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.413078, mae: 1.035675, mean_q: 2.022585, mean_eps: 0.100000\n",
            "  186/8000: episode: 18, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.390203, mae: 1.066978, mean_q: 2.059681, mean_eps: 0.100000\n",
            "  194/8000: episode: 19, duration: 0.115s, episode steps:   8, steps per second:  70, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.472355, mae: 1.143002, mean_q: 2.128847, mean_eps: 0.100000\n",
            "  202/8000: episode: 20, duration: 0.116s, episode steps:   8, steps per second:  69, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.437759, mae: 1.148418, mean_q: 2.081391, mean_eps: 0.100000\n",
            "  213/8000: episode: 21, duration: 0.180s, episode steps:  11, steps per second:  61, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.386044, mae: 1.158813, mean_q: 2.136935, mean_eps: 0.100000\n",
            "  223/8000: episode: 22, duration: 0.134s, episode steps:  10, steps per second:  74, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.414298, mae: 1.193930, mean_q: 2.254776, mean_eps: 0.100000\n",
            "  232/8000: episode: 23, duration: 0.139s, episode steps:   9, steps per second:  65, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.409610, mae: 1.229489, mean_q: 2.349202, mean_eps: 0.100000\n",
            "  241/8000: episode: 24, duration: 0.127s, episode steps:   9, steps per second:  71, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.407962, mae: 1.291767, mean_q: 2.425816, mean_eps: 0.100000\n",
            "  251/8000: episode: 25, duration: 0.136s, episode steps:  10, steps per second:  73, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.455314, mae: 1.346591, mean_q: 2.476849, mean_eps: 0.100000\n",
            "  261/8000: episode: 26, duration: 0.142s, episode steps:  10, steps per second:  70, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.451677, mae: 1.452984, mean_q: 2.594240, mean_eps: 0.100000\n",
            "  271/8000: episode: 27, duration: 0.147s, episode steps:  10, steps per second:  68, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.345663, mae: 1.372206, mean_q: 2.543012, mean_eps: 0.100000\n",
            "  281/8000: episode: 28, duration: 0.154s, episode steps:  10, steps per second:  65, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.491770, mae: 1.539710, mean_q: 2.771531, mean_eps: 0.100000\n",
            "  292/8000: episode: 29, duration: 0.152s, episode steps:  11, steps per second:  73, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.564701, mae: 1.600367, mean_q: 2.756406, mean_eps: 0.100000\n",
            "  304/8000: episode: 30, duration: 0.163s, episode steps:  12, steps per second:  74, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.552083, mae: 1.625941, mean_q: 2.740468, mean_eps: 0.100000\n",
            "  313/8000: episode: 31, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.398890, mae: 1.521478, mean_q: 2.709622, mean_eps: 0.100000\n",
            "  322/8000: episode: 32, duration: 0.133s, episode steps:   9, steps per second:  68, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.407227, mae: 1.581616, mean_q: 2.893726, mean_eps: 0.100000\n",
            "  333/8000: episode: 33, duration: 0.168s, episode steps:  11, steps per second:  66, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.397993, mae: 1.578730, mean_q: 3.023268, mean_eps: 0.100000\n",
            "  342/8000: episode: 34, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.350409, mae: 1.583153, mean_q: 3.099462, mean_eps: 0.100000\n",
            "  352/8000: episode: 35, duration: 0.157s, episode steps:  10, steps per second:  63, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.385099, mae: 1.617456, mean_q: 3.160363, mean_eps: 0.100000\n",
            "  362/8000: episode: 36, duration: 0.136s, episode steps:  10, steps per second:  73, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.333223, mae: 1.585829, mean_q: 3.193546, mean_eps: 0.100000\n",
            "  371/8000: episode: 37, duration: 0.137s, episode steps:   9, steps per second:  66, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.420118, mae: 1.690966, mean_q: 3.306657, mean_eps: 0.100000\n",
            "  381/8000: episode: 38, duration: 0.131s, episode steps:  10, steps per second:  76, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.393578, mae: 1.607292, mean_q: 3.307624, mean_eps: 0.100000\n",
            "  389/8000: episode: 39, duration: 0.118s, episode steps:   8, steps per second:  68, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.393636, mae: 1.607088, mean_q: 3.341648, mean_eps: 0.100000\n",
            "  399/8000: episode: 40, duration: 0.139s, episode steps:  10, steps per second:  72, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.387145, mae: 1.549045, mean_q: 3.350032, mean_eps: 0.100000\n",
            "  410/8000: episode: 41, duration: 0.171s, episode steps:  11, steps per second:  64, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.402652, mae: 1.592071, mean_q: 3.353992, mean_eps: 0.100000\n",
            "  418/8000: episode: 42, duration: 0.112s, episode steps:   8, steps per second:  72, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.357496, mae: 1.558841, mean_q: 3.444270, mean_eps: 0.100000\n",
            "  428/8000: episode: 43, duration: 0.166s, episode steps:  10, steps per second:  60, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.442624, mae: 1.593813, mean_q: 3.477880, mean_eps: 0.100000\n",
            "  438/8000: episode: 44, duration: 0.142s, episode steps:  10, steps per second:  70, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.385996, mae: 1.622358, mean_q: 3.432385, mean_eps: 0.100000\n",
            "  446/8000: episode: 45, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.416863, mae: 1.642128, mean_q: 3.515543, mean_eps: 0.100000\n",
            "  454/8000: episode: 46, duration: 0.120s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.357531, mae: 1.596525, mean_q: 3.570322, mean_eps: 0.100000\n",
            "  464/8000: episode: 47, duration: 0.151s, episode steps:  10, steps per second:  66, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.305461, mae: 1.614492, mean_q: 3.612000, mean_eps: 0.100000\n",
            "  476/8000: episode: 48, duration: 0.194s, episode steps:  12, steps per second:  62, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.344436, mae: 1.646584, mean_q: 3.740548, mean_eps: 0.100000\n",
            "  485/8000: episode: 49, duration: 0.170s, episode steps:   9, steps per second:  53, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.378245, mae: 1.677353, mean_q: 3.752184, mean_eps: 0.100000\n",
            "  493/8000: episode: 50, duration: 0.122s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.397150, mae: 1.688504, mean_q: 3.753566, mean_eps: 0.100000\n",
            "  502/8000: episode: 51, duration: 0.131s, episode steps:   9, steps per second:  69, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.345461, mae: 1.713858, mean_q: 3.701579, mean_eps: 0.100000\n",
            "  513/8000: episode: 52, duration: 0.145s, episode steps:  11, steps per second:  76, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.374381, mae: 1.753508, mean_q: 3.854767, mean_eps: 0.100000\n",
            "  525/8000: episode: 53, duration: 0.159s, episode steps:  12, steps per second:  76, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.337166, mae: 1.809295, mean_q: 3.858201, mean_eps: 0.100000\n",
            "  535/8000: episode: 54, duration: 0.139s, episode steps:  10, steps per second:  72, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.271526, mae: 1.869597, mean_q: 4.021659, mean_eps: 0.100000\n",
            "  545/8000: episode: 55, duration: 0.140s, episode steps:  10, steps per second:  72, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.363888, mae: 1.961453, mean_q: 4.068479, mean_eps: 0.100000\n",
            "  554/8000: episode: 56, duration: 0.124s, episode steps:   9, steps per second:  72, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.276164, mae: 1.942539, mean_q: 4.069201, mean_eps: 0.100000\n",
            "  564/8000: episode: 57, duration: 0.159s, episode steps:  10, steps per second:  63, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.291703, mae: 1.965592, mean_q: 4.140399, mean_eps: 0.100000\n",
            "  577/8000: episode: 58, duration: 0.175s, episode steps:  13, steps per second:  74, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.334601, mae: 1.992021, mean_q: 4.097386, mean_eps: 0.100000\n",
            "  585/8000: episode: 59, duration: 0.115s, episode steps:   8, steps per second:  70, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.277300, mae: 1.940197, mean_q: 4.205190, mean_eps: 0.100000\n",
            "  595/8000: episode: 60, duration: 0.133s, episode steps:  10, steps per second:  75, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.301074, mae: 1.940098, mean_q: 4.275108, mean_eps: 0.100000\n",
            "  606/8000: episode: 61, duration: 0.160s, episode steps:  11, steps per second:  69, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.274522, mae: 1.946242, mean_q: 4.296183, mean_eps: 0.100000\n",
            "  616/8000: episode: 62, duration: 0.151s, episode steps:  10, steps per second:  66, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.251260, mae: 2.004718, mean_q: 4.421323, mean_eps: 0.100000\n",
            "  629/8000: episode: 63, duration: 0.196s, episode steps:  13, steps per second:  66, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.349169, mae: 2.072268, mean_q: 4.357588, mean_eps: 0.100000\n",
            "  640/8000: episode: 64, duration: 0.162s, episode steps:  11, steps per second:  68, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.316981, mae: 2.083123, mean_q: 4.301784, mean_eps: 0.100000\n",
            "  651/8000: episode: 65, duration: 0.153s, episode steps:  11, steps per second:  72, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.291866, mae: 2.126128, mean_q: 4.493605, mean_eps: 0.100000\n",
            "  662/8000: episode: 66, duration: 0.149s, episode steps:  11, steps per second:  74, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.302708, mae: 2.153002, mean_q: 4.460864, mean_eps: 0.100000\n",
            "  671/8000: episode: 67, duration: 0.128s, episode steps:   9, steps per second:  70, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.260064, mae: 2.187047, mean_q: 4.532744, mean_eps: 0.100000\n",
            "  681/8000: episode: 68, duration: 0.137s, episode steps:  10, steps per second:  73, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.288866, mae: 2.245266, mean_q: 4.706915, mean_eps: 0.100000\n",
            "  691/8000: episode: 69, duration: 0.143s, episode steps:  10, steps per second:  70, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.315773, mae: 2.263859, mean_q: 4.631922, mean_eps: 0.100000\n",
            "  701/8000: episode: 70, duration: 0.160s, episode steps:  10, steps per second:  62, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.271771, mae: 2.227008, mean_q: 4.589121, mean_eps: 0.100000\n",
            "  711/8000: episode: 71, duration: 0.173s, episode steps:  10, steps per second:  58, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.312894, mae: 2.267947, mean_q: 4.735012, mean_eps: 0.100000\n",
            "  721/8000: episode: 72, duration: 0.137s, episode steps:  10, steps per second:  73, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.225409, mae: 2.264540, mean_q: 4.800842, mean_eps: 0.100000\n",
            "  731/8000: episode: 73, duration: 0.136s, episode steps:  10, steps per second:  73, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.312092, mae: 2.307671, mean_q: 4.741030, mean_eps: 0.100000\n",
            "  740/8000: episode: 74, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.247503, mae: 2.311588, mean_q: 4.778081, mean_eps: 0.100000\n",
            "  750/8000: episode: 75, duration: 0.133s, episode steps:  10, steps per second:  75, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.301142, mae: 2.319887, mean_q: 4.763575, mean_eps: 0.100000\n",
            "  760/8000: episode: 76, duration: 0.128s, episode steps:  10, steps per second:  78, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.308155, mae: 2.345866, mean_q: 4.771029, mean_eps: 0.100000\n",
            "  771/8000: episode: 77, duration: 0.163s, episode steps:  11, steps per second:  68, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.240361, mae: 2.377429, mean_q: 5.047182, mean_eps: 0.100000\n",
            "  783/8000: episode: 78, duration: 0.153s, episode steps:  12, steps per second:  79, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.272184, mae: 2.373571, mean_q: 4.877629, mean_eps: 0.100000\n",
            "  794/8000: episode: 79, duration: 0.148s, episode steps:  11, steps per second:  74, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.276225, mae: 2.397864, mean_q: 4.923398, mean_eps: 0.100000\n",
            "  805/8000: episode: 80, duration: 0.142s, episode steps:  11, steps per second:  78, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.284948, mae: 2.431249, mean_q: 5.007297, mean_eps: 0.100000\n",
            "  815/8000: episode: 81, duration: 0.135s, episode steps:  10, steps per second:  74, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.247609, mae: 2.426140, mean_q: 5.017354, mean_eps: 0.100000\n",
            "  825/8000: episode: 82, duration: 0.131s, episode steps:  10, steps per second:  76, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.259521, mae: 2.440659, mean_q: 5.008884, mean_eps: 0.100000\n",
            "  836/8000: episode: 83, duration: 0.150s, episode steps:  11, steps per second:  73, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.283238, mae: 2.507716, mean_q: 5.160560, mean_eps: 0.100000\n",
            "  847/8000: episode: 84, duration: 0.166s, episode steps:  11, steps per second:  66, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.250725, mae: 2.512045, mean_q: 5.181180, mean_eps: 0.100000\n",
            "  860/8000: episode: 85, duration: 0.175s, episode steps:  13, steps per second:  74, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.227384, mae: 2.528135, mean_q: 5.269177, mean_eps: 0.100000\n",
            "  871/8000: episode: 86, duration: 0.150s, episode steps:  11, steps per second:  73, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.231167, mae: 2.497563, mean_q: 5.129680, mean_eps: 0.100000\n",
            "  882/8000: episode: 87, duration: 0.169s, episode steps:  11, steps per second:  65, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.209313, mae: 2.624751, mean_q: 5.435924, mean_eps: 0.100000\n",
            "  891/8000: episode: 88, duration: 0.133s, episode steps:   9, steps per second:  68, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.259814, mae: 2.631408, mean_q: 5.325645, mean_eps: 0.100000\n",
            "  902/8000: episode: 89, duration: 0.165s, episode steps:  11, steps per second:  67, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.286252, mae: 2.644801, mean_q: 5.310859, mean_eps: 0.100000\n",
            "  911/8000: episode: 90, duration: 0.122s, episode steps:   9, steps per second:  74, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.254649, mae: 2.664837, mean_q: 5.353433, mean_eps: 0.100000\n",
            "  921/8000: episode: 91, duration: 0.150s, episode steps:  10, steps per second:  66, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.220025, mae: 2.699997, mean_q: 5.532218, mean_eps: 0.100000\n",
            "  932/8000: episode: 92, duration: 0.141s, episode steps:  11, steps per second:  78, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.250742, mae: 2.667672, mean_q: 5.381114, mean_eps: 0.100000\n",
            "  942/8000: episode: 93, duration: 0.154s, episode steps:  10, steps per second:  65, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.233188, mae: 2.701485, mean_q: 5.411719, mean_eps: 0.100000\n",
            "  951/8000: episode: 94, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.201272, mae: 2.765933, mean_q: 5.606130, mean_eps: 0.100000\n",
            "  961/8000: episode: 95, duration: 0.133s, episode steps:  10, steps per second:  75, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.190838, mae: 2.822549, mean_q: 5.704405, mean_eps: 0.100000\n",
            "  973/8000: episode: 96, duration: 0.176s, episode steps:  12, steps per second:  68, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.272757, mae: 2.857362, mean_q: 5.682244, mean_eps: 0.100000\n",
            "  984/8000: episode: 97, duration: 0.161s, episode steps:  11, steps per second:  68, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.231934, mae: 2.789042, mean_q: 5.510743, mean_eps: 0.100000\n",
            "  995/8000: episode: 98, duration: 0.154s, episode steps:  11, steps per second:  71, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.308200, mae: 2.826468, mean_q: 5.586740, mean_eps: 0.100000\n",
            " 1004/8000: episode: 99, duration: 0.130s, episode steps:   9, steps per second:  69, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.313034, mae: 2.767720, mean_q: 5.415756, mean_eps: 0.100000\n",
            " 1014/8000: episode: 100, duration: 0.134s, episode steps:  10, steps per second:  75, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.282266, mae: 2.803567, mean_q: 5.509835, mean_eps: 0.100000\n",
            " 1024/8000: episode: 101, duration: 0.144s, episode steps:  10, steps per second:  69, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.223349, mae: 2.836061, mean_q: 5.694570, mean_eps: 0.100000\n",
            " 1035/8000: episode: 102, duration: 0.145s, episode steps:  11, steps per second:  76, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.322015, mae: 2.897352, mean_q: 5.695434, mean_eps: 0.100000\n",
            " 1047/8000: episode: 103, duration: 0.159s, episode steps:  12, steps per second:  75, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.205571, mae: 2.889126, mean_q: 5.752958, mean_eps: 0.100000\n",
            " 1057/8000: episode: 104, duration: 0.132s, episode steps:  10, steps per second:  76, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.248141, mae: 2.847038, mean_q: 5.586126, mean_eps: 0.100000\n",
            " 1068/8000: episode: 105, duration: 0.158s, episode steps:  11, steps per second:  69, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.253043, mae: 2.874040, mean_q: 5.642490, mean_eps: 0.100000\n",
            " 1078/8000: episode: 106, duration: 0.129s, episode steps:  10, steps per second:  78, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.265051, mae: 2.886113, mean_q: 5.644335, mean_eps: 0.100000\n",
            " 1093/8000: episode: 107, duration: 0.203s, episode steps:  15, steps per second:  74, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.293317, mae: 2.931872, mean_q: 5.725013, mean_eps: 0.100000\n",
            " 1102/8000: episode: 108, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.240739, mae: 2.943156, mean_q: 5.755242, mean_eps: 0.100000\n",
            " 1114/8000: episode: 109, duration: 0.170s, episode steps:  12, steps per second:  71, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.187921, mae: 3.102582, mean_q: 6.154651, mean_eps: 0.100000\n",
            " 1127/8000: episode: 110, duration: 0.176s, episode steps:  13, steps per second:  74, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.278296, mae: 3.037107, mean_q: 5.869368, mean_eps: 0.100000\n",
            " 1137/8000: episode: 111, duration: 0.211s, episode steps:  10, steps per second:  47, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.247390, mae: 3.113891, mean_q: 6.038336, mean_eps: 0.100000\n",
            " 1152/8000: episode: 112, duration: 0.277s, episode steps:  15, steps per second:  54, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.315947, mae: 3.216000, mean_q: 6.212834, mean_eps: 0.100000\n",
            " 1164/8000: episode: 113, duration: 0.232s, episode steps:  12, steps per second:  52, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.274658, mae: 3.071185, mean_q: 5.902562, mean_eps: 0.100000\n",
            " 1182/8000: episode: 114, duration: 0.369s, episode steps:  18, steps per second:  49, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.294611, mae: 3.134320, mean_q: 6.013195, mean_eps: 0.100000\n",
            " 1195/8000: episode: 115, duration: 0.228s, episode steps:  13, steps per second:  57, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.251066, mae: 3.185808, mean_q: 6.128631, mean_eps: 0.100000\n",
            " 1221/8000: episode: 116, duration: 0.329s, episode steps:  26, steps per second:  79, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.287299, mae: 3.272724, mean_q: 6.263797, mean_eps: 0.100000\n",
            " 1249/8000: episode: 117, duration: 0.374s, episode steps:  28, steps per second:  75, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.282872, mae: 3.318786, mean_q: 6.312116, mean_eps: 0.100000\n",
            " 1266/8000: episode: 118, duration: 0.263s, episode steps:  17, steps per second:  65, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.824 [0.000, 1.000],  loss: 0.290995, mae: 3.350236, mean_q: 6.378039, mean_eps: 0.100000\n",
            " 1277/8000: episode: 119, duration: 0.194s, episode steps:  11, steps per second:  57, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.171062, mae: 3.666736, mean_q: 6.939399, mean_eps: 0.100000\n",
            " 1288/8000: episode: 120, duration: 0.204s, episode steps:  11, steps per second:  54, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.422984, mae: 3.498139, mean_q: 6.590832, mean_eps: 0.100000\n",
            " 1296/8000: episode: 121, duration: 0.142s, episode steps:   8, steps per second:  56, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.816635, mae: 3.523259, mean_q: 6.582441, mean_eps: 0.100000\n",
            " 1307/8000: episode: 122, duration: 0.176s, episode steps:  11, steps per second:  63, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 1.162527, mae: 3.655769, mean_q: 6.806167, mean_eps: 0.100000\n",
            " 1317/8000: episode: 123, duration: 0.166s, episode steps:  10, steps per second:  60, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.281002, mae: 3.697721, mean_q: 7.045487, mean_eps: 0.100000\n",
            " 1345/8000: episode: 124, duration: 0.375s, episode steps:  28, steps per second:  75, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.576714, mae: 3.577583, mean_q: 6.779388, mean_eps: 0.100000\n",
            " 1355/8000: episode: 125, duration: 0.139s, episode steps:  10, steps per second:  72, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 1.580290, mae: 3.840331, mean_q: 7.211091, mean_eps: 0.100000\n",
            " 1367/8000: episode: 126, duration: 0.167s, episode steps:  12, steps per second:  72, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.592309, mae: 3.769401, mean_q: 7.196735, mean_eps: 0.100000\n",
            " 1377/8000: episode: 127, duration: 0.132s, episode steps:  10, steps per second:  76, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 2.536037, mae: 3.885067, mean_q: 7.069990, mean_eps: 0.100000\n",
            " 1400/8000: episode: 128, duration: 0.297s, episode steps:  23, steps per second:  77, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.783 [0.000, 1.000],  loss: 0.921986, mae: 3.829484, mean_q: 7.184119, mean_eps: 0.100000\n",
            " 1428/8000: episode: 129, duration: 0.371s, episode steps:  28, steps per second:  75, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 1.300374, mae: 3.906432, mean_q: 7.324992, mean_eps: 0.100000\n",
            " 1579/8000: episode: 130, duration: 1.852s, episode steps: 151, steps per second:  82, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 1.570578, mae: 4.246418, mean_q: 7.948913, mean_eps: 0.100000\n",
            " 1695/8000: episode: 131, duration: 1.466s, episode steps: 116, steps per second:  79, episode reward: 116.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 1.469625, mae: 4.572292, mean_q: 8.566299, mean_eps: 0.100000\n",
            " 1862/8000: episode: 132, duration: 2.057s, episode steps: 167, steps per second:  81, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 1.690113, mae: 4.954982, mean_q: 9.310361, mean_eps: 0.100000\n",
            " 2062/8000: episode: 133, duration: 2.606s, episode steps: 200, steps per second:  77, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 1.896689, mae: 5.472050, mean_q: 10.308035, mean_eps: 0.100000\n",
            " 2262/8000: episode: 134, duration: 2.539s, episode steps: 200, steps per second:  79, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 1.904156, mae: 5.915299, mean_q: 11.186857, mean_eps: 0.100000\n",
            " 2462/8000: episode: 135, duration: 2.850s, episode steps: 200, steps per second:  70, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.839148, mae: 6.402807, mean_q: 12.222156, mean_eps: 0.100000\n",
            " 2662/8000: episode: 136, duration: 2.483s, episode steps: 200, steps per second:  81, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 2.137866, mae: 6.824796, mean_q: 13.035141, mean_eps: 0.100000\n",
            " 2803/8000: episode: 137, duration: 2.112s, episode steps: 141, steps per second:  67, episode reward: 141.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 2.296343, mae: 7.143916, mean_q: 13.686514, mean_eps: 0.100000\n",
            " 2888/8000: episode: 138, duration: 1.605s, episode steps:  85, steps per second:  53, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 2.587289, mae: 7.342952, mean_q: 14.049299, mean_eps: 0.100000\n",
            " 2908/8000: episode: 139, duration: 0.385s, episode steps:  20, steps per second:  52, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 1.208137, mae: 7.362766, mean_q: 14.282538, mean_eps: 0.100000\n",
            " 2927/8000: episode: 140, duration: 0.314s, episode steps:  19, steps per second:  61, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 2.380122, mae: 7.441325, mean_q: 14.443228, mean_eps: 0.100000\n",
            " 2948/8000: episode: 141, duration: 0.275s, episode steps:  21, steps per second:  76, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 1.849675, mae: 7.448801, mean_q: 14.492576, mean_eps: 0.100000\n",
            " 2966/8000: episode: 142, duration: 0.252s, episode steps:  18, steps per second:  71, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.541770, mae: 7.667591, mean_q: 14.830566, mean_eps: 0.100000\n",
            " 2993/8000: episode: 143, duration: 0.363s, episode steps:  27, steps per second:  74, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 1.501141, mae: 7.513680, mean_q: 14.632010, mean_eps: 0.100000\n",
            " 3014/8000: episode: 144, duration: 0.289s, episode steps:  21, steps per second:  73, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 1.413239, mae: 7.754724, mean_q: 15.176250, mean_eps: 0.100000\n",
            " 3035/8000: episode: 145, duration: 0.267s, episode steps:  21, steps per second:  79, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 1.365340, mae: 7.806811, mean_q: 15.328594, mean_eps: 0.100000\n",
            " 3054/8000: episode: 146, duration: 0.257s, episode steps:  19, steps per second:  74, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 4.927462, mae: 8.191105, mean_q: 15.725697, mean_eps: 0.100000\n",
            " 3070/8000: episode: 147, duration: 0.233s, episode steps:  16, steps per second:  69, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.051386, mae: 8.233724, mean_q: 15.730247, mean_eps: 0.100000\n",
            " 3084/8000: episode: 148, duration: 0.194s, episode steps:  14, steps per second:  72, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.751470, mae: 8.274289, mean_q: 16.059173, mean_eps: 0.100000\n",
            " 3099/8000: episode: 149, duration: 0.201s, episode steps:  15, steps per second:  74, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 4.084114, mae: 8.142229, mean_q: 15.656275, mean_eps: 0.100000\n",
            " 3117/8000: episode: 150, duration: 0.267s, episode steps:  18, steps per second:  68, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 3.081063, mae: 8.105398, mean_q: 15.654438, mean_eps: 0.100000\n",
            " 3132/8000: episode: 151, duration: 0.216s, episode steps:  15, steps per second:  70, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1.861621, mae: 8.238985, mean_q: 16.012813, mean_eps: 0.100000\n",
            " 3152/8000: episode: 152, duration: 0.280s, episode steps:  20, steps per second:  71, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.872740, mae: 8.219116, mean_q: 16.122867, mean_eps: 0.100000\n",
            " 3169/8000: episode: 153, duration: 0.231s, episode steps:  17, steps per second:  74, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.347373, mae: 8.437697, mean_q: 16.553082, mean_eps: 0.100000\n",
            " 3184/8000: episode: 154, duration: 0.193s, episode steps:  15, steps per second:  78, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 4.144861, mae: 8.641917, mean_q: 16.774773, mean_eps: 0.100000\n",
            " 3199/8000: episode: 155, duration: 0.202s, episode steps:  15, steps per second:  74, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 3.306804, mae: 8.600143, mean_q: 16.666399, mean_eps: 0.100000\n",
            " 3218/8000: episode: 156, duration: 0.271s, episode steps:  19, steps per second:  70, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 3.194612, mae: 8.691330, mean_q: 16.912748, mean_eps: 0.100000\n",
            " 3232/8000: episode: 157, duration: 0.186s, episode steps:  14, steps per second:  75, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 2.771764, mae: 8.604050, mean_q: 16.810734, mean_eps: 0.100000\n",
            " 3247/8000: episode: 158, duration: 0.200s, episode steps:  15, steps per second:  75, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.587842, mae: 8.559249, mean_q: 16.852637, mean_eps: 0.100000\n",
            " 3263/8000: episode: 159, duration: 0.212s, episode steps:  16, steps per second:  76, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 2.627454, mae: 8.819500, mean_q: 17.321556, mean_eps: 0.100000\n",
            " 3275/8000: episode: 160, duration: 0.171s, episode steps:  12, steps per second:  70, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.814119, mae: 8.968219, mean_q: 17.360742, mean_eps: 0.100000\n",
            " 3292/8000: episode: 161, duration: 0.237s, episode steps:  17, steps per second:  72, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 3.412702, mae: 8.696289, mean_q: 16.851787, mean_eps: 0.100000\n",
            " 3305/8000: episode: 162, duration: 0.191s, episode steps:  13, steps per second:  68, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 4.281113, mae: 8.997239, mean_q: 17.325794, mean_eps: 0.100000\n",
            " 3319/8000: episode: 163, duration: 0.193s, episode steps:  14, steps per second:  73, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 6.686169, mae: 9.018653, mean_q: 17.090834, mean_eps: 0.100000\n",
            " 3334/8000: episode: 164, duration: 0.196s, episode steps:  15, steps per second:  77, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 3.688474, mae: 9.030712, mean_q: 17.327054, mean_eps: 0.100000\n",
            " 3349/8000: episode: 165, duration: 0.203s, episode steps:  15, steps per second:  74, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 3.525254, mae: 8.915061, mean_q: 17.232801, mean_eps: 0.100000\n",
            " 3365/8000: episode: 166, duration: 0.219s, episode steps:  16, steps per second:  73, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 4.001239, mae: 9.045319, mean_q: 17.331143, mean_eps: 0.100000\n",
            " 3378/8000: episode: 167, duration: 0.174s, episode steps:  13, steps per second:  75, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 5.742890, mae: 9.235568, mean_q: 17.602891, mean_eps: 0.100000\n",
            " 3393/8000: episode: 168, duration: 0.206s, episode steps:  15, steps per second:  73, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 3.550132, mae: 9.401145, mean_q: 18.109676, mean_eps: 0.100000\n",
            " 3406/8000: episode: 169, duration: 0.168s, episode steps:  13, steps per second:  78, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 4.311700, mae: 8.882639, mean_q: 17.189483, mean_eps: 0.100000\n",
            " 3417/8000: episode: 170, duration: 0.152s, episode steps:  11, steps per second:  72, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 6.165302, mae: 9.205332, mean_q: 17.729394, mean_eps: 0.100000\n",
            " 3429/8000: episode: 171, duration: 0.189s, episode steps:  12, steps per second:  63, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 7.643696, mae: 9.472037, mean_q: 18.028349, mean_eps: 0.100000\n",
            " 3441/8000: episode: 172, duration: 0.170s, episode steps:  12, steps per second:  71, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 6.396378, mae: 9.342440, mean_q: 17.749924, mean_eps: 0.100000\n",
            " 3457/8000: episode: 173, duration: 0.228s, episode steps:  16, steps per second:  70, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 3.421630, mae: 9.271292, mean_q: 17.815485, mean_eps: 0.100000\n",
            " 3470/8000: episode: 174, duration: 0.166s, episode steps:  13, steps per second:  78, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 2.983031, mae: 9.291728, mean_q: 17.980074, mean_eps: 0.100000\n",
            " 3483/8000: episode: 175, duration: 0.180s, episode steps:  13, steps per second:  72, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 6.692178, mae: 9.578311, mean_q: 18.272800, mean_eps: 0.100000\n",
            " 3493/8000: episode: 176, duration: 0.151s, episode steps:  10, steps per second:  66, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 4.761343, mae: 9.556428, mean_q: 18.303339, mean_eps: 0.100000\n",
            " 3506/8000: episode: 177, duration: 0.191s, episode steps:  13, steps per second:  68, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 4.430902, mae: 9.483787, mean_q: 18.247442, mean_eps: 0.100000\n",
            " 3520/8000: episode: 178, duration: 0.182s, episode steps:  14, steps per second:  77, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 4.729795, mae: 9.493121, mean_q: 18.246823, mean_eps: 0.100000\n",
            " 3540/8000: episode: 179, duration: 0.261s, episode steps:  20, steps per second:  77, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 5.801766, mae: 9.458295, mean_q: 18.131420, mean_eps: 0.100000\n",
            " 3553/8000: episode: 180, duration: 0.173s, episode steps:  13, steps per second:  75, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 5.964095, mae: 9.792897, mean_q: 18.685835, mean_eps: 0.100000\n",
            " 3566/8000: episode: 181, duration: 0.174s, episode steps:  13, steps per second:  75, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 5.232895, mae: 9.772530, mean_q: 18.910504, mean_eps: 0.100000\n",
            " 3579/8000: episode: 182, duration: 0.200s, episode steps:  13, steps per second:  65, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 4.074018, mae: 9.646218, mean_q: 18.624198, mean_eps: 0.100000\n",
            " 3593/8000: episode: 183, duration: 0.191s, episode steps:  14, steps per second:  73, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 6.877537, mae: 9.684025, mean_q: 18.448545, mean_eps: 0.100000\n",
            " 3609/8000: episode: 184, duration: 0.206s, episode steps:  16, steps per second:  78, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 4.723593, mae: 9.569252, mean_q: 18.304016, mean_eps: 0.100000\n",
            " 3622/8000: episode: 185, duration: 0.169s, episode steps:  13, steps per second:  77, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 3.534537, mae: 9.602759, mean_q: 18.647867, mean_eps: 0.100000\n",
            " 3635/8000: episode: 186, duration: 0.174s, episode steps:  13, steps per second:  75, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 6.594554, mae: 9.897326, mean_q: 18.898097, mean_eps: 0.100000\n",
            " 3647/8000: episode: 187, duration: 0.166s, episode steps:  12, steps per second:  72, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 8.532654, mae: 9.954631, mean_q: 18.813445, mean_eps: 0.100000\n",
            " 3659/8000: episode: 188, duration: 0.171s, episode steps:  12, steps per second:  70, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 5.986749, mae: 9.759329, mean_q: 18.536422, mean_eps: 0.100000\n",
            " 3673/8000: episode: 189, duration: 0.210s, episode steps:  14, steps per second:  67, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 6.729547, mae: 9.902171, mean_q: 18.908175, mean_eps: 0.100000\n",
            " 3683/8000: episode: 190, duration: 0.136s, episode steps:  10, steps per second:  74, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 5.239556, mae: 9.897077, mean_q: 19.021106, mean_eps: 0.100000\n",
            " 3696/8000: episode: 191, duration: 0.178s, episode steps:  13, steps per second:  73, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 7.361123, mae: 10.025244, mean_q: 19.207117, mean_eps: 0.100000\n",
            " 3705/8000: episode: 192, duration: 0.139s, episode steps:   9, steps per second:  65, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 6.048976, mae: 10.074556, mean_q: 19.328599, mean_eps: 0.100000\n",
            " 3719/8000: episode: 193, duration: 0.218s, episode steps:  14, steps per second:  64, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 5.497133, mae: 9.887986, mean_q: 18.882192, mean_eps: 0.100000\n",
            " 3732/8000: episode: 194, duration: 0.211s, episode steps:  13, steps per second:  62, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 5.703200, mae: 10.162718, mean_q: 19.449492, mean_eps: 0.100000\n",
            " 3747/8000: episode: 195, duration: 0.192s, episode steps:  15, steps per second:  78, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 4.016882, mae: 9.855346, mean_q: 19.134500, mean_eps: 0.100000\n",
            " 3761/8000: episode: 196, duration: 0.187s, episode steps:  14, steps per second:  75, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 7.812388, mae: 10.076274, mean_q: 19.148546, mean_eps: 0.100000\n",
            " 3773/8000: episode: 197, duration: 0.157s, episode steps:  12, steps per second:  76, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 5.850549, mae: 10.341240, mean_q: 19.770284, mean_eps: 0.100000\n",
            " 3788/8000: episode: 198, duration: 0.209s, episode steps:  15, steps per second:  72, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 8.342803, mae: 10.274403, mean_q: 19.375910, mean_eps: 0.100000\n",
            " 3803/8000: episode: 199, duration: 0.230s, episode steps:  15, steps per second:  65, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 5.295176, mae: 10.094275, mean_q: 19.240893, mean_eps: 0.100000\n",
            " 3817/8000: episode: 200, duration: 0.182s, episode steps:  14, steps per second:  77, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 9.111651, mae: 10.351450, mean_q: 19.503160, mean_eps: 0.100000\n",
            " 3833/8000: episode: 201, duration: 0.211s, episode steps:  16, steps per second:  76, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 4.046807, mae: 9.878315, mean_q: 19.018661, mean_eps: 0.100000\n",
            " 3846/8000: episode: 202, duration: 0.163s, episode steps:  13, steps per second:  80, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 6.850852, mae: 10.232913, mean_q: 19.468691, mean_eps: 0.100000\n",
            " 3857/8000: episode: 203, duration: 0.152s, episode steps:  11, steps per second:  72, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 6.918047, mae: 10.092793, mean_q: 19.267884, mean_eps: 0.100000\n",
            " 3871/8000: episode: 204, duration: 0.205s, episode steps:  14, steps per second:  68, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 3.739332, mae: 10.220547, mean_q: 19.824790, mean_eps: 0.100000\n",
            " 3886/8000: episode: 205, duration: 0.202s, episode steps:  15, steps per second:  74, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 6.851388, mae: 10.391253, mean_q: 19.855861, mean_eps: 0.100000\n",
            " 3901/8000: episode: 206, duration: 0.204s, episode steps:  15, steps per second:  74, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 8.358428, mae: 10.487010, mean_q: 19.745574, mean_eps: 0.100000\n",
            " 3916/8000: episode: 207, duration: 0.188s, episode steps:  15, steps per second:  80, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 4.970917, mae: 10.109744, mean_q: 19.285003, mean_eps: 0.100000\n",
            " 3928/8000: episode: 208, duration: 0.169s, episode steps:  12, steps per second:  71, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 7.730288, mae: 10.450613, mean_q: 19.780962, mean_eps: 0.100000\n",
            " 3944/8000: episode: 209, duration: 0.233s, episode steps:  16, steps per second:  69, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 4.610486, mae: 10.446919, mean_q: 20.059293, mean_eps: 0.100000\n",
            " 3960/8000: episode: 210, duration: 0.219s, episode steps:  16, steps per second:  73, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 4.821256, mae: 10.242279, mean_q: 19.804286, mean_eps: 0.100000\n",
            " 3974/8000: episode: 211, duration: 0.178s, episode steps:  14, steps per second:  78, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 5.616814, mae: 10.403363, mean_q: 20.031170, mean_eps: 0.100000\n",
            " 3990/8000: episode: 212, duration: 0.220s, episode steps:  16, steps per second:  73, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 8.023375, mae: 10.554398, mean_q: 20.112213, mean_eps: 0.100000\n",
            " 4006/8000: episode: 213, duration: 0.217s, episode steps:  16, steps per second:  74, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 5.704019, mae: 10.495587, mean_q: 20.187771, mean_eps: 0.100000\n",
            " 4020/8000: episode: 214, duration: 0.204s, episode steps:  14, steps per second:  69, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 10.686484, mae: 10.727588, mean_q: 20.186626, mean_eps: 0.100000\n",
            " 4033/8000: episode: 215, duration: 0.183s, episode steps:  13, steps per second:  71, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 7.603907, mae: 10.627170, mean_q: 20.067376, mean_eps: 0.100000\n",
            " 4047/8000: episode: 216, duration: 0.197s, episode steps:  14, steps per second:  71, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 8.499012, mae: 10.613207, mean_q: 19.828972, mean_eps: 0.100000\n",
            " 4060/8000: episode: 217, duration: 0.169s, episode steps:  13, steps per second:  77, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 7.167500, mae: 10.650571, mean_q: 20.086668, mean_eps: 0.100000\n",
            " 4074/8000: episode: 218, duration: 0.190s, episode steps:  14, steps per second:  74, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 6.122560, mae: 10.612580, mean_q: 20.299311, mean_eps: 0.100000\n",
            " 4088/8000: episode: 219, duration: 0.193s, episode steps:  14, steps per second:  72, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 4.907719, mae: 10.463735, mean_q: 20.166596, mean_eps: 0.100000\n",
            " 4102/8000: episode: 220, duration: 0.203s, episode steps:  14, steps per second:  69, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 8.936323, mae: 10.664254, mean_q: 20.095791, mean_eps: 0.100000\n",
            " 4115/8000: episode: 221, duration: 0.172s, episode steps:  13, steps per second:  75, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 6.021129, mae: 10.597955, mean_q: 20.211169, mean_eps: 0.100000\n",
            " 4132/8000: episode: 222, duration: 0.227s, episode steps:  17, steps per second:  75, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 6.180097, mae: 10.634924, mean_q: 20.297938, mean_eps: 0.100000\n",
            " 4152/8000: episode: 223, duration: 0.280s, episode steps:  20, steps per second:  71, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.744666, mae: 10.704355, mean_q: 20.295207, mean_eps: 0.100000\n",
            " 4171/8000: episode: 224, duration: 0.271s, episode steps:  19, steps per second:  70, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 5.987327, mae: 10.728332, mean_q: 20.421092, mean_eps: 0.100000\n",
            " 4189/8000: episode: 225, duration: 0.250s, episode steps:  18, steps per second:  72, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.986053, mae: 10.777103, mean_q: 20.327004, mean_eps: 0.100000\n",
            " 4207/8000: episode: 226, duration: 0.252s, episode steps:  18, steps per second:  71, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.514266, mae: 10.702365, mean_q: 20.065511, mean_eps: 0.100000\n",
            " 4221/8000: episode: 227, duration: 0.190s, episode steps:  14, steps per second:  74, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.057547, mae: 10.513372, mean_q: 20.289203, mean_eps: 0.100000\n",
            " 4238/8000: episode: 228, duration: 0.232s, episode steps:  17, steps per second:  73, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 7.593778, mae: 10.740825, mean_q: 20.442036, mean_eps: 0.100000\n",
            " 4254/8000: episode: 229, duration: 0.205s, episode steps:  16, steps per second:  78, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.039454, mae: 10.703211, mean_q: 20.489112, mean_eps: 0.100000\n",
            " 4265/8000: episode: 230, duration: 0.143s, episode steps:  11, steps per second:  77, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 5.768810, mae: 10.862372, mean_q: 20.726997, mean_eps: 0.100000\n",
            " 4282/8000: episode: 231, duration: 0.230s, episode steps:  17, steps per second:  74, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 7.226513, mae: 10.835455, mean_q: 20.681749, mean_eps: 0.100000\n",
            " 4299/8000: episode: 232, duration: 0.232s, episode steps:  17, steps per second:  73, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 7.032698, mae: 10.883352, mean_q: 20.796356, mean_eps: 0.100000\n",
            " 4316/8000: episode: 233, duration: 0.242s, episode steps:  17, steps per second:  70, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 9.351780, mae: 10.960802, mean_q: 20.576662, mean_eps: 0.100000\n",
            " 4337/8000: episode: 234, duration: 0.261s, episode steps:  21, steps per second:  81, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 8.006452, mae: 10.893310, mean_q: 20.445405, mean_eps: 0.100000\n",
            " 4352/8000: episode: 235, duration: 0.197s, episode steps:  15, steps per second:  76, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 5.757070, mae: 10.810499, mean_q: 20.596154, mean_eps: 0.100000\n",
            " 4372/8000: episode: 236, duration: 0.263s, episode steps:  20, steps per second:  76, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 6.149954, mae: 10.798204, mean_q: 20.613091, mean_eps: 0.100000\n",
            " 4389/8000: episode: 237, duration: 0.258s, episode steps:  17, steps per second:  66, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 7.310973, mae: 11.018317, mean_q: 20.986822, mean_eps: 0.100000\n",
            " 4407/8000: episode: 238, duration: 0.237s, episode steps:  18, steps per second:  76, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 6.354506, mae: 10.937993, mean_q: 20.880241, mean_eps: 0.100000\n",
            " 4425/8000: episode: 239, duration: 0.239s, episode steps:  18, steps per second:  75, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 7.078593, mae: 11.089726, mean_q: 21.086815, mean_eps: 0.100000\n",
            " 4436/8000: episode: 240, duration: 0.142s, episode steps:  11, steps per second:  78, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 10.663143, mae: 11.075683, mean_q: 20.636638, mean_eps: 0.100000\n",
            " 4454/8000: episode: 241, duration: 0.236s, episode steps:  18, steps per second:  76, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 8.249321, mae: 11.066900, mean_q: 20.769476, mean_eps: 0.100000\n",
            " 4470/8000: episode: 242, duration: 0.215s, episode steps:  16, steps per second:  74, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.076116, mae: 10.988393, mean_q: 20.539812, mean_eps: 0.100000\n",
            " 4486/8000: episode: 243, duration: 0.229s, episode steps:  16, steps per second:  70, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.535141, mae: 11.092676, mean_q: 20.833757, mean_eps: 0.100000\n",
            " 4518/8000: episode: 244, duration: 0.416s, episode steps:  32, steps per second:  77, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.262646, mae: 10.874547, mean_q: 20.582295, mean_eps: 0.100000\n",
            " 4543/8000: episode: 245, duration: 0.347s, episode steps:  25, steps per second:  72, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 6.412884, mae: 10.927442, mean_q: 20.743096, mean_eps: 0.100000\n",
            " 4569/8000: episode: 246, duration: 0.326s, episode steps:  26, steps per second:  80, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 6.141116, mae: 10.985086, mean_q: 20.912040, mean_eps: 0.100000\n",
            " 4605/8000: episode: 247, duration: 0.462s, episode steps:  36, steps per second:  78, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 4.705704, mae: 10.940845, mean_q: 21.075678, mean_eps: 0.100000\n",
            " 4628/8000: episode: 248, duration: 0.317s, episode steps:  23, steps per second:  72, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 9.446797, mae: 11.101630, mean_q: 20.916200, mean_eps: 0.100000\n",
            " 4651/8000: episode: 249, duration: 0.320s, episode steps:  23, steps per second:  72, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 6.401104, mae: 11.178912, mean_q: 21.197457, mean_eps: 0.100000\n",
            " 4673/8000: episode: 250, duration: 0.338s, episode steps:  22, steps per second:  65, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.820693, mae: 11.114108, mean_q: 21.094138, mean_eps: 0.100000\n",
            " 4690/8000: episode: 251, duration: 0.267s, episode steps:  17, steps per second:  64, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 10.297621, mae: 11.393069, mean_q: 21.406018, mean_eps: 0.100000\n",
            " 4714/8000: episode: 252, duration: 0.320s, episode steps:  24, steps per second:  75, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.095166, mae: 11.147249, mean_q: 21.214930, mean_eps: 0.100000\n",
            " 4739/8000: episode: 253, duration: 0.380s, episode steps:  25, steps per second:  66, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 8.539184, mae: 10.997748, mean_q: 20.609732, mean_eps: 0.100000\n",
            " 4758/8000: episode: 254, duration: 0.251s, episode steps:  19, steps per second:  76, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 8.727661, mae: 11.235261, mean_q: 21.044368, mean_eps: 0.100000\n",
            " 4775/8000: episode: 255, duration: 0.231s, episode steps:  17, steps per second:  74, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 8.670832, mae: 11.184606, mean_q: 20.979496, mean_eps: 0.100000\n",
            " 4795/8000: episode: 256, duration: 0.262s, episode steps:  20, steps per second:  76, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.544667, mae: 11.091634, mean_q: 20.933449, mean_eps: 0.100000\n",
            " 4816/8000: episode: 257, duration: 0.299s, episode steps:  21, steps per second:  70, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 5.279497, mae: 11.163303, mean_q: 21.323992, mean_eps: 0.100000\n",
            " 4844/8000: episode: 258, duration: 0.403s, episode steps:  28, steps per second:  69, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 7.091917, mae: 11.223916, mean_q: 21.308574, mean_eps: 0.100000\n",
            " 4881/8000: episode: 259, duration: 0.678s, episode steps:  37, steps per second:  55, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 7.279887, mae: 11.141052, mean_q: 21.064171, mean_eps: 0.100000\n",
            " 4908/8000: episode: 260, duration: 0.539s, episode steps:  27, steps per second:  50, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 6.398912, mae: 11.402642, mean_q: 21.766755, mean_eps: 0.100000\n",
            " 4929/8000: episode: 261, duration: 0.398s, episode steps:  21, steps per second:  53, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 6.808582, mae: 11.362417, mean_q: 21.518966, mean_eps: 0.100000\n",
            " 4963/8000: episode: 262, duration: 0.677s, episode steps:  34, steps per second:  50, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 6.884898, mae: 11.278000, mean_q: 21.362597, mean_eps: 0.100000\n",
            " 4991/8000: episode: 263, duration: 0.518s, episode steps:  28, steps per second:  54, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 6.543547, mae: 11.218144, mean_q: 21.285221, mean_eps: 0.100000\n",
            " 5024/8000: episode: 264, duration: 0.608s, episode steps:  33, steps per second:  54, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 5.886602, mae: 11.313905, mean_q: 21.590207, mean_eps: 0.100000\n",
            " 5057/8000: episode: 265, duration: 0.624s, episode steps:  33, steps per second:  53, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 7.284377, mae: 11.247783, mean_q: 21.315431, mean_eps: 0.100000\n",
            " 5083/8000: episode: 266, duration: 0.337s, episode steps:  26, steps per second:  77, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 7.319889, mae: 11.380349, mean_q: 21.482846, mean_eps: 0.100000\n",
            " 5117/8000: episode: 267, duration: 0.447s, episode steps:  34, steps per second:  76, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 8.003449, mae: 11.367589, mean_q: 21.397966, mean_eps: 0.100000\n",
            " 5147/8000: episode: 268, duration: 0.404s, episode steps:  30, steps per second:  74, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 7.348751, mae: 11.328287, mean_q: 21.348714, mean_eps: 0.100000\n",
            " 5211/8000: episode: 269, duration: 0.849s, episode steps:  64, steps per second:  75, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 6.504963, mae: 11.280348, mean_q: 21.394084, mean_eps: 0.100000\n",
            " 5241/8000: episode: 270, duration: 0.539s, episode steps:  30, steps per second:  56, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 6.286648, mae: 11.376358, mean_q: 21.687174, mean_eps: 0.100000\n",
            " 5270/8000: episode: 271, duration: 0.505s, episode steps:  29, steps per second:  57, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.586 [0.000, 1.000],  loss: 5.889884, mae: 11.384286, mean_q: 21.730025, mean_eps: 0.100000\n",
            " 5293/8000: episode: 272, duration: 0.315s, episode steps:  23, steps per second:  73, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 7.772936, mae: 11.541473, mean_q: 21.791902, mean_eps: 0.100000\n",
            " 5319/8000: episode: 273, duration: 0.375s, episode steps:  26, steps per second:  69, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.649489, mae: 11.396081, mean_q: 21.543130, mean_eps: 0.100000\n",
            " 5364/8000: episode: 274, duration: 0.586s, episode steps:  45, steps per second:  77, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 5.459210, mae: 11.355039, mean_q: 21.719872, mean_eps: 0.100000\n",
            " 5424/8000: episode: 275, duration: 0.765s, episode steps:  60, steps per second:  78, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 6.272175, mae: 11.446055, mean_q: 21.831365, mean_eps: 0.100000\n",
            " 5464/8000: episode: 276, duration: 0.491s, episode steps:  40, steps per second:  82, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.514178, mae: 11.549447, mean_q: 21.648802, mean_eps: 0.100000\n",
            " 5501/8000: episode: 277, duration: 0.462s, episode steps:  37, steps per second:  80, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 6.355022, mae: 11.432552, mean_q: 21.625033, mean_eps: 0.100000\n",
            " 5536/8000: episode: 278, duration: 0.442s, episode steps:  35, steps per second:  79, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 5.464768, mae: 11.511366, mean_q: 21.984840, mean_eps: 0.100000\n",
            " 5560/8000: episode: 279, duration: 0.299s, episode steps:  24, steps per second:  80, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 6.660387, mae: 11.703309, mean_q: 22.271034, mean_eps: 0.100000\n",
            " 5595/8000: episode: 280, duration: 0.453s, episode steps:  35, steps per second:  77, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 7.177182, mae: 11.637996, mean_q: 22.028338, mean_eps: 0.100000\n",
            " 5631/8000: episode: 281, duration: 0.455s, episode steps:  36, steps per second:  79, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 6.495853, mae: 11.517759, mean_q: 21.907195, mean_eps: 0.100000\n",
            " 5655/8000: episode: 282, duration: 0.294s, episode steps:  24, steps per second:  82, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 6.409838, mae: 11.601855, mean_q: 22.061402, mean_eps: 0.100000\n",
            " 5678/8000: episode: 283, duration: 0.301s, episode steps:  23, steps per second:  76, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 6.332050, mae: 11.465752, mean_q: 21.801031, mean_eps: 0.100000\n",
            " 5712/8000: episode: 284, duration: 0.425s, episode steps:  34, steps per second:  80, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 7.045383, mae: 11.604551, mean_q: 21.935038, mean_eps: 0.100000\n",
            " 5774/8000: episode: 285, duration: 0.787s, episode steps:  62, steps per second:  79, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 7.264159, mae: 11.675124, mean_q: 22.123957, mean_eps: 0.100000\n",
            " 5822/8000: episode: 286, duration: 0.592s, episode steps:  48, steps per second:  81, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 5.657553, mae: 11.670009, mean_q: 22.315321, mean_eps: 0.100000\n",
            " 5883/8000: episode: 287, duration: 0.765s, episode steps:  61, steps per second:  80, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 6.426268, mae: 11.729233, mean_q: 22.402053, mean_eps: 0.100000\n",
            " 5928/8000: episode: 288, duration: 0.557s, episode steps:  45, steps per second:  81, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 5.733045, mae: 11.671901, mean_q: 22.339029, mean_eps: 0.100000\n",
            " 5960/8000: episode: 289, duration: 0.419s, episode steps:  32, steps per second:  76, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 5.187909, mae: 11.721438, mean_q: 22.549937, mean_eps: 0.100000\n",
            " 5998/8000: episode: 290, duration: 0.492s, episode steps:  38, steps per second:  77, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 6.114140, mae: 11.806219, mean_q: 22.608320, mean_eps: 0.100000\n",
            " 6034/8000: episode: 291, duration: 0.454s, episode steps:  36, steps per second:  79, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 5.762763, mae: 12.017084, mean_q: 22.980868, mean_eps: 0.100000\n",
            " 6078/8000: episode: 292, duration: 0.634s, episode steps:  44, steps per second:  69, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 5.811737, mae: 11.899591, mean_q: 22.807078, mean_eps: 0.100000\n",
            " 6112/8000: episode: 293, duration: 0.659s, episode steps:  34, steps per second:  52, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 6.704749, mae: 12.022952, mean_q: 22.902774, mean_eps: 0.100000\n",
            " 6156/8000: episode: 294, duration: 0.630s, episode steps:  44, steps per second:  70, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 7.274753, mae: 12.025547, mean_q: 22.793103, mean_eps: 0.100000\n",
            " 6237/8000: episode: 295, duration: 1.024s, episode steps:  81, steps per second:  79, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 6.034057, mae: 11.954558, mean_q: 22.881512, mean_eps: 0.100000\n",
            " 6269/8000: episode: 296, duration: 0.405s, episode steps:  32, steps per second:  79, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 6.618931, mae: 12.132044, mean_q: 23.198475, mean_eps: 0.100000\n",
            " 6318/8000: episode: 297, duration: 0.614s, episode steps:  49, steps per second:  80, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 6.263201, mae: 12.195799, mean_q: 23.294274, mean_eps: 0.100000\n",
            " 6349/8000: episode: 298, duration: 0.431s, episode steps:  31, steps per second:  72, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 7.277970, mae: 12.151159, mean_q: 23.142393, mean_eps: 0.100000\n",
            " 6407/8000: episode: 299, duration: 0.748s, episode steps:  58, steps per second:  77, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 6.682281, mae: 12.206205, mean_q: 23.319785, mean_eps: 0.100000\n",
            " 6478/8000: episode: 300, duration: 0.896s, episode steps:  71, steps per second:  79, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 7.292849, mae: 12.302656, mean_q: 23.527518, mean_eps: 0.100000\n",
            " 6534/8000: episode: 301, duration: 0.719s, episode steps:  56, steps per second:  78, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 6.898221, mae: 12.338617, mean_q: 23.492587, mean_eps: 0.100000\n",
            " 6564/8000: episode: 302, duration: 0.407s, episode steps:  30, steps per second:  74, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.189229, mae: 12.345099, mean_q: 23.647150, mean_eps: 0.100000\n",
            " 6610/8000: episode: 303, duration: 0.591s, episode steps:  46, steps per second:  78, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 7.357332, mae: 12.428025, mean_q: 23.596476, mean_eps: 0.100000\n",
            " 6674/8000: episode: 304, duration: 0.813s, episode steps:  64, steps per second:  79, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 7.770424, mae: 12.451332, mean_q: 23.560671, mean_eps: 0.100000\n",
            " 6750/8000: episode: 305, duration: 0.990s, episode steps:  76, steps per second:  77, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 5.879958, mae: 12.388031, mean_q: 23.683134, mean_eps: 0.100000\n",
            " 6814/8000: episode: 306, duration: 0.766s, episode steps:  64, steps per second:  84, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 6.350589, mae: 12.314903, mean_q: 23.465307, mean_eps: 0.100000\n",
            " 6923/8000: episode: 307, duration: 1.384s, episode steps: 109, steps per second:  79, episode reward: 109.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 5.859456, mae: 12.491872, mean_q: 23.907211, mean_eps: 0.100000\n",
            " 6972/8000: episode: 308, duration: 0.603s, episode steps:  49, steps per second:  81, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 5.811804, mae: 12.522933, mean_q: 24.024166, mean_eps: 0.100000\n",
            " 7002/8000: episode: 309, duration: 0.390s, episode steps:  30, steps per second:  77, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 6.367742, mae: 12.623942, mean_q: 24.125411, mean_eps: 0.100000\n",
            " 7101/8000: episode: 310, duration: 1.208s, episode steps:  99, steps per second:  82, episode reward: 99.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 6.087047, mae: 12.588316, mean_q: 24.093644, mean_eps: 0.100000\n",
            " 7174/8000: episode: 311, duration: 0.962s, episode steps:  73, steps per second:  76, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 6.739225, mae: 12.706981, mean_q: 24.273297, mean_eps: 0.100000\n",
            " 7250/8000: episode: 312, duration: 0.978s, episode steps:  76, steps per second:  78, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 5.621918, mae: 12.688001, mean_q: 24.389417, mean_eps: 0.100000\n",
            " 7330/8000: episode: 313, duration: 1.017s, episode steps:  80, steps per second:  79, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 6.638682, mae: 12.766715, mean_q: 24.462272, mean_eps: 0.100000\n",
            " 7391/8000: episode: 314, duration: 0.745s, episode steps:  61, steps per second:  82, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 6.424036, mae: 12.737849, mean_q: 24.360936, mean_eps: 0.100000\n",
            " 7429/8000: episode: 315, duration: 0.481s, episode steps:  38, steps per second:  79, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 6.402570, mae: 12.835461, mean_q: 24.582392, mean_eps: 0.100000\n",
            " 7514/8000: episode: 316, duration: 1.081s, episode steps:  85, steps per second:  79, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 6.378758, mae: 12.840262, mean_q: 24.551497, mean_eps: 0.100000\n",
            " 7581/8000: episode: 317, duration: 0.865s, episode steps:  67, steps per second:  77, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 5.981282, mae: 12.777719, mean_q: 24.479333, mean_eps: 0.100000\n",
            " 7675/8000: episode: 318, duration: 1.173s, episode steps:  94, steps per second:  80, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 5.991572, mae: 12.934008, mean_q: 24.828361, mean_eps: 0.100000\n",
            " 7764/8000: episode: 319, duration: 1.101s, episode steps:  89, steps per second:  81, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 6.617669, mae: 12.982027, mean_q: 24.845292, mean_eps: 0.100000\n",
            " 7801/8000: episode: 320, duration: 0.465s, episode steps:  37, steps per second:  80, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 6.675995, mae: 12.995272, mean_q: 24.855075, mean_eps: 0.100000\n",
            " 7839/8000: episode: 321, duration: 0.476s, episode steps:  38, steps per second:  80, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 6.016918, mae: 12.906145, mean_q: 24.733404, mean_eps: 0.100000\n",
            " 7892/8000: episode: 322, duration: 0.670s, episode steps:  53, steps per second:  79, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 6.697174, mae: 13.139860, mean_q: 25.135488, mean_eps: 0.100000\n",
            " 7937/8000: episode: 323, duration: 0.559s, episode steps:  45, steps per second:  81, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 6.260940, mae: 13.004922, mean_q: 24.837122, mean_eps: 0.100000\n",
            " 7990/8000: episode: 324, duration: 0.674s, episode steps:  53, steps per second:  79, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 5.605282, mae: 13.044585, mean_q: 25.041763, mean_eps: 0.100000\n",
            "done, took 118.245 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 44.000, steps: 44\n",
            "Episode 2: reward: 33.000, steps: 33\n",
            "Episode 3: reward: 100.000, steps: 100\n",
            "Episode 4: reward: 50.000, steps: 50\n",
            "Episode 5: reward: 46.000, steps: 46\n",
            "Episode 6: reward: 41.000, steps: 41\n",
            "Episode 7: reward: 59.000, steps: 59\n",
            "Episode 8: reward: 63.000, steps: 63\n",
            "Episode 9: reward: 33.000, steps: 33\n",
            "Episode 10: reward: 33.000, steps: 33\n",
            "Episode 11: reward: 59.000, steps: 59\n",
            "Episode 12: reward: 46.000, steps: 46\n",
            "Episode 13: reward: 38.000, steps: 38\n",
            "Episode 14: reward: 49.000, steps: 49\n",
            "Episode 15: reward: 57.000, steps: 57\n",
            "Episode 16: reward: 62.000, steps: 62\n",
            "Episode 17: reward: 41.000, steps: 41\n",
            "Episode 18: reward: 37.000, steps: 37\n",
            "Episode 19: reward: 47.000, steps: 47\n",
            "Episode 20: reward: 75.000, steps: 75\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb171b61310>"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    }
  ]
}