{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPoleDQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Megacity1/CartpoleDQN/blob/main/Week%203%20Deep%20RL%202/CartPoleDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Deep Q Network (DQN) for CartPole Using Boltzmann Q Policy\n",
        "This exercise implements a DQN for CartPole using a Boltzmann Q policy for selecting the actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGsC7cJ5jNcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda73cbf-3020-40f4-e45e-7b87053448f1"
      },
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.24.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (13.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.44.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.10.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMIHLgQ3Z-lF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5a8608-456f-4127-ce8a-f551e5f4e4f3"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0AMLzq08ap0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b380e1-6c3d-4bce-8233-a8ac01deaf26"
      },
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "!pip install Adam\n",
        "#from keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Adam in /usr/local/lib/python3.7/dist-packages (0.0.0.dev0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll6bNdUm54WS"
      },
      "source": [
        "Implementation of DQN for CartPole, applying policy BoltzmannQPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCrPKNy40PC"
      },
      "source": [
        "##Implement DQN with BoltzmannGumbelQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efM9jkXr5A3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d0af0deb-b006-46b9-c8cd-12017f347169"
      },
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import BoltzmannGumbelQPolicy\n",
        "from rl.policy import LinearAnnealedPolicy\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n",
        "# setup experience replay buffer\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
        "                               attr='eps',            \n",
        "                               value_max=5.,\n",
        "                               value_min=.5, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=20)\n",
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "# add extra layers here\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=20,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy) \n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=8000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_43\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_41 (Flatten)        (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_83 (Dense)            (None, 16)                80        \n",
            "                                                                 \n",
            " dense_84 (Dense)            (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 8000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   26/8000: episode: 1, duration: 11.935s, episode steps:  26, steps per second:   2, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.577 [0.000, 1.000],  loss: 0.488553, mae: 0.566223, mean_q: -0.128485, mean_eps: 0.500000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   46/8000: episode: 2, duration: 0.476s, episode steps:  20, steps per second:  42, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.447511, mae: 0.522043, mean_q: -0.059933, mean_eps: 0.500000\n",
            "   56/8000: episode: 3, duration: 0.257s, episode steps:  10, steps per second:  39, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.380601, mae: 0.478004, mean_q: -0.004628, mean_eps: 0.500000\n",
            "   78/8000: episode: 4, duration: 0.485s, episode steps:  22, steps per second:  45, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.682 [0.000, 1.000],  loss: 0.337320, mae: 0.449646, mean_q: 0.111948, mean_eps: 0.500000\n",
            "   87/8000: episode: 5, duration: 0.229s, episode steps:   9, steps per second:  39, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.290972, mae: 0.419757, mean_q: 0.221467, mean_eps: 0.500000\n",
            "  100/8000: episode: 6, duration: 0.327s, episode steps:  13, steps per second:  40, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.266700, mae: 0.403566, mean_q: 0.306622, mean_eps: 0.500000\n",
            "  109/8000: episode: 7, duration: 0.205s, episode steps:   9, steps per second:  44, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.252982, mae: 0.390435, mean_q: 0.396499, mean_eps: 0.500000\n",
            "  124/8000: episode: 8, duration: 0.315s, episode steps:  15, steps per second:  48, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.255633, mae: 0.394986, mean_q: 0.475462, mean_eps: 0.500000\n",
            "  134/8000: episode: 9, duration: 0.182s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.247691, mae: 0.411839, mean_q: 0.564829, mean_eps: 0.500000\n",
            "  152/8000: episode: 10, duration: 0.309s, episode steps:  18, steps per second:  58, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.265743, mae: 0.470626, mean_q: 0.622538, mean_eps: 0.500000\n",
            "  168/8000: episode: 11, duration: 0.266s, episode steps:  16, steps per second:  60, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.243474, mae: 0.510452, mean_q: 0.746596, mean_eps: 0.500000\n",
            "  185/8000: episode: 12, duration: 0.277s, episode steps:  17, steps per second:  61, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.252849, mae: 0.577056, mean_q: 0.825915, mean_eps: 0.500000\n",
            "  199/8000: episode: 13, duration: 0.334s, episode steps:  14, steps per second:  42, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.228568, mae: 0.609352, mean_q: 0.938848, mean_eps: 0.500000\n",
            "  209/8000: episode: 14, duration: 0.241s, episode steps:  10, steps per second:  42, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.215724, mae: 0.657517, mean_q: 1.037191, mean_eps: 0.500000\n",
            "  220/8000: episode: 15, duration: 0.250s, episode steps:  11, steps per second:  44, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.198687, mae: 0.683051, mean_q: 1.116190, mean_eps: 0.500000\n",
            "  235/8000: episode: 16, duration: 0.319s, episode steps:  15, steps per second:  47, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.867 [0.000, 1.000],  loss: 0.188895, mae: 0.737682, mean_q: 1.263609, mean_eps: 0.500000\n",
            "  244/8000: episode: 17, duration: 0.237s, episode steps:   9, steps per second:  38, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.185054, mae: 0.773292, mean_q: 1.350129, mean_eps: 0.500000\n",
            "  254/8000: episode: 18, duration: 0.231s, episode steps:  10, steps per second:  43, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.178970, mae: 0.802708, mean_q: 1.456892, mean_eps: 0.500000\n",
            "  268/8000: episode: 19, duration: 0.293s, episode steps:  14, steps per second:  48, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.206696, mae: 0.870216, mean_q: 1.562374, mean_eps: 0.500000\n",
            "  298/8000: episode: 20, duration: 0.627s, episode steps:  30, steps per second:  48, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.633 [0.000, 1.000],  loss: 0.185816, mae: 0.928611, mean_q: 1.671285, mean_eps: 0.500000\n",
            "  309/8000: episode: 21, duration: 0.194s, episode steps:  11, steps per second:  57, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.177015, mae: 0.979430, mean_q: 1.803773, mean_eps: 0.500000\n",
            "  323/8000: episode: 22, duration: 0.251s, episode steps:  14, steps per second:  56, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.213612, mae: 1.039967, mean_q: 1.939606, mean_eps: 0.500000\n",
            "  336/8000: episode: 23, duration: 0.261s, episode steps:  13, steps per second:  50, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.190069, mae: 1.058049, mean_q: 1.951233, mean_eps: 0.500000\n",
            "  363/8000: episode: 24, duration: 0.595s, episode steps:  27, steps per second:  45, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.704 [0.000, 1.000],  loss: 0.224268, mae: 1.147515, mean_q: 2.133793, mean_eps: 0.500000\n",
            "  403/8000: episode: 25, duration: 0.791s, episode steps:  40, steps per second:  51, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.246698, mae: 1.262531, mean_q: 2.349097, mean_eps: 0.500000\n",
            "  430/8000: episode: 26, duration: 0.551s, episode steps:  27, steps per second:  49, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.630 [0.000, 1.000],  loss: 0.247130, mae: 1.370208, mean_q: 2.600967, mean_eps: 0.500000\n",
            "  441/8000: episode: 27, duration: 0.252s, episode steps:  11, steps per second:  44, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.307651, mae: 1.448199, mean_q: 2.739849, mean_eps: 0.500000\n",
            "  450/8000: episode: 28, duration: 0.193s, episode steps:   9, steps per second:  47, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.301617, mae: 1.527508, mean_q: 2.850305, mean_eps: 0.500000\n",
            "  464/8000: episode: 29, duration: 0.268s, episode steps:  14, steps per second:  52, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.286917, mae: 1.522795, mean_q: 2.929062, mean_eps: 0.500000\n",
            "  473/8000: episode: 30, duration: 0.179s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.345308, mae: 1.589046, mean_q: 3.026509, mean_eps: 0.500000\n",
            "  483/8000: episode: 31, duration: 0.203s, episode steps:  10, steps per second:  49, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.413195, mae: 1.681697, mean_q: 3.071041, mean_eps: 0.500000\n",
            "  501/8000: episode: 32, duration: 0.391s, episode steps:  18, steps per second:  46, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.315892, mae: 1.654567, mean_q: 3.087032, mean_eps: 0.500000\n",
            "  510/8000: episode: 33, duration: 0.223s, episode steps:   9, steps per second:  40, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.371886, mae: 1.722141, mean_q: 3.240416, mean_eps: 0.500000\n",
            "  522/8000: episode: 34, duration: 0.300s, episode steps:  12, steps per second:  40, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.311925, mae: 1.763979, mean_q: 3.323625, mean_eps: 0.500000\n",
            "  535/8000: episode: 35, duration: 0.317s, episode steps:  13, steps per second:  41, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.368663, mae: 1.807057, mean_q: 3.442047, mean_eps: 0.500000\n",
            "  545/8000: episode: 36, duration: 0.274s, episode steps:  10, steps per second:  36, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.271352, mae: 1.823819, mean_q: 3.470684, mean_eps: 0.500000\n",
            "  554/8000: episode: 37, duration: 0.232s, episode steps:   9, steps per second:  39, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.320831, mae: 1.880909, mean_q: 3.526285, mean_eps: 0.500000\n",
            "  565/8000: episode: 38, duration: 0.273s, episode steps:  11, steps per second:  40, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.383824, mae: 1.944308, mean_q: 3.568901, mean_eps: 0.500000\n",
            "  579/8000: episode: 39, duration: 0.351s, episode steps:  14, steps per second:  40, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 0.431466, mae: 2.001548, mean_q: 3.662997, mean_eps: 0.500000\n",
            "  588/8000: episode: 40, duration: 0.224s, episode steps:   9, steps per second:  40, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.370290, mae: 1.983534, mean_q: 3.724610, mean_eps: 0.500000\n",
            "  601/8000: episode: 41, duration: 0.328s, episode steps:  13, steps per second:  40, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.326074, mae: 2.016073, mean_q: 3.772815, mean_eps: 0.500000\n",
            "  613/8000: episode: 42, duration: 0.268s, episode steps:  12, steps per second:  45, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.323892, mae: 2.075411, mean_q: 3.857588, mean_eps: 0.500000\n",
            "  629/8000: episode: 43, duration: 0.317s, episode steps:  16, steps per second:  50, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.356653, mae: 2.137712, mean_q: 4.009197, mean_eps: 0.500000\n",
            "  640/8000: episode: 44, duration: 0.227s, episode steps:  11, steps per second:  48, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.335722, mae: 2.160696, mean_q: 4.052915, mean_eps: 0.500000\n",
            "  654/8000: episode: 45, duration: 0.245s, episode steps:  14, steps per second:  57, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.294108, mae: 2.191616, mean_q: 4.141251, mean_eps: 0.500000\n",
            "  663/8000: episode: 46, duration: 0.174s, episode steps:   9, steps per second:  52, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.362888, mae: 2.251265, mean_q: 4.258038, mean_eps: 0.500000\n",
            "  673/8000: episode: 47, duration: 0.232s, episode steps:  10, steps per second:  43, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.244385, mae: 2.233435, mean_q: 4.296081, mean_eps: 0.500000\n",
            "  684/8000: episode: 48, duration: 0.231s, episode steps:  11, steps per second:  48, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.384764, mae: 2.316376, mean_q: 4.282679, mean_eps: 0.500000\n",
            "  700/8000: episode: 49, duration: 0.360s, episode steps:  16, steps per second:  44, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.384079, mae: 2.339390, mean_q: 4.368562, mean_eps: 0.500000\n",
            "  719/8000: episode: 50, duration: 0.453s, episode steps:  19, steps per second:  42, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 0.325321, mae: 2.376811, mean_q: 4.450742, mean_eps: 0.500000\n",
            "  734/8000: episode: 51, duration: 0.364s, episode steps:  15, steps per second:  41, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 0.308233, mae: 2.444515, mean_q: 4.575341, mean_eps: 0.500000\n",
            "  746/8000: episode: 52, duration: 0.262s, episode steps:  12, steps per second:  46, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.280238, mae: 2.477484, mean_q: 4.686570, mean_eps: 0.500000\n",
            "  757/8000: episode: 53, duration: 0.252s, episode steps:  11, steps per second:  44, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.314094, mae: 2.544237, mean_q: 4.768182, mean_eps: 0.500000\n",
            "  769/8000: episode: 54, duration: 0.303s, episode steps:  12, steps per second:  40, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.255300, mae: 2.567385, mean_q: 4.787478, mean_eps: 0.500000\n",
            "  784/8000: episode: 55, duration: 0.350s, episode steps:  15, steps per second:  43, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.266539, mae: 2.588253, mean_q: 4.915160, mean_eps: 0.500000\n",
            "  796/8000: episode: 56, duration: 0.295s, episode steps:  12, steps per second:  41, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.296604, mae: 2.640293, mean_q: 4.947237, mean_eps: 0.500000\n",
            "  810/8000: episode: 57, duration: 0.383s, episode steps:  14, steps per second:  37, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.382053, mae: 2.697705, mean_q: 5.024308, mean_eps: 0.500000\n",
            "  827/8000: episode: 58, duration: 0.393s, episode steps:  17, steps per second:  43, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.491960, mae: 2.787601, mean_q: 5.115363, mean_eps: 0.500000\n",
            "  838/8000: episode: 59, duration: 0.220s, episode steps:  11, steps per second:  50, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.377872, mae: 2.767663, mean_q: 5.049358, mean_eps: 0.500000\n",
            "  848/8000: episode: 60, duration: 0.160s, episode steps:  10, steps per second:  63, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.359378, mae: 2.828520, mean_q: 5.272484, mean_eps: 0.500000\n",
            "  860/8000: episode: 61, duration: 0.240s, episode steps:  12, steps per second:  50, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.422500, mae: 2.843982, mean_q: 5.314528, mean_eps: 0.500000\n",
            "  877/8000: episode: 62, duration: 0.297s, episode steps:  17, steps per second:  57, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.447508, mae: 2.915426, mean_q: 5.365190, mean_eps: 0.500000\n",
            "  890/8000: episode: 63, duration: 0.242s, episode steps:  13, steps per second:  54, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.314492, mae: 2.935795, mean_q: 5.467675, mean_eps: 0.500000\n",
            "  904/8000: episode: 64, duration: 0.214s, episode steps:  14, steps per second:  65, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.426857, mae: 2.984380, mean_q: 5.527847, mean_eps: 0.500000\n",
            "  920/8000: episode: 65, duration: 0.280s, episode steps:  16, steps per second:  57, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 0.430356, mae: 3.054643, mean_q: 5.715309, mean_eps: 0.500000\n",
            "  936/8000: episode: 66, duration: 0.284s, episode steps:  16, steps per second:  56, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.464183, mae: 3.098262, mean_q: 5.821634, mean_eps: 0.500000\n",
            "  944/8000: episode: 67, duration: 0.150s, episode steps:   8, steps per second:  53, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.643903, mae: 3.262772, mean_q: 6.103474, mean_eps: 0.500000\n",
            "  959/8000: episode: 68, duration: 0.222s, episode steps:  15, steps per second:  67, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 0.521995, mae: 3.242502, mean_q: 6.076954, mean_eps: 0.500000\n",
            "  974/8000: episode: 69, duration: 0.258s, episode steps:  15, steps per second:  58, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.728953, mae: 3.341115, mean_q: 6.219888, mean_eps: 0.500000\n",
            "  987/8000: episode: 70, duration: 0.197s, episode steps:  13, steps per second:  66, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.596916, mae: 3.337196, mean_q: 6.216487, mean_eps: 0.500000\n",
            "  999/8000: episode: 71, duration: 0.190s, episode steps:  12, steps per second:  63, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.794249, mae: 3.359317, mean_q: 6.223541, mean_eps: 0.500000\n",
            " 1013/8000: episode: 72, duration: 0.207s, episode steps:  14, steps per second:  68, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.830279, mae: 3.479724, mean_q: 6.460751, mean_eps: 0.500000\n",
            " 1027/8000: episode: 73, duration: 0.206s, episode steps:  14, steps per second:  68, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.143 [0.000, 1.000],  loss: 0.959805, mae: 3.545754, mean_q: 6.548446, mean_eps: 0.500000\n",
            " 1036/8000: episode: 74, duration: 0.164s, episode steps:   9, steps per second:  55, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.617819, mae: 3.523233, mean_q: 6.567243, mean_eps: 0.500000\n",
            " 1048/8000: episode: 75, duration: 0.195s, episode steps:  12, steps per second:  61, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.055266, mae: 3.606717, mean_q: 6.679878, mean_eps: 0.500000\n",
            " 1060/8000: episode: 76, duration: 0.202s, episode steps:  12, steps per second:  59, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 1.065131, mae: 3.679320, mean_q: 6.713039, mean_eps: 0.500000\n",
            " 1074/8000: episode: 77, duration: 0.248s, episode steps:  14, steps per second:  56, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.844843, mae: 3.695557, mean_q: 6.859836, mean_eps: 0.500000\n",
            " 1085/8000: episode: 78, duration: 0.175s, episode steps:  11, steps per second:  63, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.666399, mae: 3.643245, mean_q: 6.768020, mean_eps: 0.500000\n",
            " 1099/8000: episode: 79, duration: 0.229s, episode steps:  14, steps per second:  61, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.214 [0.000, 1.000],  loss: 0.776176, mae: 3.736223, mean_q: 7.012821, mean_eps: 0.500000\n",
            " 1121/8000: episode: 80, duration: 0.316s, episode steps:  22, steps per second:  70, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.318 [0.000, 1.000],  loss: 1.368230, mae: 3.859537, mean_q: 7.067098, mean_eps: 0.500000\n",
            " 1134/8000: episode: 81, duration: 0.188s, episode steps:  13, steps per second:  69, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.154 [0.000, 1.000],  loss: 1.083102, mae: 3.853611, mean_q: 7.039995, mean_eps: 0.500000\n",
            " 1146/8000: episode: 82, duration: 0.206s, episode steps:  12, steps per second:  58, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.827689, mae: 3.869755, mean_q: 7.093795, mean_eps: 0.500000\n",
            " 1157/8000: episode: 83, duration: 0.237s, episode steps:  11, steps per second:  46, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 1.371698, mae: 3.988565, mean_q: 7.363228, mean_eps: 0.500000\n",
            " 1168/8000: episode: 84, duration: 0.262s, episode steps:  11, steps per second:  42, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.642239, mae: 3.957563, mean_q: 7.487677, mean_eps: 0.500000\n",
            " 1178/8000: episode: 85, duration: 0.213s, episode steps:  10, steps per second:  47, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.873272, mae: 4.063804, mean_q: 7.688621, mean_eps: 0.500000\n",
            " 1187/8000: episode: 86, duration: 0.204s, episode steps:   9, steps per second:  44, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 1.392169, mae: 4.082559, mean_q: 7.593590, mean_eps: 0.500000\n",
            " 1200/8000: episode: 87, duration: 0.213s, episode steps:  13, steps per second:  61, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 1.244257, mae: 4.146850, mean_q: 7.715874, mean_eps: 0.500000\n",
            " 1211/8000: episode: 88, duration: 0.168s, episode steps:  11, steps per second:  65, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 1.277286, mae: 4.122403, mean_q: 7.595645, mean_eps: 0.500000\n",
            " 1224/8000: episode: 89, duration: 0.186s, episode steps:  13, steps per second:  70, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.154 [0.000, 1.000],  loss: 1.079543, mae: 4.151568, mean_q: 7.708545, mean_eps: 0.500000\n",
            " 1233/8000: episode: 90, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.385880, mae: 4.249874, mean_q: 7.850069, mean_eps: 0.500000\n",
            " 1247/8000: episode: 91, duration: 0.222s, episode steps:  14, steps per second:  63, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 1.323432, mae: 4.235758, mean_q: 7.799764, mean_eps: 0.500000\n",
            " 1273/8000: episode: 92, duration: 0.406s, episode steps:  26, steps per second:  64, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 1.355633, mae: 4.318747, mean_q: 7.980429, mean_eps: 0.500000\n",
            " 1286/8000: episode: 93, duration: 0.206s, episode steps:  13, steps per second:  63, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.154 [0.000, 1.000],  loss: 1.480727, mae: 4.403248, mean_q: 8.142647, mean_eps: 0.500000\n",
            " 1299/8000: episode: 94, duration: 0.197s, episode steps:  13, steps per second:  66, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.154 [0.000, 1.000],  loss: 1.905180, mae: 4.448623, mean_q: 8.169173, mean_eps: 0.500000\n",
            " 1311/8000: episode: 95, duration: 0.174s, episode steps:  12, steps per second:  69, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 1.431632, mae: 4.408820, mean_q: 8.082119, mean_eps: 0.500000\n",
            " 1321/8000: episode: 96, duration: 0.166s, episode steps:  10, steps per second:  60, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 1.023113, mae: 4.390645, mean_q: 8.141435, mean_eps: 0.500000\n",
            " 1329/8000: episode: 97, duration: 0.188s, episode steps:   8, steps per second:  42, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.675912, mae: 4.542676, mean_q: 8.283590, mean_eps: 0.500000\n",
            " 1340/8000: episode: 98, duration: 0.257s, episode steps:  11, steps per second:  43, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 1.189925, mae: 4.524978, mean_q: 8.375514, mean_eps: 0.500000\n",
            " 1377/8000: episode: 99, duration: 0.693s, episode steps:  37, steps per second:  53, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.378 [0.000, 1.000],  loss: 1.614718, mae: 4.596776, mean_q: 8.312932, mean_eps: 0.500000\n",
            " 1387/8000: episode: 100, duration: 0.157s, episode steps:  10, steps per second:  64, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.716694, mae: 4.547762, mean_q: 8.207525, mean_eps: 0.500000\n",
            " 1398/8000: episode: 101, duration: 0.192s, episode steps:  11, steps per second:  57, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 1.400671, mae: 4.550858, mean_q: 8.304248, mean_eps: 0.500000\n",
            " 1411/8000: episode: 102, duration: 0.184s, episode steps:  13, steps per second:  71, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.154 [0.000, 1.000],  loss: 1.579134, mae: 4.622764, mean_q: 8.394538, mean_eps: 0.500000\n",
            " 1425/8000: episode: 103, duration: 0.227s, episode steps:  14, steps per second:  62, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.214 [0.000, 1.000],  loss: 1.197344, mae: 4.628591, mean_q: 8.517069, mean_eps: 0.500000\n",
            " 1440/8000: episode: 104, duration: 0.216s, episode steps:  15, steps per second:  70, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.037182, mae: 4.625484, mean_q: 8.527601, mean_eps: 0.500000\n",
            " 1475/8000: episode: 105, duration: 0.513s, episode steps:  35, steps per second:  68, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 1.710681, mae: 4.793842, mean_q: 8.684560, mean_eps: 0.500000\n",
            " 1506/8000: episode: 106, duration: 0.435s, episode steps:  31, steps per second:  71, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.581 [0.000, 1.000],  loss: 1.543034, mae: 4.797159, mean_q: 8.727184, mean_eps: 0.500000\n",
            " 1526/8000: episode: 107, duration: 0.297s, episode steps:  20, steps per second:  67, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 0.949953, mae: 4.812572, mean_q: 8.984302, mean_eps: 0.500000\n",
            " 1638/8000: episode: 108, duration: 1.599s, episode steps: 112, steps per second:  70, episode reward: 112.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 1.339756, mae: 4.957848, mean_q: 9.149804, mean_eps: 0.500000\n",
            " 1785/8000: episode: 109, duration: 2.764s, episode steps: 147, steps per second:  53, episode reward: 147.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 1.102452, mae: 5.237975, mean_q: 9.833150, mean_eps: 0.500000\n",
            " 1837/8000: episode: 110, duration: 0.892s, episode steps:  52, steps per second:  58, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1.170106, mae: 5.478023, mean_q: 10.294331, mean_eps: 0.500000\n",
            " 1869/8000: episode: 111, duration: 0.515s, episode steps:  32, steps per second:  62, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.466869, mae: 5.590080, mean_q: 10.450799, mean_eps: 0.500000\n",
            " 2000/8000: episode: 112, duration: 1.993s, episode steps: 131, steps per second:  66, episode reward: 131.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 1.182557, mae: 5.794200, mean_q: 11.008548, mean_eps: 0.500000\n",
            " 2047/8000: episode: 113, duration: 0.718s, episode steps:  47, steps per second:  65, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 1.219178, mae: 6.050362, mean_q: 11.511911, mean_eps: 0.500000\n",
            " 2094/8000: episode: 114, duration: 0.637s, episode steps:  47, steps per second:  74, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 1.484405, mae: 6.111493, mean_q: 11.552571, mean_eps: 0.500000\n",
            " 2148/8000: episode: 115, duration: 0.822s, episode steps:  54, steps per second:  66, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 1.201863, mae: 6.222228, mean_q: 11.850198, mean_eps: 0.500000\n",
            " 2190/8000: episode: 116, duration: 0.628s, episode steps:  42, steps per second:  67, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.241482, mae: 6.418727, mean_q: 12.289713, mean_eps: 0.500000\n",
            " 2240/8000: episode: 117, duration: 0.727s, episode steps:  50, steps per second:  69, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 1.472235, mae: 6.492651, mean_q: 12.339475, mean_eps: 0.500000\n",
            " 2305/8000: episode: 118, duration: 0.983s, episode steps:  65, steps per second:  66, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 1.286701, mae: 6.693262, mean_q: 12.823020, mean_eps: 0.500000\n",
            " 2399/8000: episode: 119, duration: 1.738s, episode steps:  94, steps per second:  54, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 1.489866, mae: 6.868245, mean_q: 13.130103, mean_eps: 0.500000\n",
            " 2498/8000: episode: 120, duration: 1.635s, episode steps:  99, steps per second:  61, episode reward: 99.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 1.314393, mae: 7.146676, mean_q: 13.760053, mean_eps: 0.500000\n",
            " 2536/8000: episode: 121, duration: 0.739s, episode steps:  38, steps per second:  51, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.361050, mae: 7.331223, mean_q: 14.163577, mean_eps: 0.500000\n",
            " 2641/8000: episode: 122, duration: 1.797s, episode steps: 105, steps per second:  58, episode reward: 105.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 1.610457, mae: 7.578560, mean_q: 14.605797, mean_eps: 0.500000\n",
            " 2667/8000: episode: 123, duration: 0.492s, episode steps:  26, steps per second:  53, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1.502141, mae: 7.661496, mean_q: 14.752487, mean_eps: 0.500000\n",
            " 2718/8000: episode: 124, duration: 0.908s, episode steps:  51, steps per second:  56, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.263150, mae: 7.863644, mean_q: 15.028053, mean_eps: 0.500000\n",
            " 2799/8000: episode: 125, duration: 1.484s, episode steps:  81, steps per second:  55, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.873322, mae: 8.048460, mean_q: 15.534814, mean_eps: 0.500000\n",
            " 2854/8000: episode: 126, duration: 1.356s, episode steps:  55, steps per second:  41, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 2.026807, mae: 8.230822, mean_q: 15.892650, mean_eps: 0.500000\n",
            " 2905/8000: episode: 127, duration: 1.249s, episode steps:  51, steps per second:  41, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1.992782, mae: 8.355839, mean_q: 16.151252, mean_eps: 0.500000\n",
            " 2977/8000: episode: 128, duration: 1.326s, episode steps:  72, steps per second:  54, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 2.303288, mae: 8.587287, mean_q: 16.588398, mean_eps: 0.500000\n",
            " 3066/8000: episode: 129, duration: 1.232s, episode steps:  89, steps per second:  72, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 2.221017, mae: 8.753286, mean_q: 16.953926, mean_eps: 0.500000\n",
            " 3140/8000: episode: 130, duration: 1.454s, episode steps:  74, steps per second:  51, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 2.513286, mae: 8.976395, mean_q: 17.297549, mean_eps: 0.500000\n",
            " 3194/8000: episode: 131, duration: 0.971s, episode steps:  54, steps per second:  56, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 2.424114, mae: 9.166211, mean_q: 17.743646, mean_eps: 0.500000\n",
            " 3223/8000: episode: 132, duration: 0.430s, episode steps:  29, steps per second:  67, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.586 [0.000, 1.000],  loss: 1.986588, mae: 9.332415, mean_q: 18.106886, mean_eps: 0.500000\n",
            " 3271/8000: episode: 133, duration: 0.963s, episode steps:  48, steps per second:  50, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 2.502646, mae: 9.327239, mean_q: 18.023965, mean_eps: 0.500000\n",
            " 3350/8000: episode: 134, duration: 1.482s, episode steps:  79, steps per second:  53, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 3.107588, mae: 9.566819, mean_q: 18.468104, mean_eps: 0.500000\n",
            " 3430/8000: episode: 135, duration: 1.750s, episode steps:  80, steps per second:  46, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 2.650165, mae: 9.668339, mean_q: 18.689927, mean_eps: 0.500000\n",
            " 3510/8000: episode: 136, duration: 1.327s, episode steps:  80, steps per second:  60, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 2.647468, mae: 9.875169, mean_q: 19.054747, mean_eps: 0.500000\n",
            " 3581/8000: episode: 137, duration: 1.151s, episode steps:  71, steps per second:  62, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 2.803763, mae: 10.098303, mean_q: 19.538721, mean_eps: 0.500000\n",
            " 3616/8000: episode: 138, duration: 0.571s, episode steps:  35, steps per second:  61, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 2.594135, mae: 10.321736, mean_q: 20.023925, mean_eps: 0.500000\n",
            " 3751/8000: episode: 139, duration: 2.483s, episode steps: 135, steps per second:  54, episode reward: 135.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 2.872873, mae: 10.433461, mean_q: 20.273084, mean_eps: 0.500000\n",
            " 3784/8000: episode: 140, duration: 0.631s, episode steps:  33, steps per second:  52, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 2.919500, mae: 10.691111, mean_q: 20.907778, mean_eps: 0.500000\n",
            " 3812/8000: episode: 141, duration: 0.536s, episode steps:  28, steps per second:  52, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.509837, mae: 10.730675, mean_q: 20.974558, mean_eps: 0.500000\n",
            " 3865/8000: episode: 142, duration: 0.943s, episode steps:  53, steps per second:  56, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 2.097886, mae: 10.821722, mean_q: 21.208383, mean_eps: 0.500000\n",
            " 3898/8000: episode: 143, duration: 0.588s, episode steps:  33, steps per second:  56, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 3.473897, mae: 11.080418, mean_q: 21.613306, mean_eps: 0.500000\n",
            " 3973/8000: episode: 144, duration: 1.391s, episode steps:  75, steps per second:  54, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 3.408774, mae: 11.121371, mean_q: 21.615610, mean_eps: 0.500000\n",
            " 4060/8000: episode: 145, duration: 1.530s, episode steps:  87, steps per second:  57, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 2.881343, mae: 11.336076, mean_q: 22.135517, mean_eps: 0.500000\n",
            " 4125/8000: episode: 146, duration: 1.197s, episode steps:  65, steps per second:  54, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 3.321192, mae: 11.485113, mean_q: 22.407129, mean_eps: 0.500000\n",
            " 4156/8000: episode: 147, duration: 0.647s, episode steps:  31, steps per second:  48, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 3.106240, mae: 11.628437, mean_q: 22.737729, mean_eps: 0.500000\n",
            " 4227/8000: episode: 148, duration: 1.294s, episode steps:  71, steps per second:  55, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 3.901754, mae: 11.745045, mean_q: 22.872864, mean_eps: 0.500000\n",
            " 4268/8000: episode: 149, duration: 0.901s, episode steps:  41, steps per second:  46, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.561 [0.000, 1.000],  loss: 3.181107, mae: 11.763502, mean_q: 22.988414, mean_eps: 0.500000\n",
            " 4365/8000: episode: 150, duration: 1.626s, episode steps:  97, steps per second:  60, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 3.995039, mae: 11.980996, mean_q: 23.298140, mean_eps: 0.500000\n",
            " 4392/8000: episode: 151, duration: 0.513s, episode steps:  27, steps per second:  53, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 5.227958, mae: 12.232097, mean_q: 23.717389, mean_eps: 0.500000\n",
            " 4419/8000: episode: 152, duration: 0.547s, episode steps:  27, steps per second:  49, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 3.581547, mae: 12.114750, mean_q: 23.582722, mean_eps: 0.500000\n",
            " 4457/8000: episode: 153, duration: 0.621s, episode steps:  38, steps per second:  61, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  loss: 3.354000, mae: 12.183014, mean_q: 23.815086, mean_eps: 0.500000\n",
            " 4526/8000: episode: 154, duration: 1.497s, episode steps:  69, steps per second:  46, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 3.688680, mae: 12.397667, mean_q: 24.205597, mean_eps: 0.500000\n",
            " 4566/8000: episode: 155, duration: 0.765s, episode steps:  40, steps per second:  52, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 2.920484, mae: 12.408573, mean_q: 24.378981, mean_eps: 0.500000\n",
            " 4606/8000: episode: 156, duration: 0.662s, episode steps:  40, steps per second:  60, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 4.020511, mae: 12.562690, mean_q: 24.539397, mean_eps: 0.500000\n",
            " 4656/8000: episode: 157, duration: 0.899s, episode steps:  50, steps per second:  56, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 3.842842, mae: 12.659491, mean_q: 24.771675, mean_eps: 0.500000\n",
            " 4688/8000: episode: 158, duration: 0.541s, episode steps:  32, steps per second:  59, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 4.006865, mae: 12.808720, mean_q: 25.013946, mean_eps: 0.500000\n",
            " 4743/8000: episode: 159, duration: 0.891s, episode steps:  55, steps per second:  62, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 3.998346, mae: 12.882557, mean_q: 25.217284, mean_eps: 0.500000\n",
            " 4801/8000: episode: 160, duration: 0.972s, episode steps:  58, steps per second:  60, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 5.177586, mae: 13.108536, mean_q: 25.507351, mean_eps: 0.500000\n",
            " 4989/8000: episode: 161, duration: 3.894s, episode steps: 188, steps per second:  48, episode reward: 188.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 4.748414, mae: 13.187042, mean_q: 25.712791, mean_eps: 0.500000\n",
            " 5142/8000: episode: 162, duration: 3.018s, episode steps: 153, steps per second:  51, episode reward: 153.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 5.233367, mae: 13.506944, mean_q: 26.288236, mean_eps: 0.500000\n",
            " 5224/8000: episode: 163, duration: 1.663s, episode steps:  82, steps per second:  49, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 4.469586, mae: 13.690802, mean_q: 26.831766, mean_eps: 0.500000\n",
            " 5264/8000: episode: 164, duration: 0.803s, episode steps:  40, steps per second:  50, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 5.024928, mae: 13.896606, mean_q: 27.163740, mean_eps: 0.500000\n",
            " 5326/8000: episode: 165, duration: 1.302s, episode steps:  62, steps per second:  48, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 4.292564, mae: 13.952989, mean_q: 27.376539, mean_eps: 0.500000\n",
            " 5526/8000: episode: 166, duration: 3.568s, episode steps: 200, steps per second:  56, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 4.104821, mae: 14.229349, mean_q: 27.949909, mean_eps: 0.500000\n",
            " 5553/8000: episode: 167, duration: 0.401s, episode steps:  27, steps per second:  67, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 5.263863, mae: 14.404190, mean_q: 28.214228, mean_eps: 0.500000\n",
            " 5665/8000: episode: 168, duration: 1.830s, episode steps: 112, steps per second:  61, episode reward: 112.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 4.057619, mae: 14.611433, mean_q: 28.762163, mean_eps: 0.500000\n",
            " 5720/8000: episode: 169, duration: 0.901s, episode steps:  55, steps per second:  61, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 4.896838, mae: 14.907117, mean_q: 29.349301, mean_eps: 0.500000\n",
            " 5807/8000: episode: 170, duration: 1.447s, episode steps:  87, steps per second:  60, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 5.036364, mae: 14.979185, mean_q: 29.475178, mean_eps: 0.500000\n",
            " 5889/8000: episode: 171, duration: 1.271s, episode steps:  82, steps per second:  64, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 5.694164, mae: 15.148415, mean_q: 29.785932, mean_eps: 0.500000\n",
            " 6060/8000: episode: 172, duration: 3.288s, episode steps: 171, steps per second:  52, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 5.135905, mae: 15.407621, mean_q: 30.323192, mean_eps: 0.500000\n",
            " 6260/8000: episode: 173, duration: 4.231s, episode steps: 200, steps per second:  47, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 5.590349, mae: 15.817431, mean_q: 31.175740, mean_eps: 0.500000\n",
            " 6340/8000: episode: 174, duration: 1.346s, episode steps:  80, steps per second:  59, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 4.701648, mae: 16.054844, mean_q: 31.782245, mean_eps: 0.500000\n",
            " 6478/8000: episode: 175, duration: 2.399s, episode steps: 138, steps per second:  58, episode reward: 138.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 6.443847, mae: 16.332937, mean_q: 32.152800, mean_eps: 0.500000\n",
            " 6678/8000: episode: 176, duration: 4.156s, episode steps: 200, steps per second:  48, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 6.580212, mae: 16.646835, mean_q: 32.783456, mean_eps: 0.500000\n",
            " 6761/8000: episode: 177, duration: 1.693s, episode steps:  83, steps per second:  49, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 7.347334, mae: 16.902716, mean_q: 33.219223, mean_eps: 0.500000\n",
            " 6948/8000: episode: 178, duration: 3.672s, episode steps: 187, steps per second:  51, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 6.301981, mae: 17.154383, mean_q: 33.860502, mean_eps: 0.500000\n",
            " 7144/8000: episode: 179, duration: 4.108s, episode steps: 196, steps per second:  48, episode reward: 196.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 5.290942, mae: 17.560838, mean_q: 34.774231, mean_eps: 0.500000\n",
            " 7265/8000: episode: 180, duration: 2.626s, episode steps: 121, steps per second:  46, episode reward: 121.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 5.578904, mae: 17.978794, mean_q: 35.658254, mean_eps: 0.500000\n",
            " 7465/8000: episode: 181, duration: 3.793s, episode steps: 200, steps per second:  53, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 6.874231, mae: 18.240061, mean_q: 36.039369, mean_eps: 0.500000\n",
            " 7665/8000: episode: 182, duration: 4.100s, episode steps: 200, steps per second:  49, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 6.696605, mae: 18.572715, mean_q: 36.726165, mean_eps: 0.500000\n",
            " 7718/8000: episode: 183, duration: 1.098s, episode steps:  53, steps per second:  48, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 6.161952, mae: 18.834072, mean_q: 37.436591, mean_eps: 0.500000\n",
            " 7918/8000: episode: 184, duration: 3.528s, episode steps: 200, steps per second:  57, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 6.888977, mae: 19.040191, mean_q: 37.728292, mean_eps: 0.500000\n",
            "done, took 161.003 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 200.000, steps: 200\n",
            "Episode 6: reward: 200.000, steps: 200\n",
            "Episode 7: reward: 200.000, steps: 200\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 200.000, steps: 200\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 200.000, steps: 200\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 200.000, steps: 200\n",
            "Episode 15: reward: 200.000, steps: 200\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 200.000, steps: 200\n",
            "Episode 18: reward: 200.000, steps: 200\n",
            "Episode 19: reward: 200.000, steps: 200\n",
            "Episode 20: reward: 200.000, steps: 200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb170f05fd0>"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    }
  ]
}