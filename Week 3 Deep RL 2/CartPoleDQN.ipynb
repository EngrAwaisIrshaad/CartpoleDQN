{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPoleDQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Megacity1/CartpoleDQN/blob/main/Week%203%20Deep%20RL%202/CartPoleDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Deep Q Network (DQN) for CartPole Using Boltzmann Q Policy\n",
        "This exercise implements a DQN for CartPole using a Boltzmann Q policy for selecting the actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGsC7cJ5jNcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda73cbf-3020-40f4-e45e-7b87053448f1"
      },
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.24.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (13.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.44.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.10.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMIHLgQ3Z-lF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5a8608-456f-4127-ce8a-f551e5f4e4f3"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0AMLzq08ap0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b380e1-6c3d-4bce-8233-a8ac01deaf26"
      },
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "!pip install Adam\n",
        "#from keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Adam in /usr/local/lib/python3.7/dist-packages (0.0.0.dev0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll6bNdUm54WS"
      },
      "source": [
        "Implementation of DQN for CartPole, applying policy BoltzmannQPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCrPKNy40PC"
      },
      "source": [
        "##Implement DQN with BoltzmannGumbelQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efM9jkXr5A3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1c48a840-9083-4921-8eb5-49034071207a"
      },
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import BoltzmannGumbelQPolicy\n",
        "from rl.policy import LinearAnnealedPolicy\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n",
        "# setup experience replay buffer\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
        "                               attr='eps',            \n",
        "                               value_max=1.,\n",
        "                               value_min=.1, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=200)\n",
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "# add extra layers here\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "print(model.summary())\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=40,\n",
        "               target_model_update=1e-3, \n",
        "               policy=policy) \n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=2000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_21 (Flatten)        (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_42 (Dense)            (None, 16)                80        \n",
            "                                                                 \n",
            " dense_43 (Dense)            (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 2000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   29/2000: episode: 1, duration: 1.576s, episode steps:  29, steps per second:  18, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.379 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   41/2000: episode: 2, duration: 5.278s, episode steps:  12, steps per second:   2, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   80/2000: episode: 3, duration: 0.472s, episode steps:  39, steps per second:  83, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.564 [0.000, 1.000],  loss: 0.445309, mae: 0.592736, mean_q: 0.268992, mean_eps: 0.730000\n",
            "   89/2000: episode: 4, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.317684, mae: 0.603765, mean_q: 0.438561, mean_eps: 0.622000\n",
            "  101/2000: episode: 5, duration: 0.167s, episode steps:  12, steps per second:  72, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.277895, mae: 0.608087, mean_q: 0.514322, mean_eps: 0.574750\n",
            "  112/2000: episode: 6, duration: 0.145s, episode steps:  11, steps per second:  76, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.238455, mae: 0.603565, mean_q: 0.574401, mean_eps: 0.523000\n",
            "  128/2000: episode: 7, duration: 0.243s, episode steps:  16, steps per second:  66, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 0.186194, mae: 0.628520, mean_q: 0.736387, mean_eps: 0.462250\n",
            "  138/2000: episode: 8, duration: 0.142s, episode steps:  10, steps per second:  70, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.150567, mae: 0.627189, mean_q: 0.812843, mean_eps: 0.403750\n",
            "  148/2000: episode: 9, duration: 0.155s, episode steps:  10, steps per second:  65, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.131795, mae: 0.656859, mean_q: 0.931397, mean_eps: 0.358750\n",
            "  158/2000: episode: 10, duration: 0.142s, episode steps:  10, steps per second:  70, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.112926, mae: 0.645756, mean_q: 0.962340, mean_eps: 0.313750\n",
            "  170/2000: episode: 11, duration: 0.205s, episode steps:  12, steps per second:  58, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.083 [0.000, 1.000],  loss: 0.099134, mae: 0.649455, mean_q: 0.990881, mean_eps: 0.264250\n",
            "  183/2000: episode: 12, duration: 0.173s, episode steps:  13, steps per second:  75, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.154 [0.000, 1.000],  loss: 0.077233, mae: 0.656206, mean_q: 1.077384, mean_eps: 0.208000\n",
            "  194/2000: episode: 13, duration: 0.156s, episode steps:  11, steps per second:  70, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.085068, mae: 0.674341, mean_q: 1.075749, mean_eps: 0.154000\n",
            "  204/2000: episode: 14, duration: 0.135s, episode steps:  10, steps per second:  74, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.065778, mae: 0.658345, mean_q: 1.062187, mean_eps: 0.109450\n",
            "  217/2000: episode: 15, duration: 0.161s, episode steps:  13, steps per second:  81, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.077 [0.000, 1.000],  loss: 0.057422, mae: 0.664081, mean_q: 1.081744, mean_eps: 0.100000\n",
            "  230/2000: episode: 16, duration: 0.163s, episode steps:  13, steps per second:  80, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.154 [0.000, 1.000],  loss: 0.054249, mae: 0.681249, mean_q: 1.135386, mean_eps: 0.100000\n",
            "  240/2000: episode: 17, duration: 0.136s, episode steps:  10, steps per second:  73, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.050117, mae: 0.692150, mean_q: 1.172611, mean_eps: 0.100000\n",
            "  251/2000: episode: 18, duration: 0.171s, episode steps:  11, steps per second:  64, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.047894, mae: 0.701151, mean_q: 1.189421, mean_eps: 0.100000\n",
            "  273/2000: episode: 19, duration: 0.312s, episode steps:  22, steps per second:  71, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.040010, mae: 0.675485, mean_q: 1.147492, mean_eps: 0.100000\n",
            "  298/2000: episode: 20, duration: 0.359s, episode steps:  25, steps per second:  70, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.680 [0.000, 1.000],  loss: 0.035361, mae: 0.691505, mean_q: 1.204865, mean_eps: 0.100000\n",
            "  322/2000: episode: 21, duration: 0.326s, episode steps:  24, steps per second:  74, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.029631, mae: 0.703495, mean_q: 1.251708, mean_eps: 0.100000\n",
            "  347/2000: episode: 22, duration: 0.399s, episode steps:  25, steps per second:  63, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.028928, mae: 0.701692, mean_q: 1.300472, mean_eps: 0.100000\n",
            "  357/2000: episode: 23, duration: 0.159s, episode steps:  10, steps per second:  63, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.043485, mae: 0.752264, mean_q: 1.402132, mean_eps: 0.100000\n",
            "  379/2000: episode: 24, duration: 0.304s, episode steps:  22, steps per second:  72, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.773 [0.000, 1.000],  loss: 0.030190, mae: 0.723070, mean_q: 1.337891, mean_eps: 0.100000\n",
            "  388/2000: episode: 25, duration: 0.132s, episode steps:   9, steps per second:  68, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.040671, mae: 0.755909, mean_q: 1.417405, mean_eps: 0.100000\n",
            "  398/2000: episode: 26, duration: 0.116s, episode steps:  10, steps per second:  86, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.031912, mae: 0.756903, mean_q: 1.433354, mean_eps: 0.100000\n",
            "  407/2000: episode: 27, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.042810, mae: 0.752736, mean_q: 1.415886, mean_eps: 0.100000\n",
            "  415/2000: episode: 28, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.035663, mae: 0.769070, mean_q: 1.418852, mean_eps: 0.100000\n",
            "  425/2000: episode: 29, duration: 0.119s, episode steps:  10, steps per second:  84, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.042334, mae: 0.784556, mean_q: 1.445395, mean_eps: 0.100000\n",
            "  436/2000: episode: 30, duration: 0.123s, episode steps:  11, steps per second:  90, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.035514, mae: 0.763104, mean_q: 1.447449, mean_eps: 0.100000\n",
            "  445/2000: episode: 31, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.030291, mae: 0.774282, mean_q: 1.477879, mean_eps: 0.100000\n",
            "  455/2000: episode: 32, duration: 0.112s, episode steps:  10, steps per second:  89, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.032338, mae: 0.768164, mean_q: 1.494913, mean_eps: 0.100000\n",
            "  464/2000: episode: 33, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.031495, mae: 0.772713, mean_q: 1.501421, mean_eps: 0.100000\n",
            "  478/2000: episode: 34, duration: 0.178s, episode steps:  14, steps per second:  79, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.857 [0.000, 1.000],  loss: 0.056207, mae: 0.805946, mean_q: 1.544161, mean_eps: 0.100000\n",
            "  489/2000: episode: 35, duration: 0.153s, episode steps:  11, steps per second:  72, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.042577, mae: 0.795139, mean_q: 1.496472, mean_eps: 0.100000\n",
            "  497/2000: episode: 36, duration: 0.115s, episode steps:   8, steps per second:  70, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.047169, mae: 0.792586, mean_q: 1.470924, mean_eps: 0.100000\n",
            "  508/2000: episode: 37, duration: 0.143s, episode steps:  11, steps per second:  77, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.050842, mae: 0.817154, mean_q: 1.535191, mean_eps: 0.100000\n",
            "  517/2000: episode: 38, duration: 0.127s, episode steps:   9, steps per second:  71, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.040882, mae: 0.825617, mean_q: 1.563157, mean_eps: 0.100000\n",
            "  526/2000: episode: 39, duration: 0.116s, episode steps:   9, steps per second:  78, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.035867, mae: 0.818946, mean_q: 1.608101, mean_eps: 0.100000\n",
            "  536/2000: episode: 40, duration: 0.161s, episode steps:  10, steps per second:  62, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.038072, mae: 0.813892, mean_q: 1.616063, mean_eps: 0.100000\n",
            "  545/2000: episode: 41, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.040301, mae: 0.838427, mean_q: 1.690737, mean_eps: 0.100000\n",
            "  557/2000: episode: 42, duration: 0.139s, episode steps:  12, steps per second:  86, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 0.054227, mae: 0.854927, mean_q: 1.659580, mean_eps: 0.100000\n",
            "  569/2000: episode: 43, duration: 0.133s, episode steps:  12, steps per second:  90, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.048678, mae: 0.836403, mean_q: 1.608216, mean_eps: 0.100000\n",
            "  579/2000: episode: 44, duration: 0.108s, episode steps:  10, steps per second:  92, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.047249, mae: 0.839553, mean_q: 1.621091, mean_eps: 0.100000\n",
            "  590/2000: episode: 45, duration: 0.112s, episode steps:  11, steps per second:  98, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.067641, mae: 0.873317, mean_q: 1.686134, mean_eps: 0.100000\n",
            "  600/2000: episode: 46, duration: 0.125s, episode steps:  10, steps per second:  80, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.051703, mae: 0.857786, mean_q: 1.651017, mean_eps: 0.100000\n",
            "  608/2000: episode: 47, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.058036, mae: 0.867863, mean_q: 1.664487, mean_eps: 0.100000\n",
            "  618/2000: episode: 48, duration: 0.111s, episode steps:  10, steps per second:  90, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.060573, mae: 0.871889, mean_q: 1.637187, mean_eps: 0.100000\n",
            "  631/2000: episode: 49, duration: 0.150s, episode steps:  13, steps per second:  87, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.042403, mae: 0.869009, mean_q: 1.652896, mean_eps: 0.100000\n",
            "  641/2000: episode: 50, duration: 0.105s, episode steps:  10, steps per second:  95, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.064912, mae: 0.890740, mean_q: 1.679901, mean_eps: 0.100000\n",
            "  650/2000: episode: 51, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.061522, mae: 0.890829, mean_q: 1.726201, mean_eps: 0.100000\n",
            "  659/2000: episode: 52, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.059874, mae: 0.900125, mean_q: 1.748695, mean_eps: 0.100000\n",
            "  669/2000: episode: 53, duration: 0.120s, episode steps:  10, steps per second:  83, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.055508, mae: 0.900167, mean_q: 1.753999, mean_eps: 0.100000\n",
            "  680/2000: episode: 54, duration: 0.118s, episode steps:  11, steps per second:  93, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.043350, mae: 0.888124, mean_q: 1.754475, mean_eps: 0.100000\n",
            "  693/2000: episode: 55, duration: 0.150s, episode steps:  13, steps per second:  87, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.846 [0.000, 1.000],  loss: 0.058339, mae: 0.902256, mean_q: 1.791331, mean_eps: 0.100000\n",
            "  701/2000: episode: 56, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.043159, mae: 0.876637, mean_q: 1.738821, mean_eps: 0.100000\n",
            "  710/2000: episode: 57, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.055955, mae: 0.885579, mean_q: 1.720685, mean_eps: 0.100000\n",
            "  719/2000: episode: 58, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.056932, mae: 0.905445, mean_q: 1.742176, mean_eps: 0.100000\n",
            "  729/2000: episode: 59, duration: 0.114s, episode steps:  10, steps per second:  88, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.062890, mae: 0.932955, mean_q: 1.795570, mean_eps: 0.100000\n",
            "  741/2000: episode: 60, duration: 0.128s, episode steps:  12, steps per second:  93, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.053670, mae: 0.919774, mean_q: 1.796568, mean_eps: 0.100000\n",
            "  751/2000: episode: 61, duration: 0.125s, episode steps:  10, steps per second:  80, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.062440, mae: 0.937358, mean_q: 1.816137, mean_eps: 0.100000\n",
            "  759/2000: episode: 62, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.042829, mae: 0.911145, mean_q: 1.810481, mean_eps: 0.100000\n",
            "  769/2000: episode: 63, duration: 0.109s, episode steps:  10, steps per second:  91, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.080895, mae: 0.945729, mean_q: 1.834352, mean_eps: 0.100000\n",
            "  779/2000: episode: 64, duration: 0.116s, episode steps:  10, steps per second:  86, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.054606, mae: 0.933147, mean_q: 1.786940, mean_eps: 0.100000\n",
            "  790/2000: episode: 65, duration: 0.108s, episode steps:  11, steps per second: 102, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.065881, mae: 0.947669, mean_q: 1.819477, mean_eps: 0.100000\n",
            "  801/2000: episode: 66, duration: 0.119s, episode steps:  11, steps per second:  92, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.054297, mae: 0.940580, mean_q: 1.833186, mean_eps: 0.100000\n",
            "  809/2000: episode: 67, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.050754, mae: 0.939596, mean_q: 1.890376, mean_eps: 0.100000\n",
            "  818/2000: episode: 68, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.050522, mae: 0.947452, mean_q: 1.906046, mean_eps: 0.100000\n",
            "  827/2000: episode: 69, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.085252, mae: 1.000722, mean_q: 1.895393, mean_eps: 0.100000\n",
            "  836/2000: episode: 70, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.076877, mae: 0.979321, mean_q: 1.850421, mean_eps: 0.100000\n",
            "  848/2000: episode: 71, duration: 0.135s, episode steps:  12, steps per second:  89, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.067302, mae: 0.966670, mean_q: 1.883579, mean_eps: 0.100000\n",
            "  856/2000: episode: 72, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.066884, mae: 0.965947, mean_q: 1.869873, mean_eps: 0.100000\n",
            "  864/2000: episode: 73, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.061921, mae: 0.971696, mean_q: 1.884733, mean_eps: 0.100000\n",
            "  872/2000: episode: 74, duration: 0.085s, episode steps:   8, steps per second:  95, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.055170, mae: 0.955753, mean_q: 1.884444, mean_eps: 0.100000\n",
            "  881/2000: episode: 75, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.045310, mae: 0.947432, mean_q: 1.918347, mean_eps: 0.100000\n",
            "  891/2000: episode: 76, duration: 0.106s, episode steps:  10, steps per second:  94, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.072016, mae: 0.975524, mean_q: 1.944264, mean_eps: 0.100000\n",
            "  903/2000: episode: 77, duration: 0.126s, episode steps:  12, steps per second:  95, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 0.056152, mae: 0.961535, mean_q: 1.950328, mean_eps: 0.100000\n",
            "  914/2000: episode: 78, duration: 0.107s, episode steps:  11, steps per second: 103, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.073946, mae: 0.975053, mean_q: 1.956895, mean_eps: 0.100000\n",
            "  923/2000: episode: 79, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.048649, mae: 0.956374, mean_q: 1.922430, mean_eps: 0.100000\n",
            "  933/2000: episode: 80, duration: 0.136s, episode steps:  10, steps per second:  73, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.072892, mae: 0.991472, mean_q: 1.967138, mean_eps: 0.100000\n",
            "  942/2000: episode: 81, duration: 0.126s, episode steps:   9, steps per second:  72, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.059738, mae: 0.980436, mean_q: 1.938383, mean_eps: 0.100000\n",
            "  951/2000: episode: 82, duration: 0.160s, episode steps:   9, steps per second:  56, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.056148, mae: 0.990164, mean_q: 1.936223, mean_eps: 0.100000\n",
            "  961/2000: episode: 83, duration: 0.270s, episode steps:  10, steps per second:  37, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.068752, mae: 1.014810, mean_q: 1.977365, mean_eps: 0.100000\n",
            "  970/2000: episode: 84, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.057675, mae: 1.000371, mean_q: 1.995564, mean_eps: 0.100000\n",
            "  985/2000: episode: 85, duration: 0.179s, episode steps:  15, steps per second:  84, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.053875, mae: 0.983346, mean_q: 2.029036, mean_eps: 0.100000\n",
            "  994/2000: episode: 86, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.069359, mae: 1.000909, mean_q: 2.038141, mean_eps: 0.100000\n",
            " 1004/2000: episode: 87, duration: 0.118s, episode steps:  10, steps per second:  85, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.065179, mae: 0.981029, mean_q: 1.990339, mean_eps: 0.100000\n",
            " 1013/2000: episode: 88, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.072285, mae: 1.006239, mean_q: 2.053841, mean_eps: 0.100000\n",
            " 1023/2000: episode: 89, duration: 0.132s, episode steps:  10, steps per second:  75, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.061110, mae: 1.006285, mean_q: 2.071718, mean_eps: 0.100000\n",
            " 1034/2000: episode: 90, duration: 0.109s, episode steps:  11, steps per second: 101, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.072972, mae: 1.016649, mean_q: 2.033521, mean_eps: 0.100000\n",
            " 1044/2000: episode: 91, duration: 0.112s, episode steps:  10, steps per second:  90, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.088391, mae: 1.050453, mean_q: 2.036088, mean_eps: 0.100000\n",
            " 1053/2000: episode: 92, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.065185, mae: 1.033121, mean_q: 2.059102, mean_eps: 0.100000\n",
            " 1063/2000: episode: 93, duration: 0.112s, episode steps:  10, steps per second:  89, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.061518, mae: 1.027851, mean_q: 2.071561, mean_eps: 0.100000\n",
            " 1072/2000: episode: 94, duration: 0.097s, episode steps:   9, steps per second:  92, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.062455, mae: 1.026286, mean_q: 2.075280, mean_eps: 0.100000\n",
            " 1081/2000: episode: 95, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.065786, mae: 1.032938, mean_q: 2.076695, mean_eps: 0.100000\n",
            " 1090/2000: episode: 96, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.068306, mae: 1.027714, mean_q: 2.109647, mean_eps: 0.100000\n",
            " 1099/2000: episode: 97, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.066795, mae: 1.023226, mean_q: 2.080198, mean_eps: 0.100000\n",
            " 1109/2000: episode: 98, duration: 0.121s, episode steps:  10, steps per second:  83, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.079195, mae: 1.056291, mean_q: 2.096896, mean_eps: 0.100000\n",
            " 1118/2000: episode: 99, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.057522, mae: 1.044311, mean_q: 2.108727, mean_eps: 0.100000\n",
            " 1127/2000: episode: 100, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.080887, mae: 1.077038, mean_q: 2.154035, mean_eps: 0.100000\n",
            " 1139/2000: episode: 101, duration: 0.132s, episode steps:  12, steps per second:  91, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 0.085048, mae: 1.063877, mean_q: 2.090595, mean_eps: 0.100000\n",
            " 1149/2000: episode: 102, duration: 0.113s, episode steps:  10, steps per second:  89, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.077786, mae: 1.062464, mean_q: 2.097445, mean_eps: 0.100000\n",
            " 1159/2000: episode: 103, duration: 0.107s, episode steps:  10, steps per second:  93, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.075281, mae: 1.077281, mean_q: 2.153184, mean_eps: 0.100000\n",
            " 1169/2000: episode: 104, duration: 0.144s, episode steps:  10, steps per second:  70, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.074182, mae: 1.076417, mean_q: 2.184880, mean_eps: 0.100000\n",
            " 1179/2000: episode: 105, duration: 0.157s, episode steps:  10, steps per second:  64, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.062297, mae: 1.053405, mean_q: 2.132165, mean_eps: 0.100000\n",
            " 1188/2000: episode: 106, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.071948, mae: 1.064891, mean_q: 2.146926, mean_eps: 0.100000\n",
            " 1198/2000: episode: 107, duration: 0.134s, episode steps:  10, steps per second:  74, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.071355, mae: 1.076987, mean_q: 2.156087, mean_eps: 0.100000\n",
            " 1209/2000: episode: 108, duration: 0.156s, episode steps:  11, steps per second:  71, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.073175, mae: 1.102614, mean_q: 2.193606, mean_eps: 0.100000\n",
            " 1218/2000: episode: 109, duration: 0.124s, episode steps:   9, steps per second:  73, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.073225, mae: 1.103821, mean_q: 2.180230, mean_eps: 0.100000\n",
            " 1227/2000: episode: 110, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.075156, mae: 1.104439, mean_q: 2.142586, mean_eps: 0.100000\n",
            " 1238/2000: episode: 111, duration: 0.135s, episode steps:  11, steps per second:  81, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.072773, mae: 1.132774, mean_q: 2.225231, mean_eps: 0.100000\n",
            " 1248/2000: episode: 112, duration: 0.110s, episode steps:  10, steps per second:  91, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.064601, mae: 1.119233, mean_q: 2.274021, mean_eps: 0.100000\n",
            " 1256/2000: episode: 113, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.072618, mae: 1.125544, mean_q: 2.247237, mean_eps: 0.100000\n",
            " 1266/2000: episode: 114, duration: 0.102s, episode steps:  10, steps per second:  98, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.069763, mae: 1.117605, mean_q: 2.220356, mean_eps: 0.100000\n",
            " 1276/2000: episode: 115, duration: 0.116s, episode steps:  10, steps per second:  86, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.099971, mae: 1.149781, mean_q: 2.239396, mean_eps: 0.100000\n",
            " 1287/2000: episode: 116, duration: 0.116s, episode steps:  11, steps per second:  95, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.063734, mae: 1.113125, mean_q: 2.214152, mean_eps: 0.100000\n",
            " 1296/2000: episode: 117, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.070892, mae: 1.125097, mean_q: 2.296027, mean_eps: 0.100000\n",
            " 1307/2000: episode: 118, duration: 0.108s, episode steps:  11, steps per second: 102, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.060391, mae: 1.110476, mean_q: 2.287811, mean_eps: 0.100000\n",
            " 1317/2000: episode: 119, duration: 0.107s, episode steps:  10, steps per second:  93, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.069672, mae: 1.122238, mean_q: 2.281513, mean_eps: 0.100000\n",
            " 1327/2000: episode: 120, duration: 0.112s, episode steps:  10, steps per second:  89, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.062994, mae: 1.118094, mean_q: 2.286678, mean_eps: 0.100000\n",
            " 1337/2000: episode: 121, duration: 0.109s, episode steps:  10, steps per second:  92, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.062730, mae: 1.130763, mean_q: 2.281428, mean_eps: 0.100000\n",
            " 1346/2000: episode: 122, duration: 0.104s, episode steps:   9, steps per second:  86, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.077745, mae: 1.144884, mean_q: 2.293995, mean_eps: 0.100000\n",
            " 1357/2000: episode: 123, duration: 0.125s, episode steps:  11, steps per second:  88, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.070568, mae: 1.117821, mean_q: 2.314828, mean_eps: 0.100000\n",
            " 1368/2000: episode: 124, duration: 0.121s, episode steps:  11, steps per second:  91, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.083755, mae: 1.114992, mean_q: 2.279300, mean_eps: 0.100000\n",
            " 1378/2000: episode: 125, duration: 0.121s, episode steps:  10, steps per second:  82, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.079399, mae: 1.119170, mean_q: 2.309294, mean_eps: 0.100000\n",
            " 1387/2000: episode: 126, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.060131, mae: 1.116436, mean_q: 2.320564, mean_eps: 0.100000\n",
            " 1397/2000: episode: 127, duration: 0.111s, episode steps:  10, steps per second:  90, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.055726, mae: 1.142198, mean_q: 2.374446, mean_eps: 0.100000\n",
            " 1406/2000: episode: 128, duration: 0.094s, episode steps:   9, steps per second:  95, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.080401, mae: 1.190355, mean_q: 2.359465, mean_eps: 0.100000\n",
            " 1416/2000: episode: 129, duration: 0.101s, episode steps:  10, steps per second:  99, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.072460, mae: 1.191705, mean_q: 2.332379, mean_eps: 0.100000\n",
            " 1427/2000: episode: 130, duration: 0.128s, episode steps:  11, steps per second:  86, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.078928, mae: 1.219813, mean_q: 2.383879, mean_eps: 0.100000\n",
            " 1439/2000: episode: 131, duration: 0.136s, episode steps:  12, steps per second:  88, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.085229, mae: 1.199427, mean_q: 2.378426, mean_eps: 0.100000\n",
            " 1449/2000: episode: 132, duration: 0.115s, episode steps:  10, steps per second:  87, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.073519, mae: 1.163682, mean_q: 2.347500, mean_eps: 0.100000\n",
            " 1460/2000: episode: 133, duration: 0.117s, episode steps:  11, steps per second:  94, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.081649, mae: 1.176436, mean_q: 2.368178, mean_eps: 0.100000\n",
            " 1469/2000: episode: 134, duration: 0.128s, episode steps:   9, steps per second:  70, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.056357, mae: 1.169254, mean_q: 2.388495, mean_eps: 0.100000\n",
            " 1478/2000: episode: 135, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.066989, mae: 1.202736, mean_q: 2.468188, mean_eps: 0.100000\n",
            " 1488/2000: episode: 136, duration: 0.113s, episode steps:  10, steps per second:  89, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.079738, mae: 1.204797, mean_q: 2.385424, mean_eps: 0.100000\n",
            " 1496/2000: episode: 137, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.098471, mae: 1.197552, mean_q: 2.340740, mean_eps: 0.100000\n",
            " 1506/2000: episode: 138, duration: 0.132s, episode steps:  10, steps per second:  76, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.074100, mae: 1.164452, mean_q: 2.368233, mean_eps: 0.100000\n",
            " 1514/2000: episode: 139, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.074791, mae: 1.171125, mean_q: 2.469924, mean_eps: 0.100000\n",
            " 1523/2000: episode: 140, duration: 0.138s, episode steps:   9, steps per second:  65, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.072761, mae: 1.184392, mean_q: 2.470124, mean_eps: 0.100000\n",
            " 1533/2000: episode: 141, duration: 0.123s, episode steps:  10, steps per second:  81, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.070292, mae: 1.171688, mean_q: 2.440946, mean_eps: 0.100000\n",
            " 1545/2000: episode: 142, duration: 0.158s, episode steps:  12, steps per second:  76, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 0.073080, mae: 1.183256, mean_q: 2.462214, mean_eps: 0.100000\n",
            " 1556/2000: episode: 143, duration: 0.143s, episode steps:  11, steps per second:  77, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.067119, mae: 1.181012, mean_q: 2.424775, mean_eps: 0.100000\n",
            " 1567/2000: episode: 144, duration: 0.141s, episode steps:  11, steps per second:  78, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.072216, mae: 1.203358, mean_q: 2.470364, mean_eps: 0.100000\n",
            " 1577/2000: episode: 145, duration: 0.134s, episode steps:  10, steps per second:  74, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.054001, mae: 1.215782, mean_q: 2.536050, mean_eps: 0.100000\n",
            " 1586/2000: episode: 146, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.057187, mae: 1.232731, mean_q: 2.505718, mean_eps: 0.100000\n",
            " 1595/2000: episode: 147, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.083194, mae: 1.254532, mean_q: 2.450704, mean_eps: 0.100000\n",
            " 1607/2000: episode: 148, duration: 0.153s, episode steps:  12, steps per second:  78, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.070335, mae: 1.252696, mean_q: 2.523101, mean_eps: 0.100000\n",
            " 1617/2000: episode: 149, duration: 0.109s, episode steps:  10, steps per second:  92, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.063492, mae: 1.220876, mean_q: 2.478221, mean_eps: 0.100000\n",
            " 1629/2000: episode: 150, duration: 0.138s, episode steps:  12, steps per second:  87, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.078008, mae: 1.220669, mean_q: 2.434797, mean_eps: 0.100000\n",
            " 1640/2000: episode: 151, duration: 0.122s, episode steps:  11, steps per second:  90, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.081502, mae: 1.210084, mean_q: 2.469648, mean_eps: 0.100000\n",
            " 1652/2000: episode: 152, duration: 0.138s, episode steps:  12, steps per second:  87, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 0.072796, mae: 1.202318, mean_q: 2.562021, mean_eps: 0.100000\n",
            " 1661/2000: episode: 153, duration: 0.102s, episode steps:   9, steps per second:  89, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.079929, mae: 1.210058, mean_q: 2.544601, mean_eps: 0.100000\n",
            " 1670/2000: episode: 154, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.077423, mae: 1.219065, mean_q: 2.533327, mean_eps: 0.100000\n",
            " 1680/2000: episode: 155, duration: 0.125s, episode steps:  10, steps per second:  80, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.069638, mae: 1.221479, mean_q: 2.571885, mean_eps: 0.100000\n",
            " 1690/2000: episode: 156, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.073059, mae: 1.222700, mean_q: 2.548242, mean_eps: 0.100000\n",
            " 1699/2000: episode: 157, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.067588, mae: 1.239883, mean_q: 2.575134, mean_eps: 0.100000\n",
            " 1709/2000: episode: 158, duration: 0.110s, episode steps:  10, steps per second:  91, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.078575, mae: 1.244549, mean_q: 2.536145, mean_eps: 0.100000\n",
            " 1719/2000: episode: 159, duration: 0.109s, episode steps:  10, steps per second:  92, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.073085, mae: 1.241046, mean_q: 2.543751, mean_eps: 0.100000\n",
            " 1729/2000: episode: 160, duration: 0.110s, episode steps:  10, steps per second:  91, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.074658, mae: 1.240274, mean_q: 2.592864, mean_eps: 0.100000\n",
            " 1739/2000: episode: 161, duration: 0.111s, episode steps:  10, steps per second:  90, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.066782, mae: 1.238643, mean_q: 2.586403, mean_eps: 0.100000\n",
            " 1749/2000: episode: 162, duration: 0.125s, episode steps:  10, steps per second:  80, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.062890, mae: 1.248923, mean_q: 2.601792, mean_eps: 0.100000\n",
            " 1760/2000: episode: 163, duration: 0.132s, episode steps:  11, steps per second:  84, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.094731, mae: 1.283918, mean_q: 2.560599, mean_eps: 0.100000\n",
            " 1771/2000: episode: 164, duration: 0.122s, episode steps:  11, steps per second:  90, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.079410, mae: 1.282449, mean_q: 2.586584, mean_eps: 0.100000\n",
            " 1781/2000: episode: 165, duration: 0.125s, episode steps:  10, steps per second:  80, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.077781, mae: 1.277302, mean_q: 2.634282, mean_eps: 0.100000\n",
            " 1790/2000: episode: 166, duration: 0.116s, episode steps:   9, steps per second:  77, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.090916, mae: 1.268158, mean_q: 2.528234, mean_eps: 0.100000\n",
            " 1798/2000: episode: 167, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.069808, mae: 1.263274, mean_q: 2.615279, mean_eps: 0.100000\n",
            " 1808/2000: episode: 168, duration: 0.120s, episode steps:  10, steps per second:  83, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.066494, mae: 1.289319, mean_q: 2.637583, mean_eps: 0.100000\n",
            " 1821/2000: episode: 169, duration: 0.134s, episode steps:  13, steps per second:  97, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.074929, mae: 1.326996, mean_q: 2.664152, mean_eps: 0.100000\n",
            " 1830/2000: episode: 170, duration: 0.103s, episode steps:   9, steps per second:  87, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.072117, mae: 1.329842, mean_q: 2.646011, mean_eps: 0.100000\n",
            " 1840/2000: episode: 171, duration: 0.114s, episode steps:  10, steps per second:  88, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.057904, mae: 1.313362, mean_q: 2.645648, mean_eps: 0.100000\n",
            " 1850/2000: episode: 172, duration: 0.105s, episode steps:  10, steps per second:  95, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.061503, mae: 1.315294, mean_q: 2.718679, mean_eps: 0.100000\n",
            " 1860/2000: episode: 173, duration: 0.110s, episode steps:  10, steps per second:  91, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.073967, mae: 1.304298, mean_q: 2.679675, mean_eps: 0.100000\n",
            " 1870/2000: episode: 174, duration: 0.118s, episode steps:  10, steps per second:  85, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.064351, mae: 1.294922, mean_q: 2.690798, mean_eps: 0.100000\n",
            " 1879/2000: episode: 175, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.084622, mae: 1.300458, mean_q: 2.666587, mean_eps: 0.100000\n",
            " 1888/2000: episode: 176, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.060977, mae: 1.295121, mean_q: 2.687773, mean_eps: 0.100000\n",
            " 1898/2000: episode: 177, duration: 0.110s, episode steps:  10, steps per second:  91, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.071424, mae: 1.336748, mean_q: 2.748167, mean_eps: 0.100000\n",
            " 1910/2000: episode: 178, duration: 0.134s, episode steps:  12, steps per second:  90, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 0.080388, mae: 1.327986, mean_q: 2.708852, mean_eps: 0.100000\n",
            " 1920/2000: episode: 179, duration: 0.106s, episode steps:  10, steps per second:  94, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.082247, mae: 1.312371, mean_q: 2.645403, mean_eps: 0.100000\n",
            " 1928/2000: episode: 180, duration: 0.081s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.092805, mae: 1.317967, mean_q: 2.706488, mean_eps: 0.100000\n",
            " 1940/2000: episode: 181, duration: 0.148s, episode steps:  12, steps per second:  81, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 0.076141, mae: 1.310937, mean_q: 2.765453, mean_eps: 0.100000\n",
            " 1949/2000: episode: 182, duration: 0.149s, episode steps:   9, steps per second:  61, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.068259, mae: 1.296052, mean_q: 2.710653, mean_eps: 0.100000\n",
            " 1959/2000: episode: 183, duration: 0.133s, episode steps:  10, steps per second:  75, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.070425, mae: 1.314251, mean_q: 2.734577, mean_eps: 0.100000\n",
            " 1968/2000: episode: 184, duration: 0.116s, episode steps:   9, steps per second:  78, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.077352, mae: 1.323276, mean_q: 2.719262, mean_eps: 0.100000\n",
            " 1977/2000: episode: 185, duration: 0.130s, episode steps:   9, steps per second:  69, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.062370, mae: 1.321855, mean_q: 2.783092, mean_eps: 0.100000\n",
            " 1987/2000: episode: 186, duration: 0.127s, episode steps:  10, steps per second:  79, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.094925, mae: 1.313790, mean_q: 2.732464, mean_eps: 0.100000\n",
            " 1997/2000: episode: 187, duration: 0.135s, episode steps:  10, steps per second:  74, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.063593, mae: 1.285848, mean_q: 2.761364, mean_eps: 0.100000\n",
            "done, took 31.090 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 10.000, steps: 10\n",
            "Episode 2: reward: 9.000, steps: 9\n",
            "Episode 3: reward: 10.000, steps: 10\n",
            "Episode 4: reward: 10.000, steps: 10\n",
            "Episode 5: reward: 9.000, steps: 9\n",
            "Episode 6: reward: 8.000, steps: 8\n",
            "Episode 7: reward: 9.000, steps: 9\n",
            "Episode 8: reward: 9.000, steps: 9\n",
            "Episode 9: reward: 10.000, steps: 10\n",
            "Episode 10: reward: 10.000, steps: 10\n",
            "Episode 11: reward: 10.000, steps: 10\n",
            "Episode 12: reward: 10.000, steps: 10\n",
            "Episode 13: reward: 10.000, steps: 10\n",
            "Episode 14: reward: 10.000, steps: 10\n",
            "Episode 15: reward: 9.000, steps: 9\n",
            "Episode 16: reward: 11.000, steps: 11\n",
            "Episode 17: reward: 9.000, steps: 9\n",
            "Episode 18: reward: 8.000, steps: 8\n",
            "Episode 19: reward: 10.000, steps: 10\n",
            "Episode 20: reward: 9.000, steps: 9\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb1732e7110>"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    }
  ]
}