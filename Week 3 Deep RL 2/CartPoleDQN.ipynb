{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPoleDQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Megacity1/CartpoleDQN/blob/main/Week%203%20Deep%20RL%202/CartPoleDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Deep Q Network (DQN) for CartPole Using Boltzmann Q Policy\n",
        "This exercise implements a DQN for CartPole using a Boltzmann Q policy for selecting the actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGsC7cJ5jNcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda73cbf-3020-40f4-e45e-7b87053448f1"
      },
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.24.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (13.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.44.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.10.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMIHLgQ3Z-lF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5a8608-456f-4127-ce8a-f551e5f4e4f3"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0AMLzq08ap0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b380e1-6c3d-4bce-8233-a8ac01deaf26"
      },
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "!pip install Adam\n",
        "#from keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Adam in /usr/local/lib/python3.7/dist-packages (0.0.0.dev0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll6bNdUm54WS"
      },
      "source": [
        "Implementation of DQN for CartPole, applying policy BoltzmannQPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCrPKNy40PC"
      },
      "source": [
        "##Implement DQN with BoltzmannGumbelQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efM9jkXr5A3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3e40f8d2-f66f-4a46-ce31-ef364c77ef15"
      },
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import BoltzmannGumbelQPolicy\n",
        "from rl.policy import LinearAnnealedPolicy\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n",
        "# setup experience replay buffer\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
        "                               attr='eps',            \n",
        "                               value_max=5.,\n",
        "                               value_min=.5, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=200)\n",
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "# add extra layers here\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=30,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy) \n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=8000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_55\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_53 (Flatten)        (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_107 (Dense)           (None, 16)                80        \n",
            "                                                                 \n",
            " dense_108 (Dense)           (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 8000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   36/8000: episode: 1, duration: 12.859s, episode steps:  36, steps per second:   3, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.533167, mae: 0.556622, mean_q: 0.148141, mean_eps: 4.257500\n",
            "   85/8000: episode: 2, duration: 0.829s, episode steps:  49, steps per second:  59, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 0.449545, mae: 0.561929, mean_q: 0.282393, mean_eps: 3.650000\n",
            "  107/8000: episode: 3, duration: 0.430s, episode steps:  22, steps per second:  51, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.364422, mae: 0.598397, mean_q: 0.501945, mean_eps: 2.851250\n",
            "  116/8000: episode: 4, duration: 0.232s, episode steps:   9, steps per second:  39, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.356177, mae: 0.628138, mean_q: 0.579221, mean_eps: 2.502500\n",
            "  183/8000: episode: 5, duration: 1.362s, episode steps:  67, steps per second:  49, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.433 [0.000, 1.000],  loss: 0.275763, mae: 0.699172, mean_q: 0.856956, mean_eps: 1.647500\n",
            "  248/8000: episode: 6, duration: 1.259s, episode steps:  65, steps per second:  52, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.585 [0.000, 1.000],  loss: 0.156348, mae: 0.853425, mean_q: 1.350854, mean_eps: 0.552962\n",
            "  262/8000: episode: 7, duration: 0.311s, episode steps:  14, steps per second:  45, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.093885, mae: 0.995278, mean_q: 1.747085, mean_eps: 0.500000\n",
            "  273/8000: episode: 8, duration: 0.372s, episode steps:  11, steps per second:  30, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.127364, mae: 1.103091, mean_q: 1.967198, mean_eps: 0.500000\n",
            "  294/8000: episode: 9, duration: 0.699s, episode steps:  21, steps per second:  30, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.170392, mae: 1.168560, mean_q: 2.067532, mean_eps: 0.500000\n",
            "  322/8000: episode: 10, duration: 0.719s, episode steps:  28, steps per second:  39, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.125915, mae: 1.251320, mean_q: 2.241545, mean_eps: 0.500000\n",
            "  340/8000: episode: 11, duration: 0.494s, episode steps:  18, steps per second:  36, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.226369, mae: 1.382976, mean_q: 2.489927, mean_eps: 0.500000\n",
            "  351/8000: episode: 12, duration: 0.350s, episode steps:  11, steps per second:  31, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.175927, mae: 1.433438, mean_q: 2.545358, mean_eps: 0.500000\n",
            "  377/8000: episode: 13, duration: 0.768s, episode steps:  26, steps per second:  34, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.654 [0.000, 1.000],  loss: 0.244263, mae: 1.535992, mean_q: 2.769549, mean_eps: 0.500000\n",
            "  398/8000: episode: 14, duration: 0.632s, episode steps:  21, steps per second:  33, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.271328, mae: 1.629988, mean_q: 2.896625, mean_eps: 0.500000\n",
            "  408/8000: episode: 15, duration: 0.215s, episode steps:  10, steps per second:  47, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.246845, mae: 1.681353, mean_q: 3.034766, mean_eps: 0.500000\n",
            "  418/8000: episode: 16, duration: 0.192s, episode steps:  10, steps per second:  52, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.214665, mae: 1.731015, mean_q: 3.169005, mean_eps: 0.500000\n",
            "  434/8000: episode: 17, duration: 0.288s, episode steps:  16, steps per second:  56, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.369214, mae: 1.836113, mean_q: 3.373122, mean_eps: 0.500000\n",
            "  445/8000: episode: 18, duration: 0.187s, episode steps:  11, steps per second:  59, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.373447, mae: 1.857582, mean_q: 3.438453, mean_eps: 0.500000\n",
            "  456/8000: episode: 19, duration: 0.217s, episode steps:  11, steps per second:  51, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.455865, mae: 1.960654, mean_q: 3.545756, mean_eps: 0.500000\n",
            "  476/8000: episode: 20, duration: 0.599s, episode steps:  20, steps per second:  33, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.650 [0.000, 1.000],  loss: 0.315772, mae: 1.960204, mean_q: 3.664543, mean_eps: 0.500000\n",
            "  492/8000: episode: 21, duration: 0.440s, episode steps:  16, steps per second:  36, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.402091, mae: 2.058286, mean_q: 3.880437, mean_eps: 0.500000\n",
            "  504/8000: episode: 22, duration: 0.251s, episode steps:  12, steps per second:  48, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.487585, mae: 2.170075, mean_q: 4.061069, mean_eps: 0.500000\n",
            "  513/8000: episode: 23, duration: 0.198s, episode steps:   9, steps per second:  45, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.502751, mae: 2.190351, mean_q: 4.067813, mean_eps: 0.500000\n",
            "  522/8000: episode: 24, duration: 0.185s, episode steps:   9, steps per second:  49, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.492293, mae: 2.210471, mean_q: 4.085456, mean_eps: 0.500000\n",
            "  532/8000: episode: 25, duration: 0.234s, episode steps:  10, steps per second:  43, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.584282, mae: 2.299456, mean_q: 4.224824, mean_eps: 0.500000\n",
            "  543/8000: episode: 26, duration: 0.261s, episode steps:  11, steps per second:  42, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.569720, mae: 2.325725, mean_q: 4.262701, mean_eps: 0.500000\n",
            "  553/8000: episode: 27, duration: 0.235s, episode steps:  10, steps per second:  43, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.587632, mae: 2.389676, mean_q: 4.367578, mean_eps: 0.500000\n",
            "  569/8000: episode: 28, duration: 0.322s, episode steps:  16, steps per second:  50, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.302388, mae: 2.322036, mean_q: 4.421576, mean_eps: 0.500000\n",
            "  585/8000: episode: 29, duration: 0.364s, episode steps:  16, steps per second:  44, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.550060, mae: 2.460370, mean_q: 4.568972, mean_eps: 0.500000\n",
            "  613/8000: episode: 30, duration: 0.902s, episode steps:  28, steps per second:  31, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.607 [0.000, 1.000],  loss: 0.462100, mae: 2.536538, mean_q: 4.733906, mean_eps: 0.500000\n",
            "  625/8000: episode: 31, duration: 0.331s, episode steps:  12, steps per second:  36, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.461006, mae: 2.588302, mean_q: 4.840745, mean_eps: 0.500000\n",
            "  636/8000: episode: 32, duration: 0.281s, episode steps:  11, steps per second:  39, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.407368, mae: 2.635503, mean_q: 4.945487, mean_eps: 0.500000\n",
            "  648/8000: episode: 33, duration: 0.268s, episode steps:  12, steps per second:  45, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.483335, mae: 2.702147, mean_q: 5.033446, mean_eps: 0.500000\n",
            "  661/8000: episode: 34, duration: 0.282s, episode steps:  13, steps per second:  46, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.485720, mae: 2.742576, mean_q: 5.109032, mean_eps: 0.500000\n",
            "  675/8000: episode: 35, duration: 0.249s, episode steps:  14, steps per second:  56, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.598822, mae: 2.813093, mean_q: 5.176352, mean_eps: 0.500000\n",
            "  707/8000: episode: 36, duration: 0.545s, episode steps:  32, steps per second:  59, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.656 [0.000, 1.000],  loss: 0.489388, mae: 2.848670, mean_q: 5.264208, mean_eps: 0.500000\n",
            "  719/8000: episode: 37, duration: 0.228s, episode steps:  12, steps per second:  53, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.584734, mae: 2.954712, mean_q: 5.548125, mean_eps: 0.500000\n",
            "  732/8000: episode: 38, duration: 0.267s, episode steps:  13, steps per second:  49, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.666581, mae: 3.016532, mean_q: 5.557186, mean_eps: 0.500000\n",
            "  745/8000: episode: 39, duration: 0.242s, episode steps:  13, steps per second:  54, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.527284, mae: 3.015315, mean_q: 5.553894, mean_eps: 0.500000\n",
            "  763/8000: episode: 40, duration: 0.328s, episode steps:  18, steps per second:  55, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.479758, mae: 3.068208, mean_q: 5.785563, mean_eps: 0.500000\n",
            "  774/8000: episode: 41, duration: 0.239s, episode steps:  11, steps per second:  46, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.668144, mae: 3.157973, mean_q: 5.849444, mean_eps: 0.500000\n",
            "  797/8000: episode: 42, duration: 0.431s, episode steps:  23, steps per second:  53, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.633888, mae: 3.207967, mean_q: 5.876170, mean_eps: 0.500000\n",
            "  811/8000: episode: 43, duration: 0.290s, episode steps:  14, steps per second:  48, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.446398, mae: 3.205365, mean_q: 5.940380, mean_eps: 0.500000\n",
            "  824/8000: episode: 44, duration: 0.256s, episode steps:  13, steps per second:  51, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.591979, mae: 3.266258, mean_q: 6.092705, mean_eps: 0.500000\n",
            "  854/8000: episode: 45, duration: 0.636s, episode steps:  30, steps per second:  47, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.529508, mae: 3.334579, mean_q: 6.232388, mean_eps: 0.500000\n",
            "  868/8000: episode: 46, duration: 0.315s, episode steps:  14, steps per second:  44, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.214 [0.000, 1.000],  loss: 0.606311, mae: 3.419138, mean_q: 6.399127, mean_eps: 0.500000\n",
            "  887/8000: episode: 47, duration: 0.378s, episode steps:  19, steps per second:  50, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.483111, mae: 3.446107, mean_q: 6.467597, mean_eps: 0.500000\n",
            "  904/8000: episode: 48, duration: 0.393s, episode steps:  17, steps per second:  43, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.613729, mae: 3.538035, mean_q: 6.630833, mean_eps: 0.500000\n",
            "  919/8000: episode: 49, duration: 0.409s, episode steps:  15, steps per second:  37, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.482619, mae: 3.559254, mean_q: 6.640159, mean_eps: 0.500000\n",
            "  951/8000: episode: 50, duration: 0.720s, episode steps:  32, steps per second:  44, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 0.721050, mae: 3.637493, mean_q: 6.767022, mean_eps: 0.500000\n",
            "  961/8000: episode: 51, duration: 0.245s, episode steps:  10, steps per second:  41, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.464755, mae: 3.683719, mean_q: 6.914688, mean_eps: 0.500000\n",
            "  972/8000: episode: 52, duration: 0.237s, episode steps:  11, steps per second:  46, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.585706, mae: 3.704355, mean_q: 6.916563, mean_eps: 0.500000\n",
            "  983/8000: episode: 53, duration: 0.271s, episode steps:  11, steps per second:  41, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.958419, mae: 3.830193, mean_q: 7.137972, mean_eps: 0.500000\n",
            "  996/8000: episode: 54, duration: 0.293s, episode steps:  13, steps per second:  44, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.875521, mae: 3.859108, mean_q: 7.183690, mean_eps: 0.500000\n",
            " 1007/8000: episode: 55, duration: 0.275s, episode steps:  11, steps per second:  40, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.730618, mae: 3.825368, mean_q: 7.143084, mean_eps: 0.500000\n",
            " 1018/8000: episode: 56, duration: 0.266s, episode steps:  11, steps per second:  41, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.654591, mae: 3.868272, mean_q: 7.250121, mean_eps: 0.500000\n",
            " 1036/8000: episode: 57, duration: 0.416s, episode steps:  18, steps per second:  43, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.944104, mae: 3.951593, mean_q: 7.363127, mean_eps: 0.500000\n",
            " 1047/8000: episode: 58, duration: 0.292s, episode steps:  11, steps per second:  38, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.560583, mae: 3.980881, mean_q: 7.534146, mean_eps: 0.500000\n",
            " 1056/8000: episode: 59, duration: 0.292s, episode steps:   9, steps per second:  31, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.763189, mae: 4.058436, mean_q: 7.707528, mean_eps: 0.500000\n",
            " 1069/8000: episode: 60, duration: 0.421s, episode steps:  13, steps per second:  31, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 1.357740, mae: 4.148205, mean_q: 7.716340, mean_eps: 0.500000\n",
            " 1081/8000: episode: 61, duration: 0.346s, episode steps:  12, steps per second:  35, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.792348, mae: 4.133257, mean_q: 7.698972, mean_eps: 0.500000\n",
            " 1092/8000: episode: 62, duration: 0.273s, episode steps:  11, steps per second:  40, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 1.084137, mae: 4.199800, mean_q: 7.792670, mean_eps: 0.500000\n",
            " 1105/8000: episode: 63, duration: 0.315s, episode steps:  13, steps per second:  41, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.154 [0.000, 1.000],  loss: 0.762950, mae: 4.193131, mean_q: 7.881833, mean_eps: 0.500000\n",
            " 1115/8000: episode: 64, duration: 0.267s, episode steps:  10, steps per second:  37, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.895885, mae: 4.225195, mean_q: 7.925411, mean_eps: 0.500000\n",
            " 1148/8000: episode: 65, duration: 0.744s, episode steps:  33, steps per second:  44, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.394 [0.000, 1.000],  loss: 1.065864, mae: 4.339847, mean_q: 8.114427, mean_eps: 0.500000\n",
            " 1167/8000: episode: 66, duration: 0.467s, episode steps:  19, steps per second:  41, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.211 [0.000, 1.000],  loss: 0.834029, mae: 4.367146, mean_q: 8.206326, mean_eps: 0.500000\n",
            " 1179/8000: episode: 67, duration: 0.293s, episode steps:  12, steps per second:  41, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 1.418703, mae: 4.503210, mean_q: 8.379488, mean_eps: 0.500000\n",
            " 1269/8000: episode: 68, duration: 1.997s, episode steps:  90, steps per second:  45, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 1.419034, mae: 4.636864, mean_q: 8.534160, mean_eps: 0.500000\n",
            " 1328/8000: episode: 69, duration: 1.275s, episode steps:  59, steps per second:  46, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 1.183996, mae: 4.777422, mean_q: 8.957381, mean_eps: 0.500000\n",
            " 1361/8000: episode: 70, duration: 0.720s, episode steps:  33, steps per second:  46, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 1.464389, mae: 4.943548, mean_q: 9.221769, mean_eps: 0.500000\n",
            " 1395/8000: episode: 71, duration: 0.701s, episode steps:  34, steps per second:  48, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1.558713, mae: 5.020938, mean_q: 9.312752, mean_eps: 0.500000\n",
            " 1435/8000: episode: 72, duration: 0.960s, episode steps:  40, steps per second:  42, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 1.277263, mae: 5.101091, mean_q: 9.580351, mean_eps: 0.500000\n",
            " 1495/8000: episode: 73, duration: 1.373s, episode steps:  60, steps per second:  44, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 1.597753, mae: 5.264601, mean_q: 9.823373, mean_eps: 0.500000\n",
            " 1535/8000: episode: 74, duration: 0.871s, episode steps:  40, steps per second:  46, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 1.257946, mae: 5.336779, mean_q: 10.033846, mean_eps: 0.500000\n",
            " 1561/8000: episode: 75, duration: 0.616s, episode steps:  26, steps per second:  42, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1.803713, mae: 5.491158, mean_q: 10.257844, mean_eps: 0.500000\n",
            " 1581/8000: episode: 76, duration: 0.402s, episode steps:  20, steps per second:  50, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 1.156197, mae: 5.457502, mean_q: 10.268869, mean_eps: 0.500000\n",
            " 1595/8000: episode: 77, duration: 0.306s, episode steps:  14, steps per second:  46, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.884652, mae: 5.489854, mean_q: 10.532388, mean_eps: 0.500000\n",
            " 1636/8000: episode: 78, duration: 0.806s, episode steps:  41, steps per second:  51, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 1.538632, mae: 5.629265, mean_q: 10.670196, mean_eps: 0.500000\n",
            " 1666/8000: episode: 79, duration: 0.622s, episode steps:  30, steps per second:  48, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.836780, mae: 5.735989, mean_q: 10.749130, mean_eps: 0.500000\n",
            " 1700/8000: episode: 80, duration: 0.697s, episode steps:  34, steps per second:  49, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1.482857, mae: 5.794855, mean_q: 10.971331, mean_eps: 0.500000\n",
            " 1728/8000: episode: 81, duration: 0.500s, episode steps:  28, steps per second:  56, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.160308, mae: 5.927141, mean_q: 10.992260, mean_eps: 0.500000\n",
            " 1744/8000: episode: 82, duration: 0.301s, episode steps:  16, steps per second:  53, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 1.369097, mae: 5.877376, mean_q: 11.184710, mean_eps: 0.500000\n",
            " 1802/8000: episode: 83, duration: 1.085s, episode steps:  58, steps per second:  53, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.039930, mae: 6.053706, mean_q: 11.398656, mean_eps: 0.500000\n",
            " 1818/8000: episode: 84, duration: 0.312s, episode steps:  16, steps per second:  51, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.028386, mae: 6.072263, mean_q: 11.567798, mean_eps: 0.500000\n",
            " 1842/8000: episode: 85, duration: 0.467s, episode steps:  24, steps per second:  51, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.876681, mae: 6.187226, mean_q: 11.699651, mean_eps: 0.500000\n",
            " 1873/8000: episode: 86, duration: 0.690s, episode steps:  31, steps per second:  45, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 2.288392, mae: 6.247335, mean_q: 11.693668, mean_eps: 0.500000\n",
            " 1895/8000: episode: 87, duration: 0.545s, episode steps:  22, steps per second:  40, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.453975, mae: 6.263996, mean_q: 11.631496, mean_eps: 0.500000\n",
            " 1910/8000: episode: 88, duration: 0.401s, episode steps:  15, steps per second:  37, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 1.676440, mae: 6.238875, mean_q: 11.704143, mean_eps: 0.500000\n",
            " 1925/8000: episode: 89, duration: 0.354s, episode steps:  15, steps per second:  42, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.693880, mae: 6.282206, mean_q: 11.913796, mean_eps: 0.500000\n",
            " 1952/8000: episode: 90, duration: 0.630s, episode steps:  27, steps per second:  43, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.521877, mae: 6.323218, mean_q: 12.027935, mean_eps: 0.500000\n",
            " 2004/8000: episode: 91, duration: 1.120s, episode steps:  52, steps per second:  46, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.855853, mae: 6.435714, mean_q: 12.184684, mean_eps: 0.500000\n",
            " 2055/8000: episode: 92, duration: 1.011s, episode steps:  51, steps per second:  50, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 2.390594, mae: 6.560805, mean_q: 12.344900, mean_eps: 0.500000\n",
            " 2099/8000: episode: 93, duration: 0.851s, episode steps:  44, steps per second:  52, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.947458, mae: 6.637316, mean_q: 12.617237, mean_eps: 0.500000\n",
            " 2154/8000: episode: 94, duration: 1.046s, episode steps:  55, steps per second:  53, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 2.075747, mae: 6.819285, mean_q: 12.965859, mean_eps: 0.500000\n",
            " 2189/8000: episode: 95, duration: 0.634s, episode steps:  35, steps per second:  55, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 1.652696, mae: 6.853497, mean_q: 13.105813, mean_eps: 0.500000\n",
            " 2210/8000: episode: 96, duration: 0.414s, episode steps:  21, steps per second:  51, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 3.299936, mae: 6.976444, mean_q: 13.053152, mean_eps: 0.500000\n",
            " 2255/8000: episode: 97, duration: 0.886s, episode steps:  45, steps per second:  51, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 1.795497, mae: 6.958024, mean_q: 13.276861, mean_eps: 0.500000\n",
            " 2298/8000: episode: 98, duration: 0.841s, episode steps:  43, steps per second:  51, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 1.915458, mae: 7.062802, mean_q: 13.516701, mean_eps: 0.500000\n",
            " 2394/8000: episode: 99, duration: 2.027s, episode steps:  96, steps per second:  47, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 2.281831, mae: 7.255099, mean_q: 13.820974, mean_eps: 0.500000\n",
            " 2477/8000: episode: 100, duration: 1.641s, episode steps:  83, steps per second:  51, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 2.699169, mae: 7.468161, mean_q: 14.160881, mean_eps: 0.500000\n",
            " 2549/8000: episode: 101, duration: 1.447s, episode steps:  72, steps per second:  50, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 2.149877, mae: 7.647142, mean_q: 14.690967, mean_eps: 0.500000\n",
            " 2631/8000: episode: 102, duration: 1.600s, episode steps:  82, steps per second:  51, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.817702, mae: 7.826090, mean_q: 14.907621, mean_eps: 0.500000\n",
            " 2682/8000: episode: 103, duration: 0.956s, episode steps:  51, steps per second:  53, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 2.577284, mae: 7.985502, mean_q: 15.310966, mean_eps: 0.500000\n",
            " 2777/8000: episode: 104, duration: 1.653s, episode steps:  95, steps per second:  57, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 2.608564, mae: 8.120883, mean_q: 15.562902, mean_eps: 0.500000\n",
            " 2823/8000: episode: 105, duration: 0.853s, episode steps:  46, steps per second:  54, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 2.647443, mae: 8.315033, mean_q: 15.957678, mean_eps: 0.500000\n",
            " 2885/8000: episode: 106, duration: 1.149s, episode steps:  62, steps per second:  54, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.592256, mae: 8.414756, mean_q: 16.143448, mean_eps: 0.500000\n",
            " 2921/8000: episode: 107, duration: 0.738s, episode steps:  36, steps per second:  49, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 3.464376, mae: 8.538778, mean_q: 16.269796, mean_eps: 0.500000\n",
            " 2951/8000: episode: 108, duration: 0.913s, episode steps:  30, steps per second:  33, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.232439, mae: 8.520998, mean_q: 16.209377, mean_eps: 0.500000\n",
            " 3012/8000: episode: 109, duration: 1.444s, episode steps:  61, steps per second:  42, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 2.133694, mae: 8.652885, mean_q: 16.717445, mean_eps: 0.500000\n",
            " 3056/8000: episode: 110, duration: 0.897s, episode steps:  44, steps per second:  49, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 3.050386, mae: 8.796240, mean_q: 16.873683, mean_eps: 0.500000\n",
            " 3108/8000: episode: 111, duration: 1.040s, episode steps:  52, steps per second:  50, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 2.888933, mae: 8.858422, mean_q: 17.080629, mean_eps: 0.500000\n",
            " 3135/8000: episode: 112, duration: 0.874s, episode steps:  27, steps per second:  31, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 2.761742, mae: 8.947955, mean_q: 17.119307, mean_eps: 0.500000\n",
            " 3184/8000: episode: 113, duration: 1.328s, episode steps:  49, steps per second:  37, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 3.730691, mae: 9.043612, mean_q: 17.270448, mean_eps: 0.500000\n",
            " 3223/8000: episode: 114, duration: 0.887s, episode steps:  39, steps per second:  44, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 2.172652, mae: 9.103868, mean_q: 17.580908, mean_eps: 0.500000\n",
            " 3284/8000: episode: 115, duration: 1.323s, episode steps:  61, steps per second:  46, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 2.913380, mae: 9.201425, mean_q: 17.654838, mean_eps: 0.500000\n",
            " 3335/8000: episode: 116, duration: 1.124s, episode steps:  51, steps per second:  45, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 3.640767, mae: 9.310099, mean_q: 17.813826, mean_eps: 0.500000\n",
            " 3370/8000: episode: 117, duration: 0.793s, episode steps:  35, steps per second:  44, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 3.243044, mae: 9.369729, mean_q: 18.030039, mean_eps: 0.500000\n",
            " 3399/8000: episode: 118, duration: 0.641s, episode steps:  29, steps per second:  45, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 4.248730, mae: 9.511704, mean_q: 18.146157, mean_eps: 0.500000\n",
            " 3472/8000: episode: 119, duration: 1.491s, episode steps:  73, steps per second:  49, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 3.325451, mae: 9.508539, mean_q: 18.330194, mean_eps: 0.500000\n",
            " 3519/8000: episode: 120, duration: 0.981s, episode steps:  47, steps per second:  48, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 3.641146, mae: 9.631729, mean_q: 18.527091, mean_eps: 0.500000\n",
            " 3547/8000: episode: 121, duration: 0.511s, episode steps:  28, steps per second:  55, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 3.123011, mae: 9.644012, mean_q: 18.645625, mean_eps: 0.500000\n",
            " 3565/8000: episode: 122, duration: 0.333s, episode steps:  18, steps per second:  54, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 3.696972, mae: 9.750892, mean_q: 18.698781, mean_eps: 0.500000\n",
            " 3592/8000: episode: 123, duration: 0.496s, episode steps:  27, steps per second:  54, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 2.966147, mae: 9.687656, mean_q: 18.722454, mean_eps: 0.500000\n",
            " 3636/8000: episode: 124, duration: 0.827s, episode steps:  44, steps per second:  53, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 3.088211, mae: 9.833158, mean_q: 19.030488, mean_eps: 0.500000\n",
            " 3661/8000: episode: 125, duration: 0.523s, episode steps:  25, steps per second:  48, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 4.433709, mae: 9.895482, mean_q: 18.896196, mean_eps: 0.500000\n",
            " 3695/8000: episode: 126, duration: 0.732s, episode steps:  34, steps per second:  46, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 2.792616, mae: 9.905695, mean_q: 19.128963, mean_eps: 0.500000\n",
            " 3748/8000: episode: 127, duration: 1.061s, episode steps:  53, steps per second:  50, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 3.239740, mae: 10.095448, mean_q: 19.505465, mean_eps: 0.500000\n",
            " 3784/8000: episode: 128, duration: 0.708s, episode steps:  36, steps per second:  51, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.427242, mae: 10.168526, mean_q: 19.463785, mean_eps: 0.500000\n",
            " 3831/8000: episode: 129, duration: 1.004s, episode steps:  47, steps per second:  47, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 2.824829, mae: 10.172455, mean_q: 19.750283, mean_eps: 0.500000\n",
            " 3895/8000: episode: 130, duration: 1.264s, episode steps:  64, steps per second:  51, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 4.067539, mae: 10.292989, mean_q: 19.772812, mean_eps: 0.500000\n",
            " 3964/8000: episode: 131, duration: 1.327s, episode steps:  69, steps per second:  52, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 4.212680, mae: 10.464977, mean_q: 20.082483, mean_eps: 0.500000\n",
            " 4013/8000: episode: 132, duration: 1.112s, episode steps:  49, steps per second:  44, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 4.201555, mae: 10.450067, mean_q: 20.057428, mean_eps: 0.500000\n",
            " 4056/8000: episode: 133, duration: 1.070s, episode steps:  43, steps per second:  40, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 4.099949, mae: 10.530837, mean_q: 20.332391, mean_eps: 0.500000\n",
            " 4075/8000: episode: 134, duration: 0.435s, episode steps:  19, steps per second:  44, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 3.799411, mae: 10.622516, mean_q: 20.610849, mean_eps: 0.500000\n",
            " 4188/8000: episode: 135, duration: 2.427s, episode steps: 113, steps per second:  47, episode reward: 113.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 4.400119, mae: 10.700891, mean_q: 20.625192, mean_eps: 0.500000\n",
            " 4223/8000: episode: 136, duration: 0.743s, episode steps:  35, steps per second:  47, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 4.769608, mae: 10.905515, mean_q: 21.082392, mean_eps: 0.500000\n",
            " 4301/8000: episode: 137, duration: 1.630s, episode steps:  78, steps per second:  48, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 3.943822, mae: 10.922469, mean_q: 21.083606, mean_eps: 0.500000\n",
            " 4351/8000: episode: 138, duration: 1.053s, episode steps:  50, steps per second:  47, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 4.531981, mae: 10.996434, mean_q: 21.186410, mean_eps: 0.500000\n",
            " 4401/8000: episode: 139, duration: 1.060s, episode steps:  50, steps per second:  47, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 5.181039, mae: 11.022792, mean_q: 21.188550, mean_eps: 0.500000\n",
            " 4449/8000: episode: 140, duration: 0.940s, episode steps:  48, steps per second:  51, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.708370, mae: 11.058270, mean_q: 21.453692, mean_eps: 0.500000\n",
            " 4507/8000: episode: 141, duration: 1.183s, episode steps:  58, steps per second:  49, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 3.571423, mae: 11.209381, mean_q: 21.780271, mean_eps: 0.500000\n",
            " 4539/8000: episode: 142, duration: 0.633s, episode steps:  32, steps per second:  51, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 5.189611, mae: 11.396067, mean_q: 21.971896, mean_eps: 0.500000\n",
            " 4566/8000: episode: 143, duration: 0.527s, episode steps:  27, steps per second:  51, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 4.152800, mae: 11.350740, mean_q: 22.011679, mean_eps: 0.500000\n",
            " 4641/8000: episode: 144, duration: 1.521s, episode steps:  75, steps per second:  49, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 3.840947, mae: 11.401586, mean_q: 22.119044, mean_eps: 0.500000\n",
            " 4676/8000: episode: 145, duration: 0.686s, episode steps:  35, steps per second:  51, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 3.534036, mae: 11.592247, mean_q: 22.564814, mean_eps: 0.500000\n",
            " 4694/8000: episode: 146, duration: 0.386s, episode steps:  18, steps per second:  47, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 6.338056, mae: 11.652882, mean_q: 22.440750, mean_eps: 0.500000\n",
            " 4758/8000: episode: 147, duration: 1.291s, episode steps:  64, steps per second:  50, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 4.467362, mae: 11.648036, mean_q: 22.604996, mean_eps: 0.500000\n",
            " 4809/8000: episode: 148, duration: 1.068s, episode steps:  51, steps per second:  48, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 4.556151, mae: 11.760539, mean_q: 22.836252, mean_eps: 0.500000\n",
            " 4855/8000: episode: 149, duration: 0.914s, episode steps:  46, steps per second:  50, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 3.624989, mae: 11.816330, mean_q: 23.013001, mean_eps: 0.500000\n",
            " 4935/8000: episode: 150, duration: 1.523s, episode steps:  80, steps per second:  53, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 4.270435, mae: 11.953060, mean_q: 23.254336, mean_eps: 0.500000\n",
            " 4967/8000: episode: 151, duration: 0.558s, episode steps:  32, steps per second:  57, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.028735, mae: 11.989585, mean_q: 23.287950, mean_eps: 0.500000\n",
            " 5057/8000: episode: 152, duration: 1.705s, episode steps:  90, steps per second:  53, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 4.677637, mae: 12.209090, mean_q: 23.730844, mean_eps: 0.500000\n",
            " 5146/8000: episode: 153, duration: 1.733s, episode steps:  89, steps per second:  51, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 4.698449, mae: 12.359689, mean_q: 24.010917, mean_eps: 0.500000\n",
            " 5222/8000: episode: 154, duration: 1.467s, episode steps:  76, steps per second:  52, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 3.961097, mae: 12.406126, mean_q: 24.195963, mean_eps: 0.500000\n",
            " 5242/8000: episode: 155, duration: 0.407s, episode steps:  20, steps per second:  49, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 3.915338, mae: 12.547128, mean_q: 24.497119, mean_eps: 0.500000\n",
            " 5326/8000: episode: 156, duration: 1.753s, episode steps:  84, steps per second:  48, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.884002, mae: 12.645400, mean_q: 24.719105, mean_eps: 0.500000\n",
            " 5392/8000: episode: 157, duration: 1.312s, episode steps:  66, steps per second:  50, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.910592, mae: 12.831614, mean_q: 25.075955, mean_eps: 0.500000\n",
            " 5417/8000: episode: 158, duration: 0.501s, episode steps:  25, steps per second:  50, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 5.506434, mae: 12.843038, mean_q: 25.093788, mean_eps: 0.500000\n",
            " 5445/8000: episode: 159, duration: 0.583s, episode steps:  28, steps per second:  48, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 4.720365, mae: 12.919143, mean_q: 25.245742, mean_eps: 0.500000\n",
            " 5511/8000: episode: 160, duration: 1.308s, episode steps:  66, steps per second:  50, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 4.825850, mae: 12.903642, mean_q: 25.181778, mean_eps: 0.500000\n",
            " 5567/8000: episode: 161, duration: 1.182s, episode steps:  56, steps per second:  47, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 4.632093, mae: 13.098320, mean_q: 25.590582, mean_eps: 0.500000\n",
            " 5589/8000: episode: 162, duration: 0.404s, episode steps:  22, steps per second:  54, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 4.253657, mae: 12.975723, mean_q: 25.345329, mean_eps: 0.500000\n",
            " 5611/8000: episode: 163, duration: 0.476s, episode steps:  22, steps per second:  46, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 4.632590, mae: 13.088191, mean_q: 25.552139, mean_eps: 0.500000\n",
            " 5712/8000: episode: 164, duration: 2.012s, episode steps: 101, steps per second:  50, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 4.047128, mae: 13.267632, mean_q: 25.994879, mean_eps: 0.500000\n",
            " 5825/8000: episode: 165, duration: 2.135s, episode steps: 113, steps per second:  53, episode reward: 113.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 4.443509, mae: 13.389721, mean_q: 26.196309, mean_eps: 0.500000\n",
            " 5889/8000: episode: 166, duration: 1.204s, episode steps:  64, steps per second:  53, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.453 [0.000, 1.000],  loss: 4.332755, mae: 13.487257, mean_q: 26.457225, mean_eps: 0.500000\n",
            " 5926/8000: episode: 167, duration: 0.686s, episode steps:  37, steps per second:  54, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 3.822382, mae: 13.616074, mean_q: 26.850487, mean_eps: 0.500000\n",
            " 6036/8000: episode: 168, duration: 1.784s, episode steps: 110, steps per second:  62, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 6.965342, mae: 13.778712, mean_q: 26.822755, mean_eps: 0.500000\n",
            " 6163/8000: episode: 169, duration: 2.159s, episode steps: 127, steps per second:  59, episode reward: 127.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 4.810441, mae: 13.941795, mean_q: 27.310457, mean_eps: 0.500000\n",
            " 6194/8000: episode: 170, duration: 0.542s, episode steps:  31, steps per second:  57, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 6.838939, mae: 13.984307, mean_q: 27.186922, mean_eps: 0.500000\n",
            " 6350/8000: episode: 171, duration: 2.759s, episode steps: 156, steps per second:  57, episode reward: 156.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 5.335622, mae: 14.152690, mean_q: 27.694152, mean_eps: 0.500000\n",
            " 6422/8000: episode: 172, duration: 1.404s, episode steps:  72, steps per second:  51, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 5.920441, mae: 14.406276, mean_q: 28.190810, mean_eps: 0.500000\n",
            " 6448/8000: episode: 173, duration: 0.554s, episode steps:  26, steps per second:  47, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 6.179694, mae: 14.339662, mean_q: 27.980480, mean_eps: 0.500000\n",
            " 6520/8000: episode: 174, duration: 1.295s, episode steps:  72, steps per second:  56, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 6.068785, mae: 14.423552, mean_q: 28.216984, mean_eps: 0.500000\n",
            " 6568/8000: episode: 175, duration: 0.815s, episode steps:  48, steps per second:  59, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 3.965425, mae: 14.397193, mean_q: 28.374381, mean_eps: 0.500000\n",
            " 6682/8000: episode: 176, duration: 2.042s, episode steps: 114, steps per second:  56, episode reward: 114.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 4.651874, mae: 14.632130, mean_q: 28.792596, mean_eps: 0.500000\n",
            " 6726/8000: episode: 177, duration: 1.140s, episode steps:  44, steps per second:  39, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 6.566430, mae: 14.807638, mean_q: 28.931076, mean_eps: 0.500000\n",
            " 6773/8000: episode: 178, duration: 1.099s, episode steps:  47, steps per second:  43, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  loss: 4.709951, mae: 14.836000, mean_q: 29.206036, mean_eps: 0.500000\n",
            " 6832/8000: episode: 179, duration: 1.103s, episode steps:  59, steps per second:  54, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 5.136584, mae: 14.792589, mean_q: 29.147720, mean_eps: 0.500000\n",
            " 6954/8000: episode: 180, duration: 2.494s, episode steps: 122, steps per second:  49, episode reward: 122.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 6.161359, mae: 15.078686, mean_q: 29.618075, mean_eps: 0.500000\n",
            " 7003/8000: episode: 181, duration: 1.016s, episode steps:  49, steps per second:  48, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.551 [0.000, 1.000],  loss: 6.580422, mae: 15.269937, mean_q: 29.960527, mean_eps: 0.500000\n",
            " 7123/8000: episode: 182, duration: 2.115s, episode steps: 120, steps per second:  57, episode reward: 120.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 4.558434, mae: 15.372178, mean_q: 30.375683, mean_eps: 0.500000\n",
            " 7187/8000: episode: 183, duration: 1.324s, episode steps:  64, steps per second:  48, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 5.396480, mae: 15.483340, mean_q: 30.502281, mean_eps: 0.500000\n",
            " 7253/8000: episode: 184, duration: 1.280s, episode steps:  66, steps per second:  52, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 4.665522, mae: 15.610986, mean_q: 30.818490, mean_eps: 0.500000\n",
            " 7317/8000: episode: 185, duration: 1.186s, episode steps:  64, steps per second:  54, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 3.583849, mae: 15.809579, mean_q: 31.318495, mean_eps: 0.500000\n",
            " 7409/8000: episode: 186, duration: 1.819s, episode steps:  92, steps per second:  51, episode reward: 92.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 5.595926, mae: 15.859332, mean_q: 31.281961, mean_eps: 0.500000\n",
            " 7460/8000: episode: 187, duration: 1.043s, episode steps:  51, steps per second:  49, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 7.563339, mae: 16.051063, mean_q: 31.574334, mean_eps: 0.500000\n",
            " 7481/8000: episode: 188, duration: 0.431s, episode steps:  21, steps per second:  49, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 5.120362, mae: 16.090478, mean_q: 31.754506, mean_eps: 0.500000\n",
            " 7538/8000: episode: 189, duration: 1.083s, episode steps:  57, steps per second:  53, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 3.940014, mae: 16.152297, mean_q: 32.008710, mean_eps: 0.500000\n",
            " 7590/8000: episode: 190, duration: 0.978s, episode steps:  52, steps per second:  53, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 5.521360, mae: 16.220251, mean_q: 32.174267, mean_eps: 0.500000\n",
            " 7684/8000: episode: 191, duration: 1.734s, episode steps:  94, steps per second:  54, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 6.168397, mae: 16.332737, mean_q: 32.302104, mean_eps: 0.500000\n",
            " 7842/8000: episode: 192, duration: 3.233s, episode steps: 158, steps per second:  49, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 6.656020, mae: 16.570860, mean_q: 32.734873, mean_eps: 0.500000\n",
            " 7985/8000: episode: 193, duration: 3.016s, episode steps: 143, steps per second:  47, episode reward: 143.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 6.138496, mae: 16.761110, mean_q: 33.141224, mean_eps: 0.500000\n",
            "done, took 176.905 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 139.000, steps: 139\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 200.000, steps: 200\n",
            "Episode 6: reward: 200.000, steps: 200\n",
            "Episode 7: reward: 200.000, steps: 200\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 200.000, steps: 200\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 120.000, steps: 120\n",
            "Episode 12: reward: 200.000, steps: 200\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 200.000, steps: 200\n",
            "Episode 15: reward: 200.000, steps: 200\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 200.000, steps: 200\n",
            "Episode 18: reward: 134.000, steps: 134\n",
            "Episode 19: reward: 116.000, steps: 116\n",
            "Episode 20: reward: 200.000, steps: 200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb16f20efd0>"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    }
  ]
}