{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPoleDQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Megacity1/CartpoleDQN/blob/main/Week%203%20Deep%20RL%202/CartPoleDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Deep Q Network (DQN) for CartPole Using Boltzmann Q Policy\n",
        "This exercise implements a DQN for CartPole using a Boltzmann Q policy for selecting the actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGsC7cJ5jNcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda73cbf-3020-40f4-e45e-7b87053448f1"
      },
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.24.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (13.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.44.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.10.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMIHLgQ3Z-lF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5a8608-456f-4127-ce8a-f551e5f4e4f3"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0AMLzq08ap0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b380e1-6c3d-4bce-8233-a8ac01deaf26"
      },
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "!pip install Adam\n",
        "#from keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Adam in /usr/local/lib/python3.7/dist-packages (0.0.0.dev0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll6bNdUm54WS"
      },
      "source": [
        "Implementation of DQN for CartPole, applying policy BoltzmannQPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCrPKNy40PC"
      },
      "source": [
        "##Implement DQN with BoltzmannGumbelQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efM9jkXr5A3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8ca6b696-e8fb-4631-9e1d-18f1a095c2a7"
      },
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import BoltzmannGumbelQPolicy\n",
        "from rl.policy import LinearAnnealedPolicy\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n",
        "# setup experience replay buffer\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
        "                               attr='eps',            \n",
        "                               value_max=2.,\n",
        "                               value_min=.2, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=20)\n",
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "# add extra layers here\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=15,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy) \n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=4000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_34\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_32 (Flatten)        (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_65 (Dense)            (None, 16)                80        \n",
            "                                                                 \n",
            " dense_66 (Dense)            (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 4000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   11/4000: episode: 1, duration: 2.420s, episode steps:  11, steps per second:   5, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   67/4000: episode: 2, duration: 9.034s, episode steps:  56, steps per second:   6, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 0.741963, mae: 0.648453, mean_q: 0.192963, mean_eps: 0.217647\n",
            "  139/4000: episode: 3, duration: 1.106s, episode steps:  72, steps per second:  65, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 0.450493, mae: 0.537441, mean_q: 0.261176, mean_eps: 0.200000\n",
            "  226/4000: episode: 4, duration: 1.075s, episode steps:  87, steps per second:  81, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 0.281989, mae: 0.582602, mean_q: 0.601319, mean_eps: 0.200000\n",
            "  245/4000: episode: 5, duration: 0.311s, episode steps:  19, steps per second:  61, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.181491, mae: 0.670228, mean_q: 0.975432, mean_eps: 0.200000\n",
            "  266/4000: episode: 6, duration: 0.406s, episode steps:  21, steps per second:  52, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.179649, mae: 0.742052, mean_q: 1.160679, mean_eps: 0.200000\n",
            "  287/4000: episode: 7, duration: 0.426s, episode steps:  21, steps per second:  49, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.127375, mae: 0.795606, mean_q: 1.368646, mean_eps: 0.200000\n",
            "  306/4000: episode: 8, duration: 0.369s, episode steps:  19, steps per second:  52, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 0.138227, mae: 0.883231, mean_q: 1.580609, mean_eps: 0.200000\n",
            "  318/4000: episode: 9, duration: 0.233s, episode steps:  12, steps per second:  52, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.091979, mae: 0.936164, mean_q: 1.757929, mean_eps: 0.200000\n",
            "  332/4000: episode: 10, duration: 0.229s, episode steps:  14, steps per second:  61, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.123067, mae: 1.012389, mean_q: 1.892616, mean_eps: 0.200000\n",
            "  346/4000: episode: 11, duration: 0.218s, episode steps:  14, steps per second:  64, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.126356, mae: 1.094815, mean_q: 2.061443, mean_eps: 0.200000\n",
            "  357/4000: episode: 12, duration: 0.136s, episode steps:  11, steps per second:  81, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.170502, mae: 1.156648, mean_q: 2.147490, mean_eps: 0.200000\n",
            "  370/4000: episode: 13, duration: 0.191s, episode steps:  13, steps per second:  68, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.154467, mae: 1.190188, mean_q: 2.238905, mean_eps: 0.200000\n",
            "  384/4000: episode: 14, duration: 0.180s, episode steps:  14, steps per second:  78, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.146257, mae: 1.252841, mean_q: 2.390468, mean_eps: 0.200000\n",
            "  403/4000: episode: 15, duration: 0.234s, episode steps:  19, steps per second:  81, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.684 [0.000, 1.000],  loss: 0.196132, mae: 1.353423, mean_q: 2.598965, mean_eps: 0.200000\n",
            "  417/4000: episode: 16, duration: 0.167s, episode steps:  14, steps per second:  84, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.227989, mae: 1.436896, mean_q: 2.744330, mean_eps: 0.200000\n",
            "  428/4000: episode: 17, duration: 0.155s, episode steps:  11, steps per second:  71, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.184513, mae: 1.505416, mean_q: 2.874693, mean_eps: 0.200000\n",
            "  438/4000: episode: 18, duration: 0.158s, episode steps:  10, steps per second:  63, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.246915, mae: 1.504816, mean_q: 2.882364, mean_eps: 0.200000\n",
            "  450/4000: episode: 19, duration: 0.159s, episode steps:  12, steps per second:  76, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.271715, mae: 1.596744, mean_q: 2.986014, mean_eps: 0.200000\n",
            "  461/4000: episode: 20, duration: 0.139s, episode steps:  11, steps per second:  79, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.227953, mae: 1.654750, mean_q: 3.145100, mean_eps: 0.200000\n",
            "  472/4000: episode: 21, duration: 0.148s, episode steps:  11, steps per second:  74, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.264425, mae: 1.696873, mean_q: 3.237761, mean_eps: 0.200000\n",
            "  484/4000: episode: 22, duration: 0.154s, episode steps:  12, steps per second:  78, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.292572, mae: 1.773571, mean_q: 3.381709, mean_eps: 0.200000\n",
            "  499/4000: episode: 23, duration: 0.192s, episode steps:  15, steps per second:  78, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.396098, mae: 1.858889, mean_q: 3.488917, mean_eps: 0.200000\n",
            "  510/4000: episode: 24, duration: 0.133s, episode steps:  11, steps per second:  83, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.355416, mae: 1.853107, mean_q: 3.558305, mean_eps: 0.200000\n",
            "  522/4000: episode: 25, duration: 0.167s, episode steps:  12, steps per second:  72, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.436345, mae: 1.993380, mean_q: 3.727263, mean_eps: 0.200000\n",
            "  532/4000: episode: 26, duration: 0.144s, episode steps:  10, steps per second:  70, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.277228, mae: 1.920912, mean_q: 3.734506, mean_eps: 0.200000\n",
            "  544/4000: episode: 27, duration: 0.156s, episode steps:  12, steps per second:  77, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.409333, mae: 2.034245, mean_q: 3.896662, mean_eps: 0.200000\n",
            "  555/4000: episode: 28, duration: 0.133s, episode steps:  11, steps per second:  83, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.447150, mae: 2.139687, mean_q: 4.059150, mean_eps: 0.200000\n",
            "  565/4000: episode: 29, duration: 0.133s, episode steps:  10, steps per second:  75, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.530921, mae: 2.168660, mean_q: 4.119710, mean_eps: 0.200000\n",
            "  578/4000: episode: 30, duration: 0.166s, episode steps:  13, steps per second:  78, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.521317, mae: 2.230153, mean_q: 4.208477, mean_eps: 0.200000\n",
            "  587/4000: episode: 31, duration: 0.122s, episode steps:   9, steps per second:  74, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.472803, mae: 2.230632, mean_q: 4.291575, mean_eps: 0.200000\n",
            "  602/4000: episode: 32, duration: 0.213s, episode steps:  15, steps per second:  70, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 0.429194, mae: 2.288958, mean_q: 4.417422, mean_eps: 0.200000\n",
            "  612/4000: episode: 33, duration: 0.123s, episode steps:  10, steps per second:  81, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.479988, mae: 2.351573, mean_q: 4.553681, mean_eps: 0.200000\n",
            "  621/4000: episode: 34, duration: 0.115s, episode steps:   9, steps per second:  79, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.496666, mae: 2.375463, mean_q: 4.580411, mean_eps: 0.200000\n",
            "  631/4000: episode: 35, duration: 0.139s, episode steps:  10, steps per second:  72, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.648577, mae: 2.499503, mean_q: 4.624815, mean_eps: 0.200000\n",
            "  648/4000: episode: 36, duration: 0.247s, episode steps:  17, steps per second:  69, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.533122, mae: 2.489858, mean_q: 4.629807, mean_eps: 0.200000\n",
            "  662/4000: episode: 37, duration: 0.271s, episode steps:  14, steps per second:  52, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.524413, mae: 2.554519, mean_q: 4.817976, mean_eps: 0.200000\n",
            "  673/4000: episode: 38, duration: 0.208s, episode steps:  11, steps per second:  53, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.307465, mae: 2.531956, mean_q: 5.008606, mean_eps: 0.200000\n",
            "  686/4000: episode: 39, duration: 0.248s, episode steps:  13, steps per second:  53, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.554113, mae: 2.637604, mean_q: 5.121820, mean_eps: 0.200000\n",
            "  695/4000: episode: 40, duration: 0.160s, episode steps:   9, steps per second:  56, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.574373, mae: 2.686839, mean_q: 5.089788, mean_eps: 0.200000\n",
            "  707/4000: episode: 41, duration: 0.167s, episode steps:  12, steps per second:  72, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.587271, mae: 2.725255, mean_q: 5.124179, mean_eps: 0.200000\n",
            "  720/4000: episode: 42, duration: 0.162s, episode steps:  13, steps per second:  80, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.626837, mae: 2.790703, mean_q: 5.290018, mean_eps: 0.200000\n",
            "  731/4000: episode: 43, duration: 0.163s, episode steps:  11, steps per second:  68, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.453036, mae: 2.767147, mean_q: 5.381156, mean_eps: 0.200000\n",
            "  743/4000: episode: 44, duration: 0.155s, episode steps:  12, steps per second:  77, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.569120, mae: 2.870796, mean_q: 5.466582, mean_eps: 0.200000\n",
            "  753/4000: episode: 45, duration: 0.137s, episode steps:  10, steps per second:  73, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.664491, mae: 2.901510, mean_q: 5.551339, mean_eps: 0.200000\n",
            "  767/4000: episode: 46, duration: 0.169s, episode steps:  14, steps per second:  83, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.637537, mae: 2.941554, mean_q: 5.601578, mean_eps: 0.200000\n",
            "  778/4000: episode: 47, duration: 0.138s, episode steps:  11, steps per second:  80, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.678211, mae: 2.986081, mean_q: 5.661232, mean_eps: 0.200000\n",
            "  788/4000: episode: 48, duration: 0.125s, episode steps:  10, steps per second:  80, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.800458, mae: 3.049571, mean_q: 5.712446, mean_eps: 0.200000\n",
            "  804/4000: episode: 49, duration: 0.205s, episode steps:  16, steps per second:  78, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.531416, mae: 3.050188, mean_q: 5.777410, mean_eps: 0.200000\n",
            "  816/4000: episode: 50, duration: 0.148s, episode steps:  12, steps per second:  81, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.839843, mae: 3.137877, mean_q: 5.932310, mean_eps: 0.200000\n",
            "  828/4000: episode: 51, duration: 0.148s, episode steps:  12, steps per second:  81, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.593525, mae: 3.141919, mean_q: 5.979501, mean_eps: 0.200000\n",
            "  838/4000: episode: 52, duration: 0.126s, episode steps:  10, steps per second:  79, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.660034, mae: 3.179736, mean_q: 6.089326, mean_eps: 0.200000\n",
            "  851/4000: episode: 53, duration: 0.164s, episode steps:  13, steps per second:  79, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.812631, mae: 3.267034, mean_q: 6.131263, mean_eps: 0.200000\n",
            "  866/4000: episode: 54, duration: 0.187s, episode steps:  15, steps per second:  80, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.731450, mae: 3.286558, mean_q: 6.181973, mean_eps: 0.200000\n",
            "  878/4000: episode: 55, duration: 0.166s, episode steps:  12, steps per second:  72, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.807040, mae: 3.340202, mean_q: 6.302632, mean_eps: 0.200000\n",
            "  893/4000: episode: 56, duration: 0.193s, episode steps:  15, steps per second:  78, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.814310, mae: 3.395288, mean_q: 6.330044, mean_eps: 0.200000\n",
            "  911/4000: episode: 57, duration: 0.219s, episode steps:  18, steps per second:  82, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.816992, mae: 3.444308, mean_q: 6.475742, mean_eps: 0.200000\n",
            "  925/4000: episode: 58, duration: 0.175s, episode steps:  14, steps per second:  80, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.756898, mae: 3.452223, mean_q: 6.522734, mean_eps: 0.200000\n",
            "  936/4000: episode: 59, duration: 0.135s, episode steps:  11, steps per second:  82, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.898702, mae: 3.516043, mean_q: 6.613023, mean_eps: 0.200000\n",
            "  951/4000: episode: 60, duration: 0.180s, episode steps:  15, steps per second:  83, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.868753, mae: 3.550988, mean_q: 6.692615, mean_eps: 0.200000\n",
            "  963/4000: episode: 61, duration: 0.167s, episode steps:  12, steps per second:  72, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.825154, mae: 3.574877, mean_q: 6.799048, mean_eps: 0.200000\n",
            "  976/4000: episode: 62, duration: 0.164s, episode steps:  13, steps per second:  79, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1.082381, mae: 3.633332, mean_q: 6.700447, mean_eps: 0.200000\n",
            "  987/4000: episode: 63, duration: 0.137s, episode steps:  11, steps per second:  81, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.805398, mae: 3.611793, mean_q: 6.726537, mean_eps: 0.200000\n",
            "  997/4000: episode: 64, duration: 0.128s, episode steps:  10, steps per second:  78, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.547786, mae: 3.604005, mean_q: 6.911143, mean_eps: 0.200000\n",
            " 1009/4000: episode: 65, duration: 0.149s, episode steps:  12, steps per second:  81, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.931849, mae: 3.693208, mean_q: 7.082979, mean_eps: 0.200000\n",
            " 1019/4000: episode: 66, duration: 0.123s, episode steps:  10, steps per second:  81, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.243285, mae: 3.763591, mean_q: 6.987269, mean_eps: 0.200000\n",
            " 1035/4000: episode: 67, duration: 0.305s, episode steps:  16, steps per second:  52, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.953542, mae: 3.740253, mean_q: 6.926753, mean_eps: 0.200000\n",
            " 1049/4000: episode: 68, duration: 0.278s, episode steps:  14, steps per second:  50, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.720366, mae: 3.769549, mean_q: 7.134114, mean_eps: 0.200000\n",
            " 1064/4000: episode: 69, duration: 0.264s, episode steps:  15, steps per second:  57, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.191629, mae: 3.856004, mean_q: 7.223192, mean_eps: 0.200000\n",
            " 1074/4000: episode: 70, duration: 0.172s, episode steps:  10, steps per second:  58, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.958083, mae: 3.873131, mean_q: 7.295880, mean_eps: 0.200000\n",
            " 1089/4000: episode: 71, duration: 0.187s, episode steps:  15, steps per second:  80, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.917668, mae: 3.873311, mean_q: 7.234556, mean_eps: 0.200000\n",
            " 1099/4000: episode: 72, duration: 0.133s, episode steps:  10, steps per second:  75, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.860356, mae: 3.887572, mean_q: 7.360481, mean_eps: 0.200000\n",
            " 1110/4000: episode: 73, duration: 0.140s, episode steps:  11, steps per second:  78, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 1.018513, mae: 3.937468, mean_q: 7.433958, mean_eps: 0.200000\n",
            " 1119/4000: episode: 74, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 1.027411, mae: 3.934831, mean_q: 7.395590, mean_eps: 0.200000\n",
            " 1128/4000: episode: 75, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.004420, mae: 3.964724, mean_q: 7.346289, mean_eps: 0.200000\n",
            " 1144/4000: episode: 76, duration: 0.187s, episode steps:  16, steps per second:  85, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.073429, mae: 4.002884, mean_q: 7.433152, mean_eps: 0.200000\n",
            " 1155/4000: episode: 77, duration: 0.134s, episode steps:  11, steps per second:  82, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.994246, mae: 4.024349, mean_q: 7.466601, mean_eps: 0.200000\n",
            " 1166/4000: episode: 78, duration: 0.137s, episode steps:  11, steps per second:  81, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.893014, mae: 4.038007, mean_q: 7.665987, mean_eps: 0.200000\n",
            " 1186/4000: episode: 79, duration: 0.259s, episode steps:  20, steps per second:  77, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.826361, mae: 4.076162, mean_q: 7.721102, mean_eps: 0.200000\n",
            " 1204/4000: episode: 80, duration: 0.216s, episode steps:  18, steps per second:  83, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.930180, mae: 4.149589, mean_q: 7.830666, mean_eps: 0.200000\n",
            " 1245/4000: episode: 81, duration: 0.466s, episode steps:  41, steps per second:  88, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.439 [0.000, 1.000],  loss: 1.057190, mae: 4.177205, mean_q: 7.822236, mean_eps: 0.200000\n",
            " 1285/4000: episode: 82, duration: 0.471s, episode steps:  40, steps per second:  85, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 1.024532, mae: 4.255798, mean_q: 7.939185, mean_eps: 0.200000\n",
            " 1330/4000: episode: 83, duration: 0.502s, episode steps:  45, steps per second:  90, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 1.089136, mae: 4.378736, mean_q: 8.219746, mean_eps: 0.200000\n",
            " 1374/4000: episode: 84, duration: 0.513s, episode steps:  44, steps per second:  86, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.974866, mae: 4.459001, mean_q: 8.396463, mean_eps: 0.200000\n",
            " 1462/4000: episode: 85, duration: 1.177s, episode steps:  88, steps per second:  75, episode reward: 88.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 0.957016, mae: 4.651547, mean_q: 8.810921, mean_eps: 0.200000\n",
            " 1525/4000: episode: 86, duration: 0.830s, episode steps:  63, steps per second:  76, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 0.978277, mae: 4.874146, mean_q: 9.267158, mean_eps: 0.200000\n",
            " 1573/4000: episode: 87, duration: 0.528s, episode steps:  48, steps per second:  91, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 1.316138, mae: 4.978252, mean_q: 9.362584, mean_eps: 0.200000\n",
            " 1583/4000: episode: 88, duration: 0.129s, episode steps:  10, steps per second:  78, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.242469, mae: 5.024834, mean_q: 9.494415, mean_eps: 0.200000\n",
            " 1592/4000: episode: 89, duration: 0.127s, episode steps:   9, steps per second:  71, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.838938, mae: 5.045433, mean_q: 9.546725, mean_eps: 0.200000\n",
            " 1601/4000: episode: 90, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.393697, mae: 5.079211, mean_q: 9.608910, mean_eps: 0.200000\n",
            " 1609/4000: episode: 91, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.640583, mae: 5.163016, mean_q: 9.705608, mean_eps: 0.200000\n",
            " 1621/4000: episode: 92, duration: 0.140s, episode steps:  12, steps per second:  85, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 1.753652, mae: 5.213650, mean_q: 9.749531, mean_eps: 0.200000\n",
            " 1692/4000: episode: 93, duration: 0.868s, episode steps:  71, steps per second:  82, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 1.672938, mae: 5.276434, mean_q: 9.889795, mean_eps: 0.200000\n",
            " 1764/4000: episode: 94, duration: 0.996s, episode steps:  72, steps per second:  72, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 1.926065, mae: 5.483165, mean_q: 10.311221, mean_eps: 0.200000\n",
            " 1812/4000: episode: 95, duration: 0.783s, episode steps:  48, steps per second:  61, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 1.614543, mae: 5.601800, mean_q: 10.515437, mean_eps: 0.200000\n",
            " 1875/4000: episode: 96, duration: 1.083s, episode steps:  63, steps per second:  58, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 1.481256, mae: 5.646595, mean_q: 10.637189, mean_eps: 0.200000\n",
            " 1911/4000: episode: 97, duration: 0.551s, episode steps:  36, steps per second:  65, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.491154, mae: 5.795932, mean_q: 11.064648, mean_eps: 0.200000\n",
            " 1929/4000: episode: 98, duration: 0.305s, episode steps:  18, steps per second:  59, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 2.289651, mae: 5.943140, mean_q: 11.230926, mean_eps: 0.200000\n",
            " 2032/4000: episode: 99, duration: 1.611s, episode steps: 103, steps per second:  64, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 1.717222, mae: 5.989273, mean_q: 11.356396, mean_eps: 0.200000\n",
            " 2049/4000: episode: 100, duration: 0.311s, episode steps:  17, steps per second:  55, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 2.433258, mae: 6.183032, mean_q: 11.648243, mean_eps: 0.200000\n",
            " 2060/4000: episode: 101, duration: 0.148s, episode steps:  11, steps per second:  74, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 2.214426, mae: 6.209593, mean_q: 11.675186, mean_eps: 0.200000\n",
            " 2077/4000: episode: 102, duration: 0.221s, episode steps:  17, steps per second:  77, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 1.620174, mae: 6.131900, mean_q: 11.621563, mean_eps: 0.200000\n",
            " 2088/4000: episode: 103, duration: 0.137s, episode steps:  11, steps per second:  80, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 1.074331, mae: 6.217842, mean_q: 11.959723, mean_eps: 0.200000\n",
            " 2098/4000: episode: 104, duration: 0.124s, episode steps:  10, steps per second:  81, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 2.651915, mae: 6.339136, mean_q: 11.967662, mean_eps: 0.200000\n",
            " 2108/4000: episode: 105, duration: 0.128s, episode steps:  10, steps per second:  78, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 1.650934, mae: 6.313345, mean_q: 12.037855, mean_eps: 0.200000\n",
            " 2117/4000: episode: 106, duration: 0.124s, episode steps:   9, steps per second:  72, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.229047, mae: 6.266228, mean_q: 11.907428, mean_eps: 0.200000\n",
            " 2128/4000: episode: 107, duration: 0.136s, episode steps:  11, steps per second:  81, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 1.373607, mae: 6.389257, mean_q: 12.287261, mean_eps: 0.200000\n",
            " 2137/4000: episode: 108, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.824076, mae: 6.495946, mean_q: 12.350765, mean_eps: 0.200000\n",
            " 2147/4000: episode: 109, duration: 0.132s, episode steps:  10, steps per second:  76, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.082931, mae: 6.496429, mean_q: 12.447012, mean_eps: 0.200000\n",
            " 2157/4000: episode: 110, duration: 0.147s, episode steps:  10, steps per second:  68, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 2.316389, mae: 6.524534, mean_q: 12.494251, mean_eps: 0.200000\n",
            " 2167/4000: episode: 111, duration: 0.122s, episode steps:  10, steps per second:  82, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 1.816291, mae: 6.512469, mean_q: 12.476607, mean_eps: 0.200000\n",
            " 2182/4000: episode: 112, duration: 0.198s, episode steps:  15, steps per second:  76, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.557535, mae: 6.497370, mean_q: 12.551559, mean_eps: 0.200000\n",
            " 2192/4000: episode: 113, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 3.083863, mae: 6.647937, mean_q: 12.640267, mean_eps: 0.200000\n",
            " 2202/4000: episode: 114, duration: 0.136s, episode steps:  10, steps per second:  73, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.582380, mae: 6.538793, mean_q: 12.533633, mean_eps: 0.200000\n",
            " 2212/4000: episode: 115, duration: 0.164s, episode steps:  10, steps per second:  61, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 1.969229, mae: 6.575652, mean_q: 12.605759, mean_eps: 0.200000\n",
            " 2224/4000: episode: 116, duration: 0.232s, episode steps:  12, steps per second:  52, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 2.813675, mae: 6.690877, mean_q: 12.652809, mean_eps: 0.200000\n",
            " 2239/4000: episode: 117, duration: 0.299s, episode steps:  15, steps per second:  50, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 2.595500, mae: 6.727358, mean_q: 12.700787, mean_eps: 0.200000\n",
            " 2251/4000: episode: 118, duration: 0.272s, episode steps:  12, steps per second:  44, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 2.738440, mae: 6.702482, mean_q: 12.613528, mean_eps: 0.200000\n",
            " 2261/4000: episode: 119, duration: 0.191s, episode steps:  10, steps per second:  52, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 1.876741, mae: 6.755186, mean_q: 12.904142, mean_eps: 0.200000\n",
            " 2273/4000: episode: 120, duration: 0.278s, episode steps:  12, steps per second:  43, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 3.660375, mae: 6.923186, mean_q: 12.959937, mean_eps: 0.200000\n",
            " 2286/4000: episode: 121, duration: 0.253s, episode steps:  13, steps per second:  51, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 1.978552, mae: 6.728534, mean_q: 12.751948, mean_eps: 0.200000\n",
            " 2298/4000: episode: 122, duration: 0.242s, episode steps:  12, steps per second:  50, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 2.211620, mae: 6.715737, mean_q: 12.697615, mean_eps: 0.200000\n",
            " 2310/4000: episode: 123, duration: 0.210s, episode steps:  12, steps per second:  57, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 2.805065, mae: 6.809861, mean_q: 12.821433, mean_eps: 0.200000\n",
            " 2341/4000: episode: 124, duration: 0.629s, episode steps:  31, steps per second:  49, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 3.250987, mae: 6.887023, mean_q: 12.901869, mean_eps: 0.200000\n",
            " 2432/4000: episode: 125, duration: 1.588s, episode steps:  91, steps per second:  57, episode reward: 91.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 3.017028, mae: 7.036411, mean_q: 13.208156, mean_eps: 0.200000\n",
            " 2478/4000: episode: 126, duration: 0.638s, episode steps:  46, steps per second:  72, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 2.912209, mae: 7.166229, mean_q: 13.526543, mean_eps: 0.200000\n",
            " 2496/4000: episode: 127, duration: 0.241s, episode steps:  18, steps per second:  75, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 2.723627, mae: 7.189589, mean_q: 13.638232, mean_eps: 0.200000\n",
            " 2512/4000: episode: 128, duration: 0.202s, episode steps:  16, steps per second:  79, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.013519, mae: 7.256360, mean_q: 13.907391, mean_eps: 0.200000\n",
            " 2531/4000: episode: 129, duration: 0.242s, episode steps:  19, steps per second:  79, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 3.149796, mae: 7.351965, mean_q: 14.010011, mean_eps: 0.200000\n",
            " 2555/4000: episode: 130, duration: 0.307s, episode steps:  24, steps per second:  78, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 3.265392, mae: 7.345631, mean_q: 13.816494, mean_eps: 0.200000\n",
            " 2585/4000: episode: 131, duration: 0.461s, episode steps:  30, steps per second:  65, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.606878, mae: 7.383256, mean_q: 14.093099, mean_eps: 0.200000\n",
            " 2613/4000: episode: 132, duration: 0.511s, episode steps:  28, steps per second:  55, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 3.124023, mae: 7.543894, mean_q: 14.304311, mean_eps: 0.200000\n",
            " 2641/4000: episode: 133, duration: 0.413s, episode steps:  28, steps per second:  68, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.243081, mae: 7.608635, mean_q: 14.272599, mean_eps: 0.200000\n",
            " 2657/4000: episode: 134, duration: 0.200s, episode steps:  16, steps per second:  80, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.875282, mae: 7.549150, mean_q: 14.236325, mean_eps: 0.200000\n",
            " 2682/4000: episode: 135, duration: 0.298s, episode steps:  25, steps per second:  84, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 2.904908, mae: 7.575146, mean_q: 14.410006, mean_eps: 0.200000\n",
            " 2708/4000: episode: 136, duration: 0.305s, episode steps:  26, steps per second:  85, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.954815, mae: 7.663798, mean_q: 14.602783, mean_eps: 0.200000\n",
            " 2758/4000: episode: 137, duration: 0.792s, episode steps:  50, steps per second:  63, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.093562, mae: 7.762069, mean_q: 14.784619, mean_eps: 0.200000\n",
            " 2792/4000: episode: 138, duration: 0.483s, episode steps:  34, steps per second:  70, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 3.805085, mae: 7.861962, mean_q: 14.791357, mean_eps: 0.200000\n",
            " 2822/4000: episode: 139, duration: 0.357s, episode steps:  30, steps per second:  84, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 4.508564, mae: 7.953742, mean_q: 14.923044, mean_eps: 0.200000\n",
            " 2876/4000: episode: 140, duration: 0.649s, episode steps:  54, steps per second:  83, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 3.003750, mae: 7.918567, mean_q: 15.010688, mean_eps: 0.200000\n",
            " 2939/4000: episode: 141, duration: 0.703s, episode steps:  63, steps per second:  90, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 3.211965, mae: 8.061375, mean_q: 15.373227, mean_eps: 0.200000\n",
            " 3004/4000: episode: 142, duration: 0.748s, episode steps:  65, steps per second:  87, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 3.301003, mae: 8.133618, mean_q: 15.489180, mean_eps: 0.200000\n",
            " 3058/4000: episode: 143, duration: 0.708s, episode steps:  54, steps per second:  76, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 2.914707, mae: 8.278072, mean_q: 15.864761, mean_eps: 0.200000\n",
            " 3094/4000: episode: 144, duration: 0.601s, episode steps:  36, steps per second:  60, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.715671, mae: 8.442302, mean_q: 16.032582, mean_eps: 0.200000\n",
            " 3151/4000: episode: 145, duration: 0.960s, episode steps:  57, steps per second:  59, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 3.382454, mae: 8.402593, mean_q: 16.021139, mean_eps: 0.200000\n",
            " 3213/4000: episode: 146, duration: 1.075s, episode steps:  62, steps per second:  58, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 3.315678, mae: 8.519363, mean_q: 16.228270, mean_eps: 0.200000\n",
            " 3295/4000: episode: 147, duration: 1.266s, episode steps:  82, steps per second:  65, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 3.781177, mae: 8.700977, mean_q: 16.589295, mean_eps: 0.200000\n",
            " 3336/4000: episode: 148, duration: 0.689s, episode steps:  41, steps per second:  59, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 4.663327, mae: 8.818832, mean_q: 16.716276, mean_eps: 0.200000\n",
            " 3461/4000: episode: 149, duration: 1.546s, episode steps: 125, steps per second:  81, episode reward: 125.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 4.032931, mae: 8.930553, mean_q: 17.027956, mean_eps: 0.200000\n",
            " 3530/4000: episode: 150, duration: 1.153s, episode steps:  69, steps per second:  60, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 4.198194, mae: 9.059351, mean_q: 17.281291, mean_eps: 0.200000\n",
            " 3564/4000: episode: 151, duration: 0.555s, episode steps:  34, steps per second:  61, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 3.546093, mae: 9.309121, mean_q: 17.886177, mean_eps: 0.200000\n",
            " 3688/4000: episode: 152, duration: 1.983s, episode steps: 124, steps per second:  63, episode reward: 124.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 3.760580, mae: 9.328469, mean_q: 17.915250, mean_eps: 0.200000\n",
            " 3735/4000: episode: 153, duration: 0.613s, episode steps:  47, steps per second:  77, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 4.122140, mae: 9.509149, mean_q: 18.235370, mean_eps: 0.200000\n",
            " 3786/4000: episode: 154, duration: 0.600s, episode steps:  51, steps per second:  85, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 4.046333, mae: 9.639169, mean_q: 18.549448, mean_eps: 0.200000\n",
            " 3836/4000: episode: 155, duration: 0.592s, episode steps:  50, steps per second:  84, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 2.911673, mae: 9.635999, mean_q: 18.686106, mean_eps: 0.200000\n",
            " 3882/4000: episode: 156, duration: 0.522s, episode steps:  46, steps per second:  88, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 4.403825, mae: 9.805198, mean_q: 18.884300, mean_eps: 0.200000\n",
            " 3964/4000: episode: 157, duration: 0.971s, episode steps:  82, steps per second:  84, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 4.455135, mae: 9.960112, mean_q: 19.217477, mean_eps: 0.200000\n",
            "done, took 67.587 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 50.000, steps: 50\n",
            "Episode 2: reward: 39.000, steps: 39\n",
            "Episode 3: reward: 61.000, steps: 61\n",
            "Episode 4: reward: 78.000, steps: 78\n",
            "Episode 5: reward: 47.000, steps: 47\n",
            "Episode 6: reward: 49.000, steps: 49\n",
            "Episode 7: reward: 73.000, steps: 73\n",
            "Episode 8: reward: 38.000, steps: 38\n",
            "Episode 9: reward: 82.000, steps: 82\n",
            "Episode 10: reward: 56.000, steps: 56\n",
            "Episode 11: reward: 55.000, steps: 55\n",
            "Episode 12: reward: 37.000, steps: 37\n",
            "Episode 13: reward: 57.000, steps: 57\n",
            "Episode 14: reward: 80.000, steps: 80\n",
            "Episode 15: reward: 200.000, steps: 200\n",
            "Episode 16: reward: 52.000, steps: 52\n",
            "Episode 17: reward: 38.000, steps: 38\n",
            "Episode 18: reward: 58.000, steps: 58\n",
            "Episode 19: reward: 39.000, steps: 39\n",
            "Episode 20: reward: 119.000, steps: 119\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb1723a3e10>"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    }
  ]
}