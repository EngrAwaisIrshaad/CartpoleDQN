{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPoleDQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Megacity1/CartpoleDQN/blob/main/Week%203%20Deep%20RL%202/CartPoleDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Deep Q Network (DQN) for CartPole Using Boltzmann Q Policy\n",
        "This exercise implements a DQN for CartPole using a Boltzmann Q policy for selecting the actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGsC7cJ5jNcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda73cbf-3020-40f4-e45e-7b87053448f1"
      },
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.24.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (13.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.44.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.10.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMIHLgQ3Z-lF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5a8608-456f-4127-ce8a-f551e5f4e4f3"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0AMLzq08ap0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b380e1-6c3d-4bce-8233-a8ac01deaf26"
      },
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "!pip install Adam\n",
        "#from keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Adam in /usr/local/lib/python3.7/dist-packages (0.0.0.dev0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll6bNdUm54WS"
      },
      "source": [
        "Implementation of DQN for CartPole, applying policy BoltzmannQPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCrPKNy40PC"
      },
      "source": [
        "##Implement DQN with BoltzmannGumbelQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efM9jkXr5A3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c40afed0-4726-46e4-9250-1846dfa84c86"
      },
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import BoltzmannGumbelQPolicy\n",
        "from rl.policy import LinearAnnealedPolicy\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n",
        "# setup experience replay buffer\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
        "                               attr='eps',            \n",
        "                               value_max=5.,\n",
        "                               value_min=.5, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=200)\n",
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "# add extra layers here\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=20,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy) \n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_47\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_45 (Flatten)        (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_91 (Dense)            (None, 16)                80        \n",
            "                                                                 \n",
            " dense_92 (Dense)            (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 10000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   16/10000: episode: 1, duration: 2.428s, episode steps:  16, steps per second:   7, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   55/10000: episode: 2, duration: 11.228s, episode steps:  39, steps per second:   3, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.436 [0.000, 1.000],  loss: 0.528783, mae: 0.577818, mean_q: -0.035839, mean_eps: 4.156250\n",
            "   66/10000: episode: 3, duration: 0.319s, episode steps:  11, steps per second:  35, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.442412, mae: 0.513853, mean_q: 0.005641, mean_eps: 3.650000\n",
            "  121/10000: episode: 4, duration: 1.387s, episode steps:  55, steps per second:  40, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.582 [0.000, 1.000],  loss: 0.395210, mae: 0.509272, mean_q: 0.163071, mean_eps: 2.907500\n",
            "  155/10000: episode: 5, duration: 0.930s, episode steps:  34, steps per second:  37, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.382 [0.000, 1.000],  loss: 0.341125, mae: 0.548035, mean_q: 0.341597, mean_eps: 1.906250\n",
            "  176/10000: episode: 6, duration: 0.562s, episode steps:  21, steps per second:  37, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.381 [0.000, 1.000],  loss: 0.277985, mae: 0.582614, mean_q: 0.493713, mean_eps: 1.287500\n",
            "  197/10000: episode: 7, duration: 0.552s, episode steps:  21, steps per second:  38, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.246603, mae: 0.616494, mean_q: 0.629369, mean_eps: 0.815000\n",
            "  210/10000: episode: 8, duration: 0.361s, episode steps:  13, steps per second:  36, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.218677, mae: 0.665300, mean_q: 0.797253, mean_eps: 0.510385\n",
            "  224/10000: episode: 9, duration: 0.382s, episode steps:  14, steps per second:  37, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.214 [0.000, 1.000],  loss: 0.188883, mae: 0.700774, mean_q: 0.937672, mean_eps: 0.500000\n",
            "  239/10000: episode: 10, duration: 0.438s, episode steps:  15, steps per second:  34, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.186635, mae: 0.740442, mean_q: 1.026020, mean_eps: 0.500000\n",
            "  250/10000: episode: 11, duration: 0.297s, episode steps:  11, steps per second:  37, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.170276, mae: 0.769085, mean_q: 1.108711, mean_eps: 0.500000\n",
            "  261/10000: episode: 12, duration: 0.327s, episode steps:  11, steps per second:  34, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.170855, mae: 0.821785, mean_q: 1.284406, mean_eps: 0.500000\n",
            "  281/10000: episode: 13, duration: 0.354s, episode steps:  20, steps per second:  57, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 0.163871, mae: 0.859463, mean_q: 1.366445, mean_eps: 0.500000\n",
            "  301/10000: episode: 14, duration: 0.343s, episode steps:  20, steps per second:  58, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.162173, mae: 0.924761, mean_q: 1.530996, mean_eps: 0.500000\n",
            "  312/10000: episode: 15, duration: 0.188s, episode steps:  11, steps per second:  58, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.180149, mae: 0.973818, mean_q: 1.649140, mean_eps: 0.500000\n",
            "  322/10000: episode: 16, duration: 0.193s, episode steps:  10, steps per second:  52, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.187661, mae: 1.009592, mean_q: 1.723749, mean_eps: 0.500000\n",
            "  333/10000: episode: 17, duration: 0.183s, episode steps:  11, steps per second:  60, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.238040, mae: 1.079675, mean_q: 1.844637, mean_eps: 0.500000\n",
            "  343/10000: episode: 18, duration: 0.174s, episode steps:  10, steps per second:  57, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.191979, mae: 1.104838, mean_q: 1.912407, mean_eps: 0.500000\n",
            "  358/10000: episode: 19, duration: 0.255s, episode steps:  15, steps per second:  59, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.218033, mae: 1.146637, mean_q: 2.058119, mean_eps: 0.500000\n",
            "  367/10000: episode: 20, duration: 0.172s, episode steps:   9, steps per second:  52, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.201514, mae: 1.204230, mean_q: 2.151903, mean_eps: 0.500000\n",
            "  381/10000: episode: 21, duration: 0.255s, episode steps:  14, steps per second:  55, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.214 [0.000, 1.000],  loss: 0.279962, mae: 1.277747, mean_q: 2.271408, mean_eps: 0.500000\n",
            "  389/10000: episode: 22, duration: 0.132s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.250344, mae: 1.305440, mean_q: 2.277427, mean_eps: 0.500000\n",
            "  403/10000: episode: 23, duration: 0.291s, episode steps:  14, steps per second:  48, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.354536, mae: 1.386234, mean_q: 2.334597, mean_eps: 0.500000\n",
            "  419/10000: episode: 24, duration: 0.330s, episode steps:  16, steps per second:  49, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 0.259396, mae: 1.387112, mean_q: 2.415412, mean_eps: 0.500000\n",
            "  432/10000: episode: 25, duration: 0.241s, episode steps:  13, steps per second:  54, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.294096, mae: 1.458908, mean_q: 2.617284, mean_eps: 0.500000\n",
            "  442/10000: episode: 26, duration: 0.158s, episode steps:  10, steps per second:  63, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.268343, mae: 1.459111, mean_q: 2.640504, mean_eps: 0.500000\n",
            "  454/10000: episode: 27, duration: 0.193s, episode steps:  12, steps per second:  62, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.397601, mae: 1.582620, mean_q: 2.798394, mean_eps: 0.500000\n",
            "  466/10000: episode: 28, duration: 0.205s, episode steps:  12, steps per second:  59, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.328776, mae: 1.595189, mean_q: 2.884877, mean_eps: 0.500000\n",
            "  477/10000: episode: 29, duration: 0.185s, episode steps:  11, steps per second:  60, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.286596, mae: 1.582934, mean_q: 2.875344, mean_eps: 0.500000\n",
            "  497/10000: episode: 30, duration: 0.347s, episode steps:  20, steps per second:  58, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.372986, mae: 1.673088, mean_q: 3.073008, mean_eps: 0.500000\n",
            "  510/10000: episode: 31, duration: 0.222s, episode steps:  13, steps per second:  58, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.154 [0.000, 1.000],  loss: 0.406543, mae: 1.722250, mean_q: 3.128890, mean_eps: 0.500000\n",
            "  520/10000: episode: 32, duration: 0.169s, episode steps:  10, steps per second:  59, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.441304, mae: 1.808402, mean_q: 3.229882, mean_eps: 0.500000\n",
            "  535/10000: episode: 33, duration: 0.235s, episode steps:  15, steps per second:  64, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 0.428532, mae: 1.835627, mean_q: 3.273916, mean_eps: 0.500000\n",
            "  544/10000: episode: 34, duration: 0.173s, episode steps:   9, steps per second:  52, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.371320, mae: 1.860045, mean_q: 3.371517, mean_eps: 0.500000\n",
            "  559/10000: episode: 35, duration: 0.377s, episode steps:  15, steps per second:  40, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.387974, mae: 1.907577, mean_q: 3.453246, mean_eps: 0.500000\n",
            "  571/10000: episode: 36, duration: 0.336s, episode steps:  12, steps per second:  36, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.393869, mae: 1.954232, mean_q: 3.600804, mean_eps: 0.500000\n",
            "  602/10000: episode: 37, duration: 0.869s, episode steps:  31, steps per second:  36, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.387 [0.000, 1.000],  loss: 0.438481, mae: 2.015462, mean_q: 3.791114, mean_eps: 0.500000\n",
            "  611/10000: episode: 38, duration: 0.256s, episode steps:   9, steps per second:  35, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.390170, mae: 2.060084, mean_q: 3.923185, mean_eps: 0.500000\n",
            "  624/10000: episode: 39, duration: 0.344s, episode steps:  13, steps per second:  38, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.077 [0.000, 1.000],  loss: 0.582426, mae: 2.158156, mean_q: 4.045883, mean_eps: 0.500000\n",
            "  633/10000: episode: 40, duration: 0.266s, episode steps:   9, steps per second:  34, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.413816, mae: 2.164362, mean_q: 4.027550, mean_eps: 0.500000\n",
            "  645/10000: episode: 41, duration: 0.339s, episode steps:  12, steps per second:  35, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.555285, mae: 2.238939, mean_q: 4.116087, mean_eps: 0.500000\n",
            "  655/10000: episode: 42, duration: 0.243s, episode steps:  10, steps per second:  41, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.574851, mae: 2.274442, mean_q: 4.143932, mean_eps: 0.500000\n",
            "  671/10000: episode: 43, duration: 0.299s, episode steps:  16, steps per second:  54, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 0.484201, mae: 2.305286, mean_q: 4.203350, mean_eps: 0.500000\n",
            "  698/10000: episode: 44, duration: 0.453s, episode steps:  27, steps per second:  60, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.407 [0.000, 1.000],  loss: 0.469624, mae: 2.388094, mean_q: 4.388654, mean_eps: 0.500000\n",
            "  713/10000: episode: 45, duration: 0.279s, episode steps:  15, steps per second:  54, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.451093, mae: 2.454426, mean_q: 4.534385, mean_eps: 0.500000\n",
            "  723/10000: episode: 46, duration: 0.302s, episode steps:  10, steps per second:  33, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.432307, mae: 2.466017, mean_q: 4.630248, mean_eps: 0.500000\n",
            "  735/10000: episode: 47, duration: 0.324s, episode steps:  12, steps per second:  37, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.350870, mae: 2.476733, mean_q: 4.731912, mean_eps: 0.500000\n",
            "  747/10000: episode: 48, duration: 0.371s, episode steps:  12, steps per second:  32, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.083 [0.000, 1.000],  loss: 0.501106, mae: 2.544450, mean_q: 4.825864, mean_eps: 0.500000\n",
            "  758/10000: episode: 49, duration: 0.226s, episode steps:  11, steps per second:  49, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.399794, mae: 2.545437, mean_q: 4.907576, mean_eps: 0.500000\n",
            "  784/10000: episode: 50, duration: 0.498s, episode steps:  26, steps per second:  52, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.510887, mae: 2.653441, mean_q: 4.943847, mean_eps: 0.500000\n",
            "  795/10000: episode: 51, duration: 0.180s, episode steps:  11, steps per second:  61, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.406635, mae: 2.701585, mean_q: 5.080875, mean_eps: 0.500000\n",
            "  811/10000: episode: 52, duration: 0.280s, episode steps:  16, steps per second:  57, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.431226, mae: 2.738447, mean_q: 5.195483, mean_eps: 0.500000\n",
            "  827/10000: episode: 53, duration: 0.267s, episode steps:  16, steps per second:  60, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.510039, mae: 2.795648, mean_q: 5.268312, mean_eps: 0.500000\n",
            "  838/10000: episode: 54, duration: 0.176s, episode steps:  11, steps per second:  63, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.513647, mae: 2.852392, mean_q: 5.355454, mean_eps: 0.500000\n",
            "  860/10000: episode: 55, duration: 0.366s, episode steps:  22, steps per second:  60, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.500091, mae: 2.880642, mean_q: 5.407494, mean_eps: 0.500000\n",
            "  871/10000: episode: 56, duration: 0.234s, episode steps:  11, steps per second:  47, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.516191, mae: 2.951241, mean_q: 5.530659, mean_eps: 0.500000\n",
            "  883/10000: episode: 57, duration: 0.184s, episode steps:  12, steps per second:  65, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.495662, mae: 2.981760, mean_q: 5.633386, mean_eps: 0.500000\n",
            "  903/10000: episode: 58, duration: 0.323s, episode steps:  20, steps per second:  62, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 0.501992, mae: 3.030832, mean_q: 5.614263, mean_eps: 0.500000\n",
            "  922/10000: episode: 59, duration: 0.310s, episode steps:  19, steps per second:  61, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 0.546213, mae: 3.075513, mean_q: 5.753436, mean_eps: 0.500000\n",
            "  938/10000: episode: 60, duration: 0.479s, episode steps:  16, steps per second:  33, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.438387, mae: 3.119510, mean_q: 5.847322, mean_eps: 0.500000\n",
            "  950/10000: episode: 61, duration: 0.327s, episode steps:  12, steps per second:  37, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.508583, mae: 3.157672, mean_q: 5.860372, mean_eps: 0.500000\n",
            "  962/10000: episode: 62, duration: 0.306s, episode steps:  12, steps per second:  39, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.568741, mae: 3.195383, mean_q: 5.948012, mean_eps: 0.500000\n",
            "  972/10000: episode: 63, duration: 0.283s, episode steps:  10, steps per second:  35, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.360977, mae: 3.185484, mean_q: 6.005473, mean_eps: 0.500000\n",
            " 1002/10000: episode: 64, duration: 0.728s, episode steps:  30, steps per second:  41, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.559744, mae: 3.302099, mean_q: 6.202052, mean_eps: 0.500000\n",
            " 1025/10000: episode: 65, duration: 0.383s, episode steps:  23, steps per second:  60, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  loss: 0.495849, mae: 3.369516, mean_q: 6.322378, mean_eps: 0.500000\n",
            " 1038/10000: episode: 66, duration: 0.245s, episode steps:  13, steps per second:  53, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.592192, mae: 3.449685, mean_q: 6.402994, mean_eps: 0.500000\n",
            " 1055/10000: episode: 67, duration: 0.296s, episode steps:  17, steps per second:  58, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.636509, mae: 3.514534, mean_q: 6.562129, mean_eps: 0.500000\n",
            " 1071/10000: episode: 68, duration: 0.280s, episode steps:  16, steps per second:  57, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.625085, mae: 3.555176, mean_q: 6.621096, mean_eps: 0.500000\n",
            " 1133/10000: episode: 69, duration: 0.980s, episode steps:  62, steps per second:  63, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.629 [0.000, 1.000],  loss: 0.712622, mae: 3.657847, mean_q: 6.820805, mean_eps: 0.500000\n",
            " 1145/10000: episode: 70, duration: 0.199s, episode steps:  12, steps per second:  60, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.635586, mae: 3.744953, mean_q: 6.946470, mean_eps: 0.500000\n",
            " 1157/10000: episode: 71, duration: 0.192s, episode steps:  12, steps per second:  63, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.797327, mae: 3.793729, mean_q: 7.098016, mean_eps: 0.500000\n",
            " 1167/10000: episode: 72, duration: 0.179s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.928594, mae: 3.895587, mean_q: 7.268488, mean_eps: 0.500000\n",
            " 1178/10000: episode: 73, duration: 0.183s, episode steps:  11, steps per second:  60, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.872362, mae: 3.927424, mean_q: 7.360471, mean_eps: 0.500000\n",
            " 1190/10000: episode: 74, duration: 0.181s, episode steps:  12, steps per second:  66, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.603066, mae: 3.918427, mean_q: 7.422847, mean_eps: 0.500000\n",
            " 1203/10000: episode: 75, duration: 0.210s, episode steps:  13, steps per second:  62, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.650324, mae: 3.953874, mean_q: 7.557949, mean_eps: 0.500000\n",
            " 1222/10000: episode: 76, duration: 0.283s, episode steps:  19, steps per second:  67, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.737 [0.000, 1.000],  loss: 0.866754, mae: 4.085646, mean_q: 7.863612, mean_eps: 0.500000\n",
            " 1234/10000: episode: 77, duration: 0.181s, episode steps:  12, steps per second:  66, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 1.084072, mae: 4.102970, mean_q: 7.715005, mean_eps: 0.500000\n",
            " 1247/10000: episode: 78, duration: 0.200s, episode steps:  13, steps per second:  65, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.911369, mae: 4.179918, mean_q: 7.923203, mean_eps: 0.500000\n",
            " 1257/10000: episode: 79, duration: 0.160s, episode steps:  10, steps per second:  63, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 1.011473, mae: 4.219813, mean_q: 7.976795, mean_eps: 0.500000\n",
            " 1267/10000: episode: 80, duration: 0.168s, episode steps:  10, steps per second:  60, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 1.336585, mae: 4.221075, mean_q: 7.910644, mean_eps: 0.500000\n",
            " 1275/10000: episode: 81, duration: 0.127s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.875 [0.000, 1.000],  loss: 0.737364, mae: 4.166422, mean_q: 7.932453, mean_eps: 0.500000\n",
            " 1284/10000: episode: 82, duration: 0.169s, episode steps:   9, steps per second:  53, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.430423, mae: 4.347631, mean_q: 8.138055, mean_eps: 0.500000\n",
            " 1303/10000: episode: 83, duration: 0.313s, episode steps:  19, steps per second:  61, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.684 [0.000, 1.000],  loss: 1.005997, mae: 4.273574, mean_q: 8.070448, mean_eps: 0.500000\n",
            " 1312/10000: episode: 84, duration: 0.158s, episode steps:   9, steps per second:  57, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.705582, mae: 4.417480, mean_q: 8.318970, mean_eps: 0.500000\n",
            " 1328/10000: episode: 85, duration: 0.302s, episode steps:  16, steps per second:  53, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 1.139482, mae: 4.387605, mean_q: 8.236846, mean_eps: 0.500000\n",
            " 1342/10000: episode: 86, duration: 0.251s, episode steps:  14, steps per second:  56, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 1.436854, mae: 4.453648, mean_q: 8.322553, mean_eps: 0.500000\n",
            " 1360/10000: episode: 87, duration: 0.295s, episode steps:  18, steps per second:  61, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 1.032780, mae: 4.490618, mean_q: 8.517203, mean_eps: 0.500000\n",
            " 1378/10000: episode: 88, duration: 0.282s, episode steps:  18, steps per second:  64, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.065148, mae: 4.524305, mean_q: 8.642169, mean_eps: 0.500000\n",
            " 1399/10000: episode: 89, duration: 0.351s, episode steps:  21, steps per second:  60, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.357219, mae: 4.609924, mean_q: 8.737984, mean_eps: 0.500000\n",
            " 1418/10000: episode: 90, duration: 0.295s, episode steps:  19, steps per second:  65, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 1.381675, mae: 4.682043, mean_q: 8.806221, mean_eps: 0.500000\n",
            " 1429/10000: episode: 91, duration: 0.168s, episode steps:  11, steps per second:  65, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 1.153835, mae: 4.751097, mean_q: 9.021880, mean_eps: 0.500000\n",
            " 1447/10000: episode: 92, duration: 0.418s, episode steps:  18, steps per second:  43, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.511418, mae: 4.847790, mean_q: 9.224009, mean_eps: 0.500000\n",
            " 1459/10000: episode: 93, duration: 0.293s, episode steps:  12, steps per second:  41, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 2.039545, mae: 4.916860, mean_q: 9.228423, mean_eps: 0.500000\n",
            " 1475/10000: episode: 94, duration: 0.404s, episode steps:  16, steps per second:  40, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 1.406098, mae: 4.890330, mean_q: 9.277640, mean_eps: 0.500000\n",
            " 1487/10000: episode: 95, duration: 0.310s, episode steps:  12, steps per second:  39, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 2.077474, mae: 4.987439, mean_q: 9.223260, mean_eps: 0.500000\n",
            " 1495/10000: episode: 96, duration: 0.207s, episode steps:   8, steps per second:  39, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.519854, mae: 4.918958, mean_q: 9.153208, mean_eps: 0.500000\n",
            " 1509/10000: episode: 97, duration: 0.305s, episode steps:  14, steps per second:  46, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 2.436748, mae: 5.037722, mean_q: 9.168056, mean_eps: 0.500000\n",
            " 1522/10000: episode: 98, duration: 0.249s, episode steps:  13, steps per second:  52, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.846 [0.000, 1.000],  loss: 1.239448, mae: 4.953473, mean_q: 9.277354, mean_eps: 0.500000\n",
            " 1537/10000: episode: 99, duration: 0.240s, episode steps:  15, steps per second:  63, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 1.209714, mae: 5.036433, mean_q: 9.592891, mean_eps: 0.500000\n",
            " 1551/10000: episode: 100, duration: 0.224s, episode steps:  14, steps per second:  63, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 1.478312, mae: 5.110566, mean_q: 9.730345, mean_eps: 0.500000\n",
            " 1562/10000: episode: 101, duration: 0.177s, episode steps:  11, steps per second:  62, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 1.408313, mae: 5.134584, mean_q: 9.845422, mean_eps: 0.500000\n",
            " 1585/10000: episode: 102, duration: 0.372s, episode steps:  23, steps per second:  62, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 1.694684, mae: 5.227388, mean_q: 9.919294, mean_eps: 0.500000\n",
            " 1603/10000: episode: 103, duration: 0.301s, episode steps:  18, steps per second:  60, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 2.059031, mae: 5.390554, mean_q: 10.025925, mean_eps: 0.500000\n",
            " 1615/10000: episode: 104, duration: 0.194s, episode steps:  12, steps per second:  62, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 2.074240, mae: 5.464472, mean_q: 10.113112, mean_eps: 0.500000\n",
            " 1628/10000: episode: 105, duration: 0.203s, episode steps:  13, steps per second:  64, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 2.288993, mae: 5.439756, mean_q: 10.103422, mean_eps: 0.500000\n",
            " 1640/10000: episode: 106, duration: 0.179s, episode steps:  12, steps per second:  67, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 1.624452, mae: 5.325384, mean_q: 9.964922, mean_eps: 0.500000\n",
            " 1650/10000: episode: 107, duration: 0.334s, episode steps:  10, steps per second:  30, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 2.065258, mae: 5.430836, mean_q: 10.091526, mean_eps: 0.500000\n",
            " 1734/10000: episode: 108, duration: 1.567s, episode steps:  84, steps per second:  54, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 1.965490, mae: 5.486765, mean_q: 10.116167, mean_eps: 0.500000\n",
            " 1746/10000: episode: 109, duration: 0.190s, episode steps:  12, steps per second:  63, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.283591, mae: 5.520367, mean_q: 10.528546, mean_eps: 0.500000\n",
            " 1757/10000: episode: 110, duration: 0.290s, episode steps:  11, steps per second:  38, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 1.902721, mae: 5.628677, mean_q: 10.618608, mean_eps: 0.500000\n",
            " 1768/10000: episode: 111, duration: 0.282s, episode steps:  11, steps per second:  39, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 1.528463, mae: 5.615473, mean_q: 10.650309, mean_eps: 0.500000\n",
            " 1780/10000: episode: 112, duration: 0.295s, episode steps:  12, steps per second:  41, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 1.989170, mae: 5.656650, mean_q: 10.659646, mean_eps: 0.500000\n",
            " 1796/10000: episode: 113, duration: 0.366s, episode steps:  16, steps per second:  44, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 1.627859, mae: 5.706671, mean_q: 10.829069, mean_eps: 0.500000\n",
            " 1808/10000: episode: 114, duration: 0.313s, episode steps:  12, steps per second:  38, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 1.490084, mae: 5.650639, mean_q: 10.698303, mean_eps: 0.500000\n",
            " 1831/10000: episode: 115, duration: 0.490s, episode steps:  23, steps per second:  47, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.739 [0.000, 1.000],  loss: 1.920361, mae: 5.789697, mean_q: 10.843156, mean_eps: 0.500000\n",
            " 1861/10000: episode: 116, duration: 0.641s, episode steps:  30, steps per second:  47, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 2.293587, mae: 5.799533, mean_q: 10.605151, mean_eps: 0.500000\n",
            " 1917/10000: episode: 117, duration: 1.070s, episode steps:  56, steps per second:  52, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 1.594159, mae: 5.863517, mean_q: 11.063483, mean_eps: 0.500000\n",
            " 1966/10000: episode: 118, duration: 1.177s, episode steps:  49, steps per second:  42, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 1.802165, mae: 6.010779, mean_q: 11.262066, mean_eps: 0.500000\n",
            " 1997/10000: episode: 119, duration: 0.546s, episode steps:  31, steps per second:  57, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 2.388062, mae: 6.084656, mean_q: 11.268996, mean_eps: 0.500000\n",
            " 2028/10000: episode: 120, duration: 0.477s, episode steps:  31, steps per second:  65, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 1.639723, mae: 6.056834, mean_q: 11.359227, mean_eps: 0.500000\n",
            " 2076/10000: episode: 121, duration: 0.956s, episode steps:  48, steps per second:  50, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 1.926237, mae: 6.202960, mean_q: 11.620039, mean_eps: 0.500000\n",
            " 2114/10000: episode: 122, duration: 0.841s, episode steps:  38, steps per second:  45, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.446608, mae: 6.259061, mean_q: 11.887888, mean_eps: 0.500000\n",
            " 2167/10000: episode: 123, duration: 1.136s, episode steps:  53, steps per second:  47, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1.946330, mae: 6.405512, mean_q: 12.010304, mean_eps: 0.500000\n",
            " 2248/10000: episode: 124, duration: 1.796s, episode steps:  81, steps per second:  45, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 1.943292, mae: 6.540280, mean_q: 12.317051, mean_eps: 0.500000\n",
            " 2266/10000: episode: 125, duration: 0.389s, episode steps:  18, steps per second:  46, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.980503, mae: 6.592023, mean_q: 12.446211, mean_eps: 0.500000\n",
            " 2302/10000: episode: 126, duration: 0.860s, episode steps:  36, steps per second:  42, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1.748534, mae: 6.635250, mean_q: 12.578990, mean_eps: 0.500000\n",
            " 2399/10000: episode: 127, duration: 1.906s, episode steps:  97, steps per second:  51, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 1.973294, mae: 6.789047, mean_q: 12.854500, mean_eps: 0.500000\n",
            " 2428/10000: episode: 128, duration: 0.408s, episode steps:  29, steps per second:  71, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 1.878790, mae: 6.875741, mean_q: 13.112861, mean_eps: 0.500000\n",
            " 2443/10000: episode: 129, duration: 0.339s, episode steps:  15, steps per second:  44, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 2.878207, mae: 7.011499, mean_q: 13.127277, mean_eps: 0.500000\n",
            " 2463/10000: episode: 130, duration: 0.452s, episode steps:  20, steps per second:  44, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 1.895036, mae: 6.972703, mean_q: 13.203735, mean_eps: 0.500000\n",
            " 2500/10000: episode: 131, duration: 0.848s, episode steps:  37, steps per second:  44, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 2.066463, mae: 7.093891, mean_q: 13.540971, mean_eps: 0.500000\n",
            " 2542/10000: episode: 132, duration: 1.041s, episode steps:  42, steps per second:  40, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.348459, mae: 7.142698, mean_q: 13.506799, mean_eps: 0.500000\n",
            " 2620/10000: episode: 133, duration: 1.585s, episode steps:  78, steps per second:  49, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 1.894607, mae: 7.265612, mean_q: 13.911545, mean_eps: 0.500000\n",
            " 2669/10000: episode: 134, duration: 0.764s, episode steps:  49, steps per second:  64, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 1.917854, mae: 7.485152, mean_q: 14.446363, mean_eps: 0.500000\n",
            " 2759/10000: episode: 135, duration: 1.326s, episode steps:  90, steps per second:  68, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 2.308195, mae: 7.600491, mean_q: 14.559011, mean_eps: 0.500000\n",
            " 2776/10000: episode: 136, duration: 0.271s, episode steps:  17, steps per second:  63, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.327673, mae: 7.733749, mean_q: 14.768273, mean_eps: 0.500000\n",
            " 2811/10000: episode: 137, duration: 0.512s, episode steps:  35, steps per second:  68, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 2.252003, mae: 7.809639, mean_q: 14.999977, mean_eps: 0.500000\n",
            " 2883/10000: episode: 138, duration: 1.534s, episode steps:  72, steps per second:  47, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 2.482527, mae: 7.928241, mean_q: 15.164005, mean_eps: 0.500000\n",
            " 2928/10000: episode: 139, duration: 0.834s, episode steps:  45, steps per second:  54, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 3.113145, mae: 8.060630, mean_q: 15.325578, mean_eps: 0.500000\n",
            " 2968/10000: episode: 140, duration: 0.554s, episode steps:  40, steps per second:  72, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.914765, mae: 8.117174, mean_q: 15.437508, mean_eps: 0.500000\n",
            " 3013/10000: episode: 141, duration: 0.692s, episode steps:  45, steps per second:  65, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 2.432271, mae: 8.209638, mean_q: 15.784352, mean_eps: 0.500000\n",
            " 3032/10000: episode: 142, duration: 0.273s, episode steps:  19, steps per second:  70, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2.347188, mae: 8.191838, mean_q: 15.759804, mean_eps: 0.500000\n",
            " 3064/10000: episode: 143, duration: 0.477s, episode steps:  32, steps per second:  67, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.408217, mae: 8.299023, mean_q: 15.977224, mean_eps: 0.500000\n",
            " 3111/10000: episode: 144, duration: 0.701s, episode steps:  47, steps per second:  67, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 3.025793, mae: 8.475776, mean_q: 16.273576, mean_eps: 0.500000\n",
            " 3167/10000: episode: 145, duration: 1.095s, episode steps:  56, steps per second:  51, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.179144, mae: 8.504645, mean_q: 16.446598, mean_eps: 0.500000\n",
            " 3211/10000: episode: 146, duration: 0.896s, episode steps:  44, steps per second:  49, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 2.740594, mae: 8.654113, mean_q: 16.643084, mean_eps: 0.500000\n",
            " 3264/10000: episode: 147, duration: 1.155s, episode steps:  53, steps per second:  46, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 3.215834, mae: 8.754767, mean_q: 16.781058, mean_eps: 0.500000\n",
            " 3332/10000: episode: 148, duration: 1.402s, episode steps:  68, steps per second:  48, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 3.039068, mae: 8.920728, mean_q: 17.202307, mean_eps: 0.500000\n",
            " 3376/10000: episode: 149, duration: 0.896s, episode steps:  44, steps per second:  49, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 3.121977, mae: 9.038587, mean_q: 17.320571, mean_eps: 0.500000\n",
            " 3440/10000: episode: 150, duration: 1.402s, episode steps:  64, steps per second:  46, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 2.775446, mae: 9.128634, mean_q: 17.692890, mean_eps: 0.500000\n",
            " 3481/10000: episode: 151, duration: 1.240s, episode steps:  41, steps per second:  33, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 2.858717, mae: 9.336973, mean_q: 18.032827, mean_eps: 0.500000\n",
            " 3516/10000: episode: 152, duration: 0.711s, episode steps:  35, steps per second:  49, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 2.826212, mae: 9.270912, mean_q: 17.929470, mean_eps: 0.500000\n",
            " 3572/10000: episode: 153, duration: 0.959s, episode steps:  56, steps per second:  58, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 3.208004, mae: 9.479639, mean_q: 18.289174, mean_eps: 0.500000\n",
            " 3618/10000: episode: 154, duration: 0.693s, episode steps:  46, steps per second:  66, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 2.727222, mae: 9.565174, mean_q: 18.587409, mean_eps: 0.500000\n",
            " 3716/10000: episode: 155, duration: 1.398s, episode steps:  98, steps per second:  70, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.506200, mae: 9.722533, mean_q: 18.780982, mean_eps: 0.500000\n",
            " 3831/10000: episode: 156, duration: 1.710s, episode steps: 115, steps per second:  67, episode reward: 115.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 3.353436, mae: 9.963120, mean_q: 19.320143, mean_eps: 0.500000\n",
            " 3858/10000: episode: 157, duration: 0.424s, episode steps:  27, steps per second:  64, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 3.772268, mae: 10.144580, mean_q: 19.732934, mean_eps: 0.500000\n",
            " 3896/10000: episode: 158, duration: 0.711s, episode steps:  38, steps per second:  53, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2.976638, mae: 10.133465, mean_q: 19.706625, mean_eps: 0.500000\n",
            " 3948/10000: episode: 159, duration: 1.167s, episode steps:  52, steps per second:  45, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.561782, mae: 10.279828, mean_q: 19.950383, mean_eps: 0.500000\n",
            " 4035/10000: episode: 160, duration: 1.993s, episode steps:  87, steps per second:  44, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 3.305196, mae: 10.436005, mean_q: 20.328518, mean_eps: 0.500000\n",
            " 4051/10000: episode: 161, duration: 0.298s, episode steps:  16, steps per second:  54, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 2.681956, mae: 10.567232, mean_q: 20.683341, mean_eps: 0.500000\n",
            " 4102/10000: episode: 162, duration: 1.093s, episode steps:  51, steps per second:  47, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 4.214653, mae: 10.632950, mean_q: 20.625105, mean_eps: 0.500000\n",
            " 4162/10000: episode: 163, duration: 1.129s, episode steps:  60, steps per second:  53, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.607725, mae: 10.749162, mean_q: 21.113401, mean_eps: 0.500000\n",
            " 4263/10000: episode: 164, duration: 1.725s, episode steps: 101, steps per second:  59, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 3.607342, mae: 10.975623, mean_q: 21.460591, mean_eps: 0.500000\n",
            " 4350/10000: episode: 165, duration: 1.494s, episode steps:  87, steps per second:  58, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 3.705646, mae: 11.184594, mean_q: 21.858941, mean_eps: 0.500000\n",
            " 4386/10000: episode: 166, duration: 0.550s, episode steps:  36, steps per second:  65, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 3.956678, mae: 11.437330, mean_q: 22.365322, mean_eps: 0.500000\n",
            " 4432/10000: episode: 167, duration: 0.872s, episode steps:  46, steps per second:  53, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 4.395997, mae: 11.329960, mean_q: 21.967248, mean_eps: 0.500000\n",
            " 4460/10000: episode: 168, duration: 0.633s, episode steps:  28, steps per second:  44, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 3.874563, mae: 11.533261, mean_q: 22.590599, mean_eps: 0.500000\n",
            " 4498/10000: episode: 169, duration: 0.932s, episode steps:  38, steps per second:  41, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  loss: 4.325065, mae: 11.578956, mean_q: 22.663689, mean_eps: 0.500000\n",
            " 4524/10000: episode: 170, duration: 0.503s, episode steps:  26, steps per second:  52, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.577 [0.000, 1.000],  loss: 4.015435, mae: 11.591238, mean_q: 22.626279, mean_eps: 0.500000\n",
            " 4569/10000: episode: 171, duration: 0.730s, episode steps:  45, steps per second:  62, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 3.765877, mae: 11.755490, mean_q: 23.077531, mean_eps: 0.500000\n",
            " 4632/10000: episode: 172, duration: 0.958s, episode steps:  63, steps per second:  66, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 3.283649, mae: 11.808557, mean_q: 23.284872, mean_eps: 0.500000\n",
            " 4724/10000: episode: 173, duration: 2.091s, episode steps:  92, steps per second:  44, episode reward: 92.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 5.537451, mae: 12.080147, mean_q: 23.535585, mean_eps: 0.500000\n",
            " 4807/10000: episode: 174, duration: 1.385s, episode steps:  83, steps per second:  60, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 6.354180, mae: 12.236513, mean_q: 23.706413, mean_eps: 0.500000\n",
            " 4943/10000: episode: 175, duration: 3.090s, episode steps: 136, steps per second:  44, episode reward: 136.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 5.017347, mae: 12.361588, mean_q: 24.122144, mean_eps: 0.500000\n",
            " 5000/10000: episode: 176, duration: 1.386s, episode steps:  57, steps per second:  41, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  loss: 4.901691, mae: 12.591072, mean_q: 24.641080, mean_eps: 0.500000\n",
            " 5046/10000: episode: 177, duration: 0.851s, episode steps:  46, steps per second:  54, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.893187, mae: 12.654595, mean_q: 24.883611, mean_eps: 0.500000\n",
            " 5097/10000: episode: 178, duration: 0.973s, episode steps:  51, steps per second:  52, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.549 [0.000, 1.000],  loss: 6.406675, mae: 12.791982, mean_q: 24.920114, mean_eps: 0.500000\n",
            " 5163/10000: episode: 179, duration: 1.225s, episode steps:  66, steps per second:  54, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 5.739831, mae: 12.838235, mean_q: 25.049102, mean_eps: 0.500000\n",
            " 5295/10000: episode: 180, duration: 2.058s, episode steps: 132, steps per second:  64, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 4.683533, mae: 12.985238, mean_q: 25.532184, mean_eps: 0.500000\n",
            " 5377/10000: episode: 181, duration: 2.052s, episode steps:  82, steps per second:  40, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 5.848711, mae: 13.252131, mean_q: 25.977221, mean_eps: 0.500000\n",
            " 5424/10000: episode: 182, duration: 0.827s, episode steps:  47, steps per second:  57, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 5.631607, mae: 13.412614, mean_q: 26.319096, mean_eps: 0.500000\n",
            " 5527/10000: episode: 183, duration: 2.133s, episode steps: 103, steps per second:  48, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 5.704859, mae: 13.514249, mean_q: 26.512635, mean_eps: 0.500000\n",
            " 5597/10000: episode: 184, duration: 1.220s, episode steps:  70, steps per second:  57, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 5.636866, mae: 13.577099, mean_q: 26.650322, mean_eps: 0.500000\n",
            " 5706/10000: episode: 185, duration: 2.106s, episode steps: 109, steps per second:  52, episode reward: 109.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 4.372719, mae: 13.782834, mean_q: 27.309500, mean_eps: 0.500000\n",
            " 5724/10000: episode: 186, duration: 0.411s, episode steps:  18, steps per second:  44, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.737228, mae: 14.181449, mean_q: 27.763862, mean_eps: 0.500000\n",
            " 5785/10000: episode: 187, duration: 1.440s, episode steps:  61, steps per second:  42, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 6.186137, mae: 14.095475, mean_q: 27.694557, mean_eps: 0.500000\n",
            " 5857/10000: episode: 188, duration: 1.879s, episode steps:  72, steps per second:  38, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 6.151392, mae: 14.193858, mean_q: 27.919092, mean_eps: 0.500000\n",
            " 5898/10000: episode: 189, duration: 0.869s, episode steps:  41, steps per second:  47, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 6.840373, mae: 14.391105, mean_q: 28.273422, mean_eps: 0.500000\n",
            " 5956/10000: episode: 190, duration: 1.243s, episode steps:  58, steps per second:  47, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 5.245585, mae: 14.399544, mean_q: 28.480592, mean_eps: 0.500000\n",
            " 6033/10000: episode: 191, duration: 1.319s, episode steps:  77, steps per second:  58, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 5.842643, mae: 14.596228, mean_q: 28.883736, mean_eps: 0.500000\n",
            " 6062/10000: episode: 192, duration: 0.714s, episode steps:  29, steps per second:  41, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 5.793465, mae: 14.604327, mean_q: 28.777264, mean_eps: 0.500000\n",
            " 6115/10000: episode: 193, duration: 1.354s, episode steps:  53, steps per second:  39, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 7.447926, mae: 14.775569, mean_q: 29.055963, mean_eps: 0.500000\n",
            " 6193/10000: episode: 194, duration: 2.256s, episode steps:  78, steps per second:  35, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 7.761234, mae: 14.877696, mean_q: 29.251942, mean_eps: 0.500000\n",
            " 6274/10000: episode: 195, duration: 2.341s, episode steps:  81, steps per second:  35, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 5.361631, mae: 14.962412, mean_q: 29.631978, mean_eps: 0.500000\n",
            " 6382/10000: episode: 196, duration: 2.131s, episode steps: 108, steps per second:  51, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 5.917430, mae: 15.230291, mean_q: 30.192078, mean_eps: 0.500000\n",
            " 6402/10000: episode: 197, duration: 0.315s, episode steps:  20, steps per second:  64, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 7.598782, mae: 15.457938, mean_q: 30.402190, mean_eps: 0.500000\n",
            " 6570/10000: episode: 198, duration: 3.424s, episode steps: 168, steps per second:  49, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 7.314619, mae: 15.517238, mean_q: 30.551994, mean_eps: 0.500000\n",
            " 6730/10000: episode: 199, duration: 3.260s, episode steps: 160, steps per second:  49, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 7.192511, mae: 15.860366, mean_q: 31.346779, mean_eps: 0.500000\n",
            " 6814/10000: episode: 200, duration: 2.392s, episode steps:  84, steps per second:  35, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.862681, mae: 16.069781, mean_q: 31.571638, mean_eps: 0.500000\n",
            " 6889/10000: episode: 201, duration: 2.065s, episode steps:  75, steps per second:  36, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.453 [0.000, 1.000],  loss: 7.929102, mae: 16.165358, mean_q: 31.846374, mean_eps: 0.500000\n",
            " 6979/10000: episode: 202, duration: 2.424s, episode steps:  90, steps per second:  37, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 8.151763, mae: 16.302231, mean_q: 32.110661, mean_eps: 0.500000\n",
            " 7021/10000: episode: 203, duration: 1.146s, episode steps:  42, steps per second:  37, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 5.060981, mae: 16.372643, mean_q: 32.574042, mean_eps: 0.500000\n",
            " 7113/10000: episode: 204, duration: 1.943s, episode steps:  92, steps per second:  47, episode reward: 92.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 6.889000, mae: 16.533806, mean_q: 32.790883, mean_eps: 0.500000\n",
            " 7155/10000: episode: 205, duration: 1.247s, episode steps:  42, steps per second:  34, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 6.324815, mae: 16.643520, mean_q: 33.036682, mean_eps: 0.500000\n",
            " 7278/10000: episode: 206, duration: 3.447s, episode steps: 123, steps per second:  36, episode reward: 123.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 8.925638, mae: 16.865193, mean_q: 33.297287, mean_eps: 0.500000\n",
            " 7312/10000: episode: 207, duration: 0.815s, episode steps:  34, steps per second:  42, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.441 [0.000, 1.000],  loss: 7.788334, mae: 16.999402, mean_q: 33.644196, mean_eps: 0.500000\n",
            " 7341/10000: episode: 208, duration: 0.535s, episode steps:  29, steps per second:  54, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 7.445295, mae: 16.894563, mean_q: 33.500264, mean_eps: 0.500000\n",
            " 7394/10000: episode: 209, duration: 0.980s, episode steps:  53, steps per second:  54, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 9.007692, mae: 17.130849, mean_q: 33.812424, mean_eps: 0.500000\n",
            " 7459/10000: episode: 210, duration: 1.398s, episode steps:  65, steps per second:  46, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 7.704982, mae: 17.193390, mean_q: 34.009003, mean_eps: 0.500000\n",
            " 7569/10000: episode: 211, duration: 2.648s, episode steps: 110, steps per second:  42, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 7.261836, mae: 17.271407, mean_q: 34.263833, mean_eps: 0.500000\n",
            " 7685/10000: episode: 212, duration: 2.037s, episode steps: 116, steps per second:  57, episode reward: 116.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 7.898440, mae: 17.548446, mean_q: 34.787038, mean_eps: 0.500000\n",
            " 7786/10000: episode: 213, duration: 2.007s, episode steps: 101, steps per second:  50, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 7.594066, mae: 17.741193, mean_q: 35.255345, mean_eps: 0.500000\n",
            " 7891/10000: episode: 214, duration: 2.556s, episode steps: 105, steps per second:  41, episode reward: 105.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 8.577877, mae: 17.873787, mean_q: 35.422231, mean_eps: 0.500000\n",
            " 8034/10000: episode: 215, duration: 3.277s, episode steps: 143, steps per second:  44, episode reward: 143.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 8.472620, mae: 18.185480, mean_q: 36.050296, mean_eps: 0.500000\n",
            " 8092/10000: episode: 216, duration: 1.033s, episode steps:  58, steps per second:  56, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 9.982365, mae: 18.366293, mean_q: 36.424672, mean_eps: 0.500000\n",
            " 8166/10000: episode: 217, duration: 1.108s, episode steps:  74, steps per second:  67, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.859147, mae: 18.373797, mean_q: 36.444155, mean_eps: 0.500000\n",
            " 8211/10000: episode: 218, duration: 0.892s, episode steps:  45, steps per second:  50, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 5.619614, mae: 18.504698, mean_q: 36.862139, mean_eps: 0.500000\n",
            " 8411/10000: episode: 219, duration: 4.804s, episode steps: 200, steps per second:  42, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 9.277187, mae: 18.719097, mean_q: 37.149111, mean_eps: 0.500000\n",
            " 8549/10000: episode: 220, duration: 2.386s, episode steps: 138, steps per second:  58, episode reward: 138.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 7.858161, mae: 19.018771, mean_q: 37.833918, mean_eps: 0.500000\n",
            " 8678/10000: episode: 221, duration: 2.607s, episode steps: 129, steps per second:  49, episode reward: 129.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 10.158415, mae: 19.286729, mean_q: 38.290289, mean_eps: 0.500000\n",
            " 8832/10000: episode: 222, duration: 2.543s, episode steps: 154, steps per second:  61, episode reward: 154.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 8.512279, mae: 19.555697, mean_q: 38.918291, mean_eps: 0.500000\n",
            " 8930/10000: episode: 223, duration: 1.588s, episode steps:  98, steps per second:  62, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 8.307416, mae: 19.741539, mean_q: 39.286527, mean_eps: 0.500000\n",
            " 8998/10000: episode: 224, duration: 1.048s, episode steps:  68, steps per second:  65, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 8.859388, mae: 19.920367, mean_q: 39.650759, mean_eps: 0.500000\n",
            " 9058/10000: episode: 225, duration: 1.287s, episode steps:  60, steps per second:  47, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.433 [0.000, 1.000],  loss: 8.514935, mae: 19.968589, mean_q: 39.812488, mean_eps: 0.500000\n",
            " 9193/10000: episode: 226, duration: 2.186s, episode steps: 135, steps per second:  62, episode reward: 135.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 10.839299, mae: 20.149728, mean_q: 40.008311, mean_eps: 0.500000\n",
            " 9273/10000: episode: 227, duration: 1.522s, episode steps:  80, steps per second:  53, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 10.331968, mae: 20.416994, mean_q: 40.446804, mean_eps: 0.500000\n",
            " 9462/10000: episode: 228, duration: 3.957s, episode steps: 189, steps per second:  48, episode reward: 189.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 9.051730, mae: 20.582950, mean_q: 41.051325, mean_eps: 0.500000\n",
            " 9494/10000: episode: 229, duration: 0.830s, episode steps:  32, steps per second:  39, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 11.070083, mae: 20.737960, mean_q: 41.405538, mean_eps: 0.500000\n",
            " 9529/10000: episode: 230, duration: 0.636s, episode steps:  35, steps per second:  55, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 10.318280, mae: 20.757172, mean_q: 41.389764, mean_eps: 0.500000\n",
            " 9586/10000: episode: 231, duration: 0.902s, episode steps:  57, steps per second:  63, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.439 [0.000, 1.000],  loss: 11.744753, mae: 20.952231, mean_q: 41.737073, mean_eps: 0.500000\n",
            " 9603/10000: episode: 232, duration: 0.326s, episode steps:  17, steps per second:  52, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 9.006905, mae: 20.863786, mean_q: 41.491070, mean_eps: 0.500000\n",
            " 9713/10000: episode: 233, duration: 2.853s, episode steps: 110, steps per second:  39, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 8.676599, mae: 21.157065, mean_q: 42.308423, mean_eps: 0.500000\n",
            " 9736/10000: episode: 234, duration: 0.686s, episode steps:  23, steps per second:  34, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 12.175210, mae: 21.283808, mean_q: 42.211639, mean_eps: 0.500000\n",
            " 9815/10000: episode: 235, duration: 1.543s, episode steps:  79, steps per second:  51, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 12.848234, mae: 21.251370, mean_q: 42.214622, mean_eps: 0.500000\n",
            " 9907/10000: episode: 236, duration: 2.386s, episode steps:  92, steps per second:  39, episode reward: 92.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 10.422176, mae: 21.342746, mean_q: 42.506312, mean_eps: 0.500000\n",
            " 9968/10000: episode: 237, duration: 1.072s, episode steps:  61, steps per second:  57, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 10.051375, mae: 21.458636, mean_q: 42.814261, mean_eps: 0.500000\n",
            " 9980/10000: episode: 238, duration: 0.239s, episode steps:  12, steps per second:  50, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 15.161727, mae: 21.849584, mean_q: 43.316525, mean_eps: 0.500000\n",
            "done, took 217.151 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEGCAYAAABhMDI9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9eZgcV3nv/31r6Z6eTTMaLdZiSba8gA227AjbhN2EzQFDuIQ1hgA3hhsgcAlPQgg3JPcJIYEAT1iDuTg2BMxu8C8QMBjb2IAXeZNX2ZasZaSRZl9676o6vz/qnFOnqqt6memaTefzPPNMT3V116memfOe77sdYoxBo9FoNBqBsdQD0Gg0Gs3yQhsGjUaj0YTQhkGj0Wg0IbRh0Gg0Gk0IbRg0Go1GE8Ja6gEslHXr1rEdO3Ys9TA0Go1mRXHPPfeMM8bWxz234g3Djh07sGfPnqUehkaj0awoiOhQ0nPalaTRaDSaENowaDQajSaENgwajUajCaENg0aj0WhCaMOg0Wg0mhCpGgYiOpWIbiaiR4joYSJ6Pz++loh+QURP8O+D/DgR0eeI6Eki2ktEF6Y5Po1Go9HUk7ZicAD8JWPsHACXAHgPEZ0D4MMAbmKMnQngJv4zALwCwJn860oAX055fBqNRqOJkKphYIyNMMbu5Y/nADwKYAuAVwO4lp92LYDX8MevBvB15nMHgAEi2pTmGDUajabTlGsuvn/PMFbqtgaLFmMgoh0ALgBwJ4CNjLER/tRxABv54y0AjigvG+bHou91JRHtIaI9Y2NjqY1Zo9Fo5sPNj43iQ997APvHCks9lHmxKIaBiHoB/ADABxhjs+pzzDepbZlVxthVjLHdjLHd69fHVnRrNBrNklF1PQBAxXGXeCTzI3XDQEQ2fKPwTcbYD/nhE8JFxL+P8uNHAZyqvHwrP6bRaDQrBuFBqrnalVQHERGArwF4lDH2GeWpGwC8jT9+G4AfK8ffyrOTLgEwo7icNBqNZkXgccvgcOWw0ki7id5zAFwB4EEiup8f+wiAfwbwXSJ6J4BDAF7Pn/spgMsAPAmgCODtKY9Po9FoOo63whVDqoaBMXY7AEp4+sUx5zMA70lzTBqNRpM2UjF4K1Mx6MpnjUaj6TBMupJWpmLQhkGj0Wg6jAgt1FZojEEbBo1Go+kwgStJKwaNRqPRIHAlacWg0Wg0GgDtZyWVay7KteVTDKcNg0aj0XSYdusY3vut+/CR6x9Mc0htkXYdg0aj0Zx0SMXQYoxhdK6MimOnOKL20IpBo9FoOgxrUzG4HsNyasSqDYNGo9F0GK/NOgbXY/I1ywFtGDQajabDBK6k1hSDxxjcZZTaqg2DRqPRdBix+q85rU32HoN2JWk0Gs1qRkzyrfZK8rQrSaPRaFY3nicK3FqMMTBtGDQajWZVI8IF7WQlLaMQgzYMGo1G02na7ZXkeUymuC4HtGHQaDSaDtNuryTflZTmiNpDGwaNRqPpMG7bdQw4eWIMRHQ1EY0S0UPKse8Q0f3866DY8pOIdhBRSXnu39Mcm0aj0aRF0ESv9TqG5aQY0u6VdA2ALwD4ujjAGHuDeExEnwYwo5y/nzG2K+UxaTQaTarIOoYWZ3t3mcUY0t7z+ddEtCPuOSIiAK8HcGmaY9BoNJrFhrWZlaTrGAKeB+AEY+wJ5dhpRHQfEd1KRM9LeiERXUlEe4hoz9jYWPoj1Wg0mjaYXx1DmiNqj6U0DG8CcJ3y8wiAbYyxCwB8EMC3iKg/7oWMsasYY7sZY7vXr1+/CEPVaDSa1vHarHzWTfQAEJEF4LUAviOOMcYqjLEJ/vgeAPsBnLUU49NoNJqF0G53VY/pttsA8AcAHmOMDYsDRLSeiEz++HQAZwI4sETj02g0mnnTdh3DyaQYiOg6AL8DcDYRDRPRO/lTb0TYjQQAzwewl6evfh/Auxljk2mOT6PRaNKg/XTV5VXHkHZW0psSjv9pzLEfAPhBmuPRaDSaxaCdlhgiUN1iOGJR0JXPGo3mpKBUdXH17U8tyoY4gWJofi1RJb2c6hi0YdBoNCcFV//mKfzf/3oE3777cOrXamfPZ2GodLqqRqPRLDLdGRMA8OjIbOrXasuVxFjo+3JAGwaNRnNSsKGvCwDw1Hgh9Wu1E3zWikGj0WiWCAZ/5j04Xkz9Wu3UMYigs44xaDQazSIjVuRHp0vpX8trvY7B1a4kjUajWRrUFXmp6qZ6Le1K0mg0mhWAuiJPO86gg88ajUazAlALyPaP5VO9VtB2u4U6Bk/UMaQ5ovbQhkGj0ZwUuMrMe2Qq3QB0sFFPO66k5WMZtGHQaDQnBWqMoeakOwl7LFABzSqttStJo9Folgh1fnZTbkykXqtZANrVvZI0Go1maVBX5K3uxTxfQuqkiWEQQ9GKQaPRaBaZsGJI25UUPG4WgNauJI1Go1ki1FV8qzurzZewOvHgeQzlWnzthK5j0Gg0miVCVQmLGWNwXIbv7jmC5/7Lr2KVinpsubTF0IZBo9GcFIj5N2MaLRWeLYSoOnlqooDxfDU23qCqi+WiGtLe2vNqIholooeUY39PREeJ6H7+dZny3N8Q0ZNEtI+IXpbm2DQazcmFmKwzlrHorqRCxQEAVGMMg6oYlkucIW3FcA2Al8cc/yxjbBf/+ikAENE58PeCPpe/5ktEZKY8Po1Gc5IgJl3bpNQVg+qpclyGYsWVj5PGFX28lKRqGBhjvwYw2eLprwbwbcZYhTH2FIAnAVyU2uA0Gs1JhbAFtmksQowhnK6a54ohzpWkHlomdmHJYgzvJaK93NU0yI9tAXBEOWeYH6uDiK4koj1EtGdsbCztsWo0mlWAp7qSUk9XDRuGQpW7khztSkriywB2AtgFYATAp9t9A8bYVYyx3Yyx3evXr+/0+DQazSpE7JGQMY3FrWPwGPLClRRz3ZMu+BwHY+wEY8xljHkAvorAXXQUwKnKqVv5MY1Go1kwqiuptgjB54zpT68110OxoStJKwYQ0Sblxz8CIDKWbgDwRiLKEtFpAM4EcNdij0+j0axOVFfSYtQxZCx/enVcFmQlxbmSFGPAlkm/JCvNNyei6wC8EMA6IhoG8DEALySiXQAYgIMA3gUAjLGHiei7AB4B4AB4D2Ms3W2WNBrNSYNYmFuLkJXEGPMNQwVwvMbBZ28ZKoZUDQNj7E0xh7/W4PyPA/h4eiPSaDQnK4wxGATYxmLEGAJXUtVhKFSTYwwr2pVERO8non7y+RoR3UtEL01zcBqNRtMpPMZgEME0OqsY7jk0iQ9974FQtbPnAVnbn14LFUdO/rUYV5I6lJUYfH4HY2wWwEsBDAK4AsA/pzIqjUaj6TAeAwwi35XUpBV2O9z2xDi+f88wKsqkryqG6VJNHo+rfFZVwkrslUT8+2UAvsEYe1g5ptFoNMsaz2MgAkyDOupKEgbBCTXDC4LPM4phiKt8DruSOjasBdGOYbiHiG6Ebxh+TkR9AJZJDF2j0Wga4zEG0yBYHXYliXba0ViBNAzFqjzevIne8rAM7QSf3wm/KO0AY6xIREMA3p7OsDQajaazSFdSh4PPQjHUGYYWXUnLMfjcsmFgjHlEtAPAnxARA3A7Y+z6tAam0Wg0ncRj3JVkUtPtNtuhUhOupOA9VVfSdLF1V9IysQttZSV9CcC7ATwIvyjtXUT0xbQGptFoNJ2EScXQ2RhD2Yl3JfV32TANwpGpojy+Gl1JlwJ4OuNhcyK6Fn4xmkaj0bTFfz84gg39Wfze9rWLdk2P1zF0Ol1VKIZoEDljGdg80IWnxgvyeLPuqisx+PwkgG3Kz6cCeKKzw9FoNCcDn/z5Plz9m4OLek3XY6kohkqCYiACdgz1hNxDVcWVNFuu4f/ddiDUnmO5KIZ2DEMfgEeJ6BYiuhm+WugnohuI6IZ0hqfRaFYjVceDm3IjuygeA4gIVoe39gxiDGqBm2+Etq3tDp2r1k/c/Ngo/vEnj+LJ0bw8tlzqGNpxJf1daqPQaDQnFVXXS71fURTGGEwDfrpqJ4PPsYoBMLhiAPzHHgu7kkSH11LNDb1uOdBOVtKtRLQdwJmMsV8SUQ6AxRibS294Go1mNVJzvdgOpzPFGn6zfxyXPXNTzKsWRlotMcqxMQauGIZ8xdCfszFdrIVcScI4ideL1y0H2slK+jMA3wfwFX5oK4AfpTEojUazuqk58YrhhgeO4s+/eS+mCtWYVy2MoI5hMWIMvttKKIberFWnVMT9l1XFsExKhtuJMbwHwHMAzAIAY+wJABvSGJRGo1nd1FwWOzmL1bNIAe0kso7B6GyMoRwTYxBuKxFj6MlYfIOgYOYX9192VrBiAFBhjEkzTkQW/D0VNBqNpmUYY4kxhhpfMteczk8t3iJnJRlEyGVMbOzPoidrwjYptHNcrGJYgYbhViL6CIAcEb0EwPcA/H/pDEuj0axWxIQYNzmLyuCqm4Zi8IPAlukbhk5lAMW3xPDdVgCwe/ta7Fzfi4xlhFpiCLdSuCtrR4a0YNoxDB8GMAa/8vldAH7KGPvbRi8goquJaJSIHlKOfYqIHiOivUR0PREN8OM7iKhERPfzr3+fx/1oNJpljnCnxCkGMVlW01AMjMHgTfSSrt8ujDG54nci9QjcLuALb74An/rj82EZRmyMobLCFcP7GGNfZYz9MWPsdYyxrxLR+5u85hoAL48c+wWAZzDGzgPwOIC/UZ7bzxjbxb/e3cbYNBrNCkG4ieKyksRk2cleRgLREsM0DH79hU/CjsfkKj/a80goBuLfbSvsShLnq4phudQxtGMY3hZz7E8bvYAx9msAk5FjNzLGHP7jHfCzmzQazUmCcKfENZQThiGuC+lCES0xOqkY1PhAfYwhfK5tRlxJsTGGBQ+pIzStYyCiNwF4M4DTIhXO/YhM+vPgHQC+o/x8GhHdBz/z6aOMsdsSxnQlgCsBYNu2bXGnaDSaZUojV5J4Lm4LzIWi1jEA6Ejltbrajws+q9hRV5KsY1DTVZeHZWilwO23AEYArAPwaeX4HIC9870wEf0tAAfAN/mhEQDbGGMTRPR7AH5EROfy7URDMMauAnAVAOzevXt5fJIajaYlxOTfKPhcSUExuJ7v1rFNf8KuNSgaGJ0rY7A7A9ts7FRRDYMTCT5T1DAkuJLCBW4t3Mgi0NSVxBg7xBi7BcAfALiNMXYr/El8K+a5tScR/SmAVwJ4i+jWyhirMMYm+ON7AOwHcNZ83l+j0SxfAsUQF2NITzEw2V21cYyh5np48b/eiu/fM9z0PZNcSSzBlVSLcyUpNRsrMcbwawBdRLQFwI0AroAfXG4LIno5gL8CcDljrKgcX09EJn98OoAzARxo9/01Gs3yRmQcxblyxIq6lkKDPXVrTyA5xlB1PMxVHEzkK03fs1JLciWh3pUUNQz8sWoLVoxiUCA+kb8WwJcYY38M4NyGLyC6DsDvAJxNRMNE9E4AX4DfqfUXkbTU5wPYS0T3w2+98W7G2EJjGBqNZpnRKMbgyuBzOnUM1EKMQYyrleC0utpvHnyOL3ALj3F5WIZ2uqsSET0bwFvg7/8MAGajFzDG3hRz+GsJ5/4AwA/aGI9Go1mBNIoxBMHnlOoYeIEbEO/KUsfVSiBYVQxiomeMgcXFGEwDc2VH/hx3/8vFMLSjGN4Pv+bgesbYw9zdc3M6w9JoNKuVasMCt/TSVYOtPY3E6/vHk8cXpRJSDP7rxMviXEnVhGC1OsblQDttt38NP84gfj4A4C/Ez0T0ecbY+zo7PI1Gs9oQ7pTYrCRPVD6nkZUUbO0JxNdRqONqpQCuHIox+N/Fqj/OlaQag7g9IVaiYmjGczr4XhqNZpUiMo7iXDlB8DmdOgaiIPicNPE7DQxXlHjFwA2D0ST4HOtKanrJRaGThkGj0Wia0rCOIUXFwBhgEsFsMcaQ5Eq67/AUnvXxX2I8X0mIMfg/U1y6akJBnGA1KgaNRrMKGJ4q4rn/8isMTxWbnzwPGsUY0lYMhoHmikEEnxMm6ZsfG8XYXAX7R/MRxRB+XWy6qupKio0xrD7DMK9iN41Gs7x4dGQOw1MlHJpIxzCIyZ+x+syfIF01raykIPicVCvRTDE8MDwDAJgsVGNbYoiXmXWGgWLrGMJjbOVO0qdtw0BE3QlP/dsCx6LRaJYBU0V/P65O7nKm0sjPHrTdTkMx+CmkIl01WTFwV1eM4WCMYe/wNABgolANVT5HlUYzV5J67yIcseJcSUT0+0T0CIDH+M/nE9GXxPOMsWs6PzyNRrPYzBRrAOLbYneCagM/e+quJDUrqUmMwY2ZpIenSpjin0+SYmD8UHzlc32vJACweE+mlagYPgvgZQBEP6MH4FcrazSaVYRQDCnMzQCiiiF8kTSDz4ErqbUYQ9zz9x+Zlo8n8hWUay5ythl6XVK6asYk1DxPxhFUxWDzk1dkjIExdiRyqPN16xqNZkmZSlsxuMmKwUlTMXgIKYZmMYY4w/Dg0RlkLANbB3OY4IqhyzZgGiTjJUnpqpZpgDElhqHcoxhT1JV0YCyPd1xzd8hltRi0YxiOENHvA2BEZBPRhwA8mtK4NBrNEjGddoxBaXcRvYZohR2tfGaM4W9+uBd3PTX/9mlCMYhW2vOpYxjPV7ChL4uN/V2Y5DGGrGXCNEhRDP65cS0xgPgCP/Fc1Bbfe3gav3psFMemS+3c6oJpxzC8G8B7AGwBcBTALv6zRqNZRQSupPSDz9FriIBv1JXkeAzX3XUEtz8x1vb1PnfTE/j5w8eVrT1bjDHE3H/V8ZCxDAz1ZDCRVxQDkVRYrEHlMxAYPycUY4hXDF6DsaRJOy0xxuE30NNoNKuYaelKWvysJJHnH3UlCVdKXEC4Gd+44xCed8a6NuoYkussKo6HjGlgqDeDew9Po1LzkLVMWDGKIS74DASV36HgM0+hjd6ejHcscuyhla09Pw8gcVSMsb9Iek6j0aw80jYMoRhDxM8v01UjhqHiJE/WzShWHLiMweUtMcwm+zGIVXtcjKXqeMhaBtb2ZDBVrKJUc5G1DZhmTIwhJl0ViM+8shMUgzAISX2d0qIVV9IeAPcA6AJwIYAn+NcuAJn0hqbRaJaCxXQlRbfXlMHnSNttYRja3RPZ8xiKNReOx+q7qybtxyBiADFPC1fS2p4sXI9hdK6CrjrFIOoY6gvcgPiWIEnpqm6D9iFp0lQxMMauBQAi+l8AnssYc/jP/w7gtnSHp9FoFpNS1V3Q6rwV1Em/ro4hIfhc4a6kdsdUdlw/E8hldfsxJGVdBTGGGMXg+jGFdb3+mnjf8Vm8ZtcWHJwoKPs4+OdGXUkZSyiGmBhDQlaSME6L7UpqJ/g8CKBf+bmXH9NoNKuE6VJVPk6rCjfcFiI+XVUEnx86OoNv33V43oqhUAliEx5jMKn51p6N6hiqPMawtsc3DB4DXnLOxljFEHUlRVtxxGYlLZPgczuG4Z8B3EdE1xDRtQDuBfBPjV5ARFcT0SgRPaQcW0tEvyCiJ/j3QX6ciOhzRPQkEe0logvnc0MajWb+TBVq8nFafu2kOgbGmJxchfH4zt1H8I8/eVQGn9tVDKWqK6/jeZGtPRPeq5WsJGEYMpaB55+1PiHG0NiVpNZRyKykaF2HrHlYpoaBMfYfAC4GcD38LTifLdxMDbgGwMsjxz4M4CbG2JkAbuI/A8ArAJzJv64E8OVWx6bRaDqDqGEAWlul/vDeYTw1XmjrGkmVz+r1qnLy9FCuBe6tdlVMoerI92bCldR0B7f6ymR1XFnLxLreLADgeWesQ0/WgmUYMXUM4dfaEVeS6qqyjfgYg7jfxe6h1G4TvYsAPA9+K4xnNTuZ7/oWrUh5NQBhUK4F8Brl+NeZzx0ABohoU5vj02g0C0BUPQOt+bX/+gd78fXfHWzrGkn9gtSJWKR01lxfRRQq/gTf7sq5qBgGL1rHkFBdLTfcaVLHcPFpa/Enz94OwHcbyV5JSYoh4kpqpY7BiTl3MWi5joGI/hm+MfgmP/QXRPRsxthH2rzmRsbYCH98HMBG/ngLALXlxjA/NoIIRHQlfFWBbdu2tXl5jUaTxFQbisFxPdRchvF8teF5UZLqGNTj1UhKp0yhbVcxVIQLyoMbqWOYj2KocMNgmQa+865ny+O+Ymi253PYleSEXEnxdQxug9TZNGnZMAC4DMAuxvzegTzOcB+Adg2DhDHGiKhtU8gYuwrAVQCwe/fuxTWlGs0qZqbUeoxBuHvG5yptXaPqeCBCqG9Q9HpVvgGOmETFuNoNwgrF4Hn+Sp6IYBgUWuFHaRxjcJEx6x0tpkF1ez5HT6t3JSnB56SsJNECfHHtQtuupAHl8Zp5XvOEcBHx76P8+FEApyrnbeXHNJpVy0S+gl8/3n6bh7SYLddkWmWz1bnY1nKi0J5hqLleXUdSIEhVzdmmUgTmf5+eZ22Fqhg8Fmyeo8YEojTqlVThBW5RLDNoiZFUxyAMSkXZ81qcEjTRC7+vMAiLrRjaMQyfQDgr6R4AH5/HNW8A8Db++G0AfqwcfyvPTroEwIzictJoViXfvvsI3n7N3Yn+7sWm5jBkTQOWQU0nIzHBte9KYtIwqNcQE3JP1lQyd7grab6KoaZkJfHgMyBW+E0UQ8QwMsZQdT1pOFUMCtJVxcuirqRcxr/ncs2Fx2MeXZZ/LDFdNaZF92LQTq+k64joFgRB579mjB1v9Boiug7ACwGsI6JhAB+Dn/b6XSJ6J4BDAF7PT/8pfHfVkwCKAN7e+m1oNCuTUtWF6/kBVj5HLClV14VtGai6XlP3hag1mCpW4bie9JM3o+Z66BKKISYQncuYGM9X4Xls4a4kHrR2GYPnMbmKtwyq68ckSKpjENXTca4kSzE0SXUM3dwwFKuuvEZ3xkSp5srgc3Q/hkbqJU3aCT4/B8D9jLEbiOhPAPwVEf0bY+xQ0msYY29KeOrFMecy6G6tmpOMuCrYpaTmMNgm8RV1M8Xgr8YZ83cz29Df1dI1qq4nJ0l1whOfRbdtyfPqgs/tupJ4HYPjBi0xAMA0GymG+uAwEBjCOMVghgyDf6xOMdiBYRDnCgNpNUlXXc4Fbl8GUCSi8wF8EMB+AF9PZVQazUmCTF1cLq4k14NtGqH9BZJQt7Vsx51Ucz3pVlGv4SiKARCGgccYSvPLShKKweNN9MQqvmGMIbLyFzQyDJZZrxiidQyqK0lkMIljSU30GnV6TZN2DIPDV/WvBvBFxtgXAfSlMyyN5uRA/OMn7Sa22FRdv+WDpexIloRQDIC/gU3T93Y87Ds+h5rD5Eo5VjHwybLmBIphZr7B52pQMe233Q5cSdHOrjOlGo5MFoMd1iLXEllY2RifXzjGEF/HkOEGt1h1ArcZ/xyaB5/9J4pVB0+O5lu59QXRjmGYI6K/AfAnAH5CRAYAO51haTQnB3F9c5aSGg+utqQYaqpiaG4YbnjgGC773G2YKFRis5KE66Y7Y/GxsAUHn0uRAjc1Cyh6f5+76Qm89eq7AsUQNQyNFEMLriQiQs42UaoGSkg1DAbVxxiCdFX/+NW3P4VXf+H21PeGbscwvAFABcA7edB5K4BPpTIqjeYkwYlk3zTi8EQx7eGg5jLpSmo2CVfc9gzDeL4C12PJWUleWDFUHU8aCxF8btelUlB6JTHeRA/wXT/ieocnimCMYapQxVSxKg1C9FqVhjEGQ+muGh98BnzXUakWKIYufq8GEQyimDoG8d0/fmiiiILSATct2umVdJwx9hnG2G3858OMMR1j0GgWQKvB5ydH83j+p27GvYenUh+PbRLfqrIdxdA8xlCsBq6n2BiDVAxBjEG4b8R82W531biWGECgGA5PFPGCf70Ztz857sc0HC8xK0kqhiZZSW5CHYO4Nz8rSdRsGHI8vmEInx9NVx3jBrikfJZp0NQwENHt/PscEc1Gv6c6Oo1mlSO2smwWfBYr8sk2awbapeLw4HODrJ3g3PZiDMKtAyA2xlAXfFZiDNFzWiUocAvXMdiGAddlODJVBGP++GvcECVVPov7jStw8w1N2IjFKgbblCnKQOA2Mw0CUVzwOaxCRmf9z1nUZ6RFKxv1PJd/14FmjabDOC0qhqpSLZsmNddDL+8WGs0AmivX0Ju15EpYuDOGejLtK4aYOgZhBHpkjMGrSxltt8toMdRdNVjFC8UwUfDHXXMYN0RBXCNRMSQYBnG6rGOIsQw5XrcgYgxdXDEIV1L09qJurWWjGFSI6EIi+gsieh8RXZDWoDSakwXZPbNJVlLau6oJRLqqQeFrzZRquOjjN+GmR0flMTGmzQM5TLbQFqMUciXxthsxriShGGqKKyl6TqsIxSC6tRqRGMMkn2grSmpsmbvIooZRjCUp+FzfRK9+PMKVFK1jMA3//Pr9GIIW3a7HMMHHW05ZMbRsGIjo7+C3yR4CsA7ANUT00bQGptGcDMh9B1osJks7e0kUuFnc1SKYLPgb34/MluUxsYIe6LblZNqIWMUQciXVB5+jrqR2FUOJT6Dic462xJiUisGT91OqhfdwEDSKMZhK+mtSryQgcCU5Sl8owO/hFBdjCILPfk8q8XxpqV1JCm8BcD5jrAzINtz3A/jHNAam0ZwMtNryQAR606538NNVTX+iUyZFsUJ1lYlaGKverNVSVpXqF++KyUoS9yZcSWqBm6D9GIMTep1ax+C4DOPcMKiBblXZ+I33/MfNXEnN6hgAIJexUKqpMQZTjisuxuAqikHEF4CwkU2DdlxJxwCoNe9Z6O6nmlXOG77yO3z+pidSe/+gwK21hnVpd9msiqykSLpq3Naawlj1ZC3pqmlEXPBZnfij1cAVx6szmO1kJTmup3xu4cnaNv1+UCKYr6oTddJVYzqNXEl+jEEEisGvVT+mbttEseooMQYlXdWg+joGJsbBZHwBWF4xhhkAD/Puqv8B4CEA03yf5s+lMzyNZmk5MF7AwRTrB2otxhjE/gSLEWPIxLTEEK6ikGFw/HOzliE31mlEseritHU9AHyVEd0TIZquWlQMiTynjfsXCkUEeIFgsj6lvwsj06XAleQGriTVf+9G7heIz0qylM8rac9ngAefY2MMfopwXbqqkgI7piiGtGMM7biSrudfgls6OxSNZvnhuF6qmUBOpLI1CRl8TtmVVHjH5IkAACAASURBVHWCXknq6rwsDFPElZS1DH/17TSfqEpVF+du7sdHLns6nnvGurp+RdF01Xyl/j3bibEU+ev7umyUa/6kKibr7UM9uP7+o9K1lKQYQvtQNytwkzEGhK6lIrKSovEUgwgUU+Am/z7csGJI25XUTtvta4koB2AbY2xfimPSaJYNjsc6ukp/6OgM3vute/Hj9zwXa7pt1Bz/vVt1JaWvGJjSRE8xAjGupKrjIWsbyFhGS7GPYtVFd8bES87xd/ONdnAVRqcv63faEfEBlXYMg9imtL/LwhjfZU7M1duHusEYMDxV8u/FVYPPjQ1D1qzvlWQaqI8xxPhjuvkmRMINl4tmJdUpBv+74zGMzpaliy/t4HM7WUmvgh9s/hn/eRcR3ZDWwDSa5YDrsY52Pn1kZBYHJ4oYmfUnpFqL3TMrMSv2NKi6HmyLQpW8gOJKcsOulaxlwjaT9zZQKVYdWdAFhN0vQOBW6+3yz5kr1xClne6qP77/GAwCnr1zSB4LFEN36Nyq40l3WDNXUqJiYK0pBgCY40avO+vfq58iXB9jCFJgGUbnKtgykKsbYxq0E2P4ewAXAZgGAMbY/QBOT2FMGs2ywXFZR1NES8r+AOr3poohxsffaRhjoRiDet+luOCz4zfcy5imX1ncZGylmisnRqB+TwQxCfbyyTJfnr9iKNdcfOfuw3jJORuxdTAwAqKL6fahntD5C3UltbJRDxAYhlne+2ltdwaffcP5eNX5m7liaBB8nqtg80CX7NCaJu3EGGqMsZlIbu7yaCKv0aSE49WnTC4EMenIHkkJVbaCf/35PvRkLZkRk2Ydg6gOjjMMMivJDbuXspYB2+J+etdDlxG/DV2Np572KIYhUTFkhWLwJz9fkbSW1iu48ZETmCrWcMUlO/DY8aBzj5isB7tt9HVZ8hq1JFeSWsfgun6QOGbGF58XY6xpHQMA5LlisEzCH12wVZ5fX8cQxBiKVRebB7pkh9Y0aUcxPExEbwZgEtGZRPR5AL+dz0WJ6Gwiul/5miWiDxDR3xPRUeX4ZfN5f42mE4h9eTs5GYuVnpgQq02ykn712Chu2TcaKIYUXUli8rUtw2+iF6pjiM9KylqGLPhqpHqEQcwpriS1KAwIPmfbJGRMA7N80u7rsuvOiTI8VcQbr/odZvhOb4cnCgCAZ502GJrIxWRNRCF3krpbnLpoV38vVZ6FFYe4hjCuQLwrSQSbhZvMUsZmGHF1DPw7Y3A8PzFABLDTpB3D8D4A58Jvvf0t+OmrH5jPRRlj+xhjuxhjuwD8Hvw9nkXG02fFc4yxn87n/TWaTiAmwU5mJdUpBlHHkHCNsuOiVHODGEOKikGoEhl8dmMUg5rXL2MMwjAkj0240LpDiiGSlcSvbxqErG1glk+e/TzmkLGMRMNw3+Fp3HFgEvvH/U1s8hUXtknI8mI9gTpZC3dSxjRQqXmxn23UlRTnRhJjBvje0g1dSf69zJac0OvE2JJ6JYl25ZZpcMWwTFxJjLEigL/lX3UQ0ecZY++bxxheDGA/Y+xQnPTSaJYKuZNXR11JTug9m1U+V2oeDHLrCrXSIGj5QKGtKoEgXTUcjHXRk7XkZFltUOQm7jtkGMxwVlLN89txEBG6bFPGGIRi6MmYsvdRFLFfgxhDoeKgJxt0LhWok/X2tb5i2LgmK107UcKupGTDYCmKIVpMp1LnSjLUGov6jrbq1p5+KjHf7GcZ1TE04znzfN0bAVyn/PxeInorgD0A/pIxVteAnoiuBHAlAGzbtm2el9VoGpPGfrvFanjlLZRD0mpbrNSFYUizJUZNUQxGxJUU15Kj4ngY7DYUxdCCK8lWgs+RGIPjenKizNkm5ipcMeT8aao7Y0n3UhRhGMQYChVHttawEhTDay/cCssg3HFgEoWEFXg0KymuuE3cCwB88mf7cHzG7ycVt86tcyWZqpur3pUkO7Z6QWKA70paPjGGjkNEGQCXA/geP/RlADsB7AIwAuDTca9jjF3FGNvNGNu9fv36RRmr5uTDTdWVJN5bKIf4a1QcD8WqI1fCabbEUA1DfbpqnGLw6xjERvbRTqgqYoUbTVcN7/nM5ETZZRsyMCzqGrozZl1jO8FsRDHkK44MYqvGQG2FfcaGXnzwpWcjYxmJSqRdV9I1vz2Inz18vO66ApmVVBaKobErKVic+K4um7uSysuoJUYavALAvYyxEwDAGDvBGHMZYx6Ar8JPj9VoloRW21W0g+pK8hS3Q5IqKddcFKtBjEFs7DMyU8L/+dFDLdUPtIp4L3/PZyM2xqBez698NlsKPotitVC6al3lsyfVR9iVxBUDn+g95l/7oz96ECMzfj2IiEdIV1LVkfUQ6qo8zu9vm1TnShL3VGcYEoLPVswbN3IlCaMXdXPVKQbZXZWh5niwTEIuY6JYSzfG0EnDMJ8AwZuguJGIaJPy3B/B78ek0SwJzSbt+aC6kmqhqt/6a/jtOBhXDaK7qX/ebU+M4xt3HMLB8ULHxlZ1RFaQATOSIVOKUQxVkZXUQowhPvgcVgyux+RE2WWZ8nPvz3HFIFt1e3h0ZA7/ecdh/ObJCQBKjMEVisGVMYaQYoiZrH3F4NQd868VjjEku5LqjyftxwCorqRwjCGpJYbjMT/GIYPPyyzGQET9ABhjbC7y1L+1+T49AF4C4F3K4U8S0S4ADMDByHMazaLSah+jdigpriTVGMS5q8rKRDvN0zDFRCVaVHSyZ06gGKhuNV9OiDFkrPZiDN2Z5BhDzWWw+WyaVRrfCcXQk/Vf63nAyLSvFITBigs+bxnwm0GrAd44v3/GMus+x6xlIF+JuM5qzYPPKrF1DJlo8DmcSpu8HwMLXEkZs6X9LxZCy4aBiJ4F4GoAff6PNA3gHYyxewCAMXZNOxdmjBXgb/qjHruinffQaNKk1arkdihIV1J428q4oLLa9kBMfMKAVBSXSacIp6uGW1wHMQa1wM2TTfSAQHHEITqd5uoUQ7hXkqW4kgS7t6/FC89ej6dv6scvHx2F43k4xgO8wtceVQxq8DkpXVVgm/Eqwr/fsGJQO7WqxBW9xSmGDN8dL8mVFI2fCAVR5S3ILZ6VlHblczuupK8B+HPG2A7G2HYA7wHwH+kMS6NZesRqNh3FEN62Mi6orBqGfGTDGfFcMSFoGsf+sTy+fdfhxOfFngp+8DmiGCJN/BhjQYyBVz43MqAlma4aLnALGUdPDT4HhmHrYA7XvP0iDHZnAIQVQzFiGGpK8Dk+XbV+to5zD2XjDEMLBW4qcdciInRnLPm+0eBz9E9NJCVUlN/Ncitwcxljt4kfGGO3A0jXbGk0S0gaMQa5B7HLQu6jeMVQP9GKGIN4rh3FcN2dh/E31z+Y2NMoWuAWdqNEejzxqvAs75UENKtjqE9XjdZKOK4Hm7t9upTJ2uaPxeTreB5GuGKQrqRioBgYYygoWUmW0Tj4HDfZZy2xw1x7WUkqcYYBAAZ7gkrupsFn/qNYCIgYQ7nmtbVpUbs0dSUR0YX84a1E9BX4wWIG4A3QezJoVjHRfkYLxVPaJTue1zzGELMqDFxJ7ccYJgpVMOa7dcSkqSKMU6NeSeKYbEFtB72SGisGv6+SOhH6cQxltzQ3XjGIuINaXXyUK4ZyzYXnMdmttOp4KNVceAzxiiFmArdjDIN0JdUVuMX3goqNMSQsu09f14sjkyVYBoXiEHExBvH7Fp+/zbOSAL/oUFVgnaSVd43WEvwd/07wDYRGsyrptGIoK5vZ1FwWmkjj3FWVmBV44Eqq7wTajAm+W1m+7CQYBjVdNb7tdi0S48iYSoyhSfBZDTwD9VlJpZorDYLqyxfvr/YjEmmqxaqDuYoj8/+rLpNut96sGXodkJyVFCVwJUVbgLSuGMwExbBzfS9ufXysbiyxMQZ+efH5W6aBHH9dqbqEhoEx9iIAIKIuAP8DwA7lddowaFYtQfFZZ/7M1SIqx2V1GTlRKnGKIbJnQDGhlUMcE3wHsHzFwa8fH8MZG3qxmff398cgXEnEO58Gk2I0XVUolqyt1jE0CD7HTGLRrKTJQhWn8jYVqssp6kqq1DyM8o13SjVPFrcB/uQtPudmLTHk+zdQDOrvPl9xQt1ho/cSJcmVtHOD36MpakgbpauKRUXGNAA+hDTjDO3EGH4E4FUAagDyypdGsypp1hK7XdTcc8fzQj75OHdVOWa7zPqspNYnB7G/cb7i4F3fuAdX3/5U6PmqEuAUgVCxgg0K3ES6LHcltVrHUHNCGUkAb7ut3Pd4voqhHj/AnFUNA3cviRX4sZmSVAilqisDz2IMoiah1eBzI8XgKfc/U6phQ39X7P3FGYak1m871/fGHjeIoHoU/Rbe/mPxt2NbgSspzVqGdnTIVsbYy1MbiWZVsefgJE5b14Oh3uxSD2XeCIOQ1Pm0XdRAcS2iGOLcVbHB52hWUovBZ8aYdCVNFioo1cITKhCsYDOWEWoKZxpQmviFDVO4u2qjyme3brWtuqs8j2GqWMVabhjCMQb//UX8YXiyJJ8r1ZzQfdRcT3El1RuGuMk6zj0ULXA7MesHuzcmGAYrtsAt2ZUUR7RXkvonIT5vyzCkmlouiuG3RPTM1EaiWVW89eq7cO1vDy71MBZEkJqJjmSAqPEAv46hceVzXPBZrNjLspCrtcmhUHXliv7YtD/JRdtAqOmqhswAYqFYhxinDD5bQa+kRoZhplSTFcwCdaOe2XINrsfkQkLEGEyD5FjERDs8VQQAbOzPtqQYkproCVRXkjBe0awk0RjvlCaKoVkGFACs683EHo/2SopLSBDpqkC6iqEdw/BcAPcQ0T4i2ktEDxLR3rQGplm5eJ6/21Q7bo7liPqP2YkAdNiVxEI++fispGTFIOIPpRZ75oj4AgAc4xk9dYZBbNTDYwyAv4JVDZQTiTGolc9xwXLBTKmGge7whGgawf4KQs0IV1IXn5jV4jMxpuN89b59bU8oxmCbhGpIMfjvoRqDOJeP6koS/ZiidQwneExjY3+8Ahbvu2kgMBxJiiFpe4HoRj1xQjVjkVRTaSqGdlxJr0htFJpVhXBJNPI5rwSi6aSZBbYWC7uSvNAKu1nls/o6oH3FICZeIFj9zkVaWEfrGADfEKgGSqgc8dqebGtN9GZKNazJhacbNcA9kffHF3Ul2ep+BXxMYsEx0G1jdK4sFcO63iyqbn3wOdraOopax9CbtTA2V6l3JfHPbOOaxophy0AOR7irq9H2Mru3D2LPofCOAtHgc9xiwTIMabTSbMHezkY9h1IbhWZVEewdsLINg9skBtAuIcWgFLhF0zYFakpo1fVC51XajDFM5gPDcGwmSTEE1xMTnecFiiFrBdXQhyd9d86pg90wDF9hJP2+GWPcMIRdSaZS4DZZ8FfkQ73CMPiTn62s5oViEJ/jmpwtYyWWQViTs+fnSlKuIfoxCVeScCEeny2jO2OiLybNV73GloFuAJMgSlYGAPDtKy+pS+mM1jHEKQZbTQ9OceG11G23NasQ8Qe70hVDTTUMHVidiRiDbRJqnidXfDnbjJ1UxYQ80M13MMtaSrpoe4phUlEMMsZQrjcMFvfpW6pi4G6jvi5LGoZDE0XkbBPr+7L8nozEFWy+4sD1WJ1hsJUYQ+BKEjGGeleSUAzFqgPbJPRkLRlj6M/ZyFpGKPgseiU17a4aijEE24iK+wf84PPG/q7EyX6oNwODgHM39ydeR8VSJvhgbEEW2LHpkmwlHhqrRXJsaS680qmO0JzUSFfSilcMaoyh+b0wxnBksoRtyibzKmJ1vyZn+4qBT6RdGTM+K8lxkbEMf+U7V0F3xpQZUq1kJXkew/CUP55xviLPWoZ0JamK4chkEeWapxSTBT52sULvzVrSwByaKGD7ULecKG2TEhcCwtVTZxhMA+Wa62dMcUUj2kVIxWDGK4asZcqeQdPFGga6bdimIRVDzjaVgLDa2rp+fGpWUrcMPofTVX3DkJxht3WwG/d89CXSwCUFnhuh9kp63Zd/i+eeuQ5AOHvLVwz+m2vFsAg4rofRufJSD2NVUF0lrqRQjKEFxXDbE+N4wb/ejCdH48t7hGLo77L5jlz+55Oz4w1DpeahywrSE7szpuyVJBRDo8rnnz40ghd9+hacmC1jMl9FzjYx1JMJOpBWHXgew3Sxiks/fQt+eO9wUDPAZwZXiTH0ZBXFMFnEdsUAZiwzcSEQGIZw8HlDfxblmofZsoPJQhV9WUu6cALFEExRoo6hWPUNpq+0GMbyFQx2Z5CxuGGoBg30AN9lJYhb8YtrZMygJiNa4HZitpKYkSQY7MlIwzKf/etFryTGGI7PlmURXzgAH4yxkuL/lzYMnB/edxQv+tQtqW+AcTKwWlxJ6mTdSpHb8dkyGAMePDod+3yh6iDDC8JqLpOfT8424wvceIsIkZ7Yq0zMgWJI/nt9dGQWrsdwaKKIyYJfIyB2NQMg+yaN5yuouQxTxZqcdFTFIFxJ4vqex3B4sojtQz3yvTImyXTXKEmKQVRdj8yUMFGoyvgCEO9KMo3AMGQVg3l8pozBbpt/rh7yFVdmJAHh1hSNspJsk6SRUBWDmKiTahhUhGGYj2IQMYZi1e/1JDrnZpX+TBmLgmC/VgzpMzpbRqHqYrpUbX6ypiGBYljZHVOaFaBFEUHPx0aie1j5lHi/INs05O5sAHclJWQlZW1DTjbdGX9iZkoKaaHqxO6BDAAHJ/wA8YnZMsb5xNsTCZ7my47cfxhQ+xIF9y0C3b1ZC47r4fhsGVXHw7a1gWKw+aQch+h8GjUMm9ZwwzBdxmShIjOSgATFwGfbco0rhkxgGNbkMrBNAxXuSgophmbdVRWVIB5nlRjDTKmGquO1ZBhymfoU2VYRMQbh4hPbd6rptLaiatJ01WrDwBGT2WxJdxJfKFXX5d9XtmJwQwVoze9FrN4fOx5vGPzqXwu26QddxXvmbCOxjqHLMuXKuCdrSoPiMX+iZiy+3gEADiuGYSJfwVBPpq55Xr5SC6WtxsUYxPv3dlnwGOR2ojsUxWDzzKk4pGLojioGf6I9NlPCRL6KtT2BD1+03bZiDEOx6sr204D/dyYUg6hjSDYMyRv12KYhV+PqRj3CpbOhQYxBIDK65mcY/HRVse2nUAxqcFwNWq9KxUBEB3mR3P1EtIcfW0tEvyCiJ/j3wcUaj/DZzsVkAmjao7IKXUltKYbjs7HPTxWrGOyxYfEgqZqVlBR87rLNOsUg1III1MbtycAYw8EJfwI/PlPG4ckiTl3bXWcY5spOqAmdmBDVlhglRTEAwH5uGEIxBtNI3MFNGIaBiGLY0NcF0yCMTJd9V1KMYsjEuJJKXEmpvZcGezLImr5qUfdiUF8HNG6JEVYMQeXzbIIrLA4iQrdtNqxhSEIEn4WhFguNbEgxEG/XvboVw4sYY7sYY7v5zx8GcBNj7EwAN/GfF4XAMGjFsFBWTfC5zXRV8Y98YraCqUK9S3IiX8HanqxUDOLzySW4kio1fyvJXCbY81gtOFvLK4nj4mLTxUAJ7Dsxh7myg21ru+VKWmT95CtOrGIQK17XYzITSaSmPjVWgGkQNinFXg1dSbzOINp22zQIG/uyODhRwFRijKFeMQB8wxrl/aJZSapiaFbHIDYaUluIi++OF7h2om64JHIZc16KQfRKEr8PYfBVV1LGNEBEDRVaJ1hqwxDl1QCu5Y+vBfCaxbqw+JDjcoc17bFags/hArfm96Kmf8a5k8Sq2DIiMYakOoYYxeB6TLajED75OMUg1AIRcA+vsN0x1CNX0iLwmy87IZUsVuiqYhibq6Ava8nXTpeq6MmYITdPxkwucJvmxW1xmTqbBnK4dd8YHI/hvK1r5HHToFAwWBwTZBUXG4BQVlJd8LlpgVvgShLXs0yCaRA8j8lakbg9LOLozpjzCj6bht8rSfwdyV3bIjEGAMhyI5gWS2kYGIAbiegeIrqSH9vIGBvhj48D2Bj3QiK6koj2ENGesbGxjgymqhVDxxBGdsUrBrc9V1Kx6kh3w+Mn6g2DyAyyTQpt1NNlm7FZT+Wah6xlYk3OhmUQejIm9/kLw+Cv4OOK3ERl8tNO6ZdKZvtQ4Erawg3DXIJiECmejudhPF/Bur6snGDzZSfU/VS8rlEdQ5IbZtOaLrn72kWnDYWe67LM2F5JAGS6qmAgZ8tsr0LFkYVqQNgwxG3ZrMYVVFeaScQVg284W1cMVmz2UzOiMQZ1Rz2BaO/RSKF1gqU0DM9ljF0IvwfTe4jo+eqTzE+1iP1vZIxdxRjbzRjbvX79+o4MRgaftWJYMKsnK6lx99MohYqL7UPdMAgY503rRmfLuPiffol7D0+hWHUx1MsVA9/a0zT89MO4f/JKzUWXbeAtF2/Dte+4SE5a+YowDP5kG1fkdnDcNwzP2uGH6YiAUxVX0uY1gWKYLdekT1waBgqa6I3zwLVQCIVq/d4KmQYT1WypVhd4Fgjl8rRT+kJZSYC/J4OqGNTVftaKupIysmCuVHPDwefI9plRbCVdNSPrOIgXlnny825HMcynjoHIV2jRxWlIMRhBzcWqVAyMsaP8+yiA6wFcBOAEEW0CAP59dLHGI+S5VgwLR/zBNuq2uRJot45BBD3X5GwZcH1yLI8TsxX84pETAMAnWILDFYNlUKj9tIqoYxjqzeI5Z6yTmUJi1zahGOJqGQ5PFnFKf5dMKT2lvwtdtildLKILaIErhs1rcr7rJhJ8dly/Knldb1Yey5cd2f1U4Pu84z+j6WJjxQAAl5w+VPdcb9YMKRO1gjmqGAZ7fMUgPkd1Ejd4sBZo3BIjrBgM3psqaB2StHtblPm6kkTb7STDINqVAL77K82F15IYBiLqIaI+8RjASwE8BOAGAG/jp70NwI8Xa0xVnZXUMVaLK0k1Bq1s1lPg21euydmY5rn74vsDR/yiNz/4bMheSbZpwDKNhKwkL7L3MZ+YpWFIVgyjc2VsXNOFU/jEKzKIxEp6qCeDLtvgwWd/4t4ykJNpooYSY/BdSRlpGOYqTmhcgD+5ThereMW/3YZ7D4e7hjZyJQnFcPFpa+ue+8Rrz8Ofv3Cn/FndCycTUQyD3ZlQ9o5ayAcEhi62jkEJOKv7SxtcMRSq/v1acX6oGHL2/ILPovI5ahiyMnU3HHxPUzEsVa+kjQCu53LLAvAtxtjPiOhuAN8loncCOATg9Ys1IDGZacWwcFZL8Fk1bG5LWUkOerIm1nRnMM0Vw1TRz+h5cHgGALgriWR3Vbm/ckLls1r1KvzWItgsFMNc2cFPHxzBdXcdxjfeeTEAv431KWu6ZFGWqDkQK+n+nI3erIW5il/g1tdl4aN/+HT0dfkTuJhIK46HqWLNVwx8YipUHKyL7Mxnm4SRmTJGZsp44Mg0LtwWZJo3MgwvOGs9Pvaqc/AH59SHE5+9M6wiVMWgBp+7bANddjgeEY0HmIa/wo6bsEXTwEwk+GwZBJcXnLXqRgL8z3ghdQwipiHIWPUZWmlnJS2JYWCMHQBwfszxCQAvXvwRqQVuWjEslNWoGFrJSipUfN/2QM7GNDcIQjGIAKvw1fvBZwbLNGCZfv665zG5Ugf8SVldmYsVq/B5bx7owkC3jYeOzmA8X8VtT4yj6njIWAYmC1Wcu7lf9vcRjf3ESrq/yzcMeV7HsHWwG79/xjp5LWGERP8w35XErx8TfFb94OKeAf+eZsu1uhoGQZdt4u3POa3ZRwsgrBjUlhgDvAdTeF+FSGosn6iT5mvhRgpaggjF4GcltWMY/ufzTscrnrmp5fMFFKljkGNTejkJstbqVAzLDl3H0DnEH6zoq2PMx+G6DJhPgVtPxsRAt41DPF1UGAiByEryg88ebINCOfMZ/lnVXA+ux0K+fNXHD/gui4t2rMVv90/IthOFigPbtP0MqN4Mtg7m8LeXPR2X79oMANi9fS0+9NKz8OydQ+jtsmQdQ39X/Qob8GsyAH87StF5o1B1pctJoK5m1a0258oOGEPdtp7zIawY/O1Hs5Yh25JnlM9KzUpS7ydpJS92ohOTr4j9+IbBaTkjCQDO2dyPc3j77XaItsRQxwbUK4bVmpW0rNDpqp1DXcms5LYYzfZkVhEVwjLGIF1Jao2Agd6sxesYePA5tFtacD1RbayuzIOWEI587pLThzA8VZKKpFB1kK84qLoehnoyICL82fNPly6ljGXgvZeeyQPRlqxjiE7cgWFQFIMyMcWlqwpUw5DUQG8+qNlFYrLMZUwM8kK/Zq4k9XsU0Vk1pBhkump7hmG+iMrn2VZiDFoxLA46+Nw51D/YmuvVTSIrhXYUg5jIe7ImGPOzkkRLa8FQb4ZXrfrFYDWPhfZXVrNMRCM+te1ENPgsDINKsepKI6b2HoqjN2tjeKqIuYofY1CpVwzZ0KIp19CVFNxzRw2DMjGKyTJnm7I1iDqGqOtHZHQluZI+9NKzsXNDD85Y34d3v2AnztrYB4vvMJevOE1bbncCEXzOl6MxhnpXUsYyZAuWNNCGgVORdQxaMSwUVSWs5FoG12NyZeY2iTGo20mahuGnHVYcTBdryNn+hjIiT9/mWUiO64UyYVSFcseBCRABFynZOqbi4wf8yfFpp/SF0mPzFUe6fNQWE3FsHczh1sdHwRjqDIMVVQx9WRyZKsrn47KSBNOKYhDdige6G4+lFeIUw/suPVMaTzUrKbrCt5q4kl7/rFPl4w+/4mn+9ebpSpovRH6l9VzZARHk71HNmhKITrJpcVK7khhj+PhPHsGDwzPyQxbbEGrmT8iVtIIzkxyXSV96MwMnDQN3JQF+u+mpYlX6m4VhECtRP/hMcnWu/t3dcWACTz+lPzShWtKVFLiZDIPwVy8/G39yyTb/uYorexsN9TSejM8/dY28L5GNJBBG6PhsGV22gZ6MGXLDNHQlFVNyJUVaYgDAmy/ehufwoLltNlIMjQ1D7PVIKAZ30VxJoiXGoPJ7zyS4knSM1dOtzAAAIABJREFUISWKVRdfve0p/OKR46g6QZFQNPijaY+oK2ml4ngesnbQZbMRYrLuzpgyA2e6VMV0sYazNvbBNkmmeIoJrFR1YRnBVo1ij+mK4+Lew1O4+PRwbr8MPlf8PY/FZPeWi7fjjc/yDUOh6mCCV11HK4mjnLd1QD6ucyXxCXRsroKhnqxs3CbIRg2DFfjwZ0o13H1wEp+/6YnUDEPGqp+6xDGD6hVNYBjau57LW2JEs5zSwOAdU4tVN2TU44LPmZOsid6iIjfEqLqoup7MbtBxhoWhbjm4kqufXY/JCaaZgROKoTdryb+jqWIN06Ua1vbYePcLduJV5/spjHLlX/P3FRDZNsKVtHd4BuWaVxc/sJQYQzZSeSxWyMWqI/cdHmoSYzhtqAd92SB9VUX154t9CMKKId6VdOaGXkyXavjO3Ufw2V8+jkm+l3MnDIM6qauuK4GYOHuyVl1LCvGZt9OqwjQIFcdDueahN7vw8TfD4NcDEKoTiUtXzZgGagltzjsyltTeeQUQtLd1UXU8+cvQmUkLY7UohpobpIs2Uwyi6KxbMQxHp0pwPYbB7gz+8qVn49Kn+UVcIrunXHX9QirZsM6/xoExf8/oczaFUx6Fe6cQU3nczVe0ee5K6s6Ydf2MohgG4Zm8o2lSjAEI+irZSrpotCWGWNWeu3kNXI9h3/E5eAzYP5ZHxjLqxjsfiAKVlI15PzGGuJoDkTLdTnM7yyDZO61nERSDarPU+FCcK8m2SCuGtBCKYa5cg8f8XG1AF7ktlNViGHzF4E8IzbKSRIfTnowpUz9F6+to4FW4jmZKfmA6UAz+NUSKa9QVZBvJikHk7Rcrjuzi2grCnRSNMai+eLHTWqMYw+7ta/Gyczfigm3+++3jbcf3ncgnttyeD2IMcYohoyiGKI1aYiRhcLcY0HoDvYWgfuaqYsjGupLM1dlEbzkgsjtEpeaQVgwdYdUEnz0v2Pu3SfC5qCgG4TYRW2AORjqLCkMwUahgTbctV4LCiE4Vq8iYRuzGNoC/cIk+l+O7hhWqruyG2gqvPG8TnnvGOmwdzEXGGExSYm9mtU4glwlPHeds7sdXrtgtN/MRq9n9o/mOuJEEIvYRjXEAwco6zjDMJ/hsGSS3+o32XkoD1Wg9fVOffBxb4KYVQ3oIxSBS6jbwP+qpYv3uW5rWqbqe/Edc2QVufroqUfOWGHIzl4yFrOVvrpOkGAJDwDCQC5rTCVUyXahhoLt+lS1cULNlR7qrBIbhbylZaFMxPGPLGvzn/7y4TgGo1eqxisGKd61EW19UXa+jhsFqQTHEBYpNGWNo/VprezKYKPiB/MXIShJcuG0glBgQdH9VsrJ4Ez3G0okzaMMAYKrgKwaxeckYz+rQzI+q48kWxStbMfBeRgltsVVE8Fn49QdyNg5O+Hn/0UlcXXmvydlywhe1EtOlat1rgPAqfk2ufuLvzlooVn3DMNTbfOP6RsQrhuTKZ0FcvUJSn6T5YLQQY4i2wwDmpxguOm1I1hIshivp7oN+V9ornr09VJMh7lVtCaK2UUmDk9swlEVr5KAIpy9rYXR2eRuGGx44Frun8HKh6njSZ73SC9z8njlGC8FnV7ZVAPzeQMIoDkYVg/IPPtBty9hBTYkxxE2w6oo9znD0ZEzkKy4m8tWWXUlJqNfaFKMY4ibm6LjE6YulGGypGGIMA7VvGC5R0oXjjE2necdzTsOWgRxe8YxNIVeZ2JM6FGPgf2dpLbxObsNQCbKSAP/DXt+XXdaKYSJfwV9cdx9+eN/RpR4KZss1/NfeY3XHq64nszhWcvDZ8fwd1iwjeT9jgWi5Lbhg2yAMAk5f11M3MaqKYaA7UAwijjFdrNbFJfzXBf+ucZNtT9bCyHQJVdeTvv75ohqBdTztVc2KSVIM6rjOPsXPqupEAz2BVAwN6hgaxxhav9bTT+mX9xPN2kqDlz/jFPzmw5eiyzZD9ydbYiiuJPG3kNb/10ltGOaiXQxNbhiWsWIQGSszyyBz6r8eGMF7v3UfnhzNh45XHU+u2la0K8nl+yXwSuVGjM1VQqv8T7z2mTjwiT/Erz70wroUSVUx9OfsuiZ6U8VancoAIoohzjBkLBnX2LDA3j5q+wkxGVsN0lXlcT6pbejLYjvfPS4NxRDNyvKPNchKMv1d3NrJjjIMki1JFjPGACDWMER3sAO0YkiFfEwXw+WuGGRPnGWQOSWC9I8dnw0drzie/EdaycFn12MwDQOmYTR1ie07PoezN/Y1PEegrrwHcrbM8S9VXTDmN96LcyVZTVxJ3VkT47ygbP0CYwxxrdLDiiF56hjotrF1MIeNvDAubqzzHhef2GMrn00DZ2zoDWX0qK+bz+Y5rzxvE05dm6trS542quGL65UkjqX1/7UkhoGITiWim4noESJ6mIjez4//PREdJaL7+ddlaY4j2vrCX+l0YZQ3DluOiBqLZtXZdx6YiN3ysaNj4WMQnUAFNTdQDCvdlRT05U++j1LVxcGJAs4+pTXDoPrHB7ozMmd9olBFoer62UpxwWc1aB1jOFQ/uKhW7iSqYWrUMffpm/px4bZBbOTbinZUMZjJriTDIPzygy/Aq3dtqX+dQfPah/nVu7bgtr+6tOVtPTuFbQb7VMt0VcWVtFoVgwPgLxlj5wC4BMB7iOgc/txnGWO7+NdP0xxEdNUtYgyFqptqS9uFICbjRv2cxuYqeONX78D39gzHPl91PNxzaHLhY+E53o8dDxuG1eRKsgzfldSojuGJUb/KN26lGoc6yQzkbJlaOp6vyESIuBiD6kqIjzEEk/WGBcYYBK/fvTX2+tG22yrXvP0ifPSV58hW1WnUMcQphoavM4yOFdktBkQkjV9cS4wgxpBOcsdSbe05AmCEP54jokcB1Jv5lInbKUn8Q43NVRbdr9gKM1IxJBuGI1NFMAYcnS7FPv+TB4/hf3/nAfzmw5fKFN35IBVDxJXkB59XiWIQezI3iDEIxfS0U1rbtUtd+YsYw0C3zQ2D/5k2zUqKmWy7uWLoso2OpFce+KfLQnn/rSoGwdNO6YdlEHas61nwWARmg+Bz49e1F3heDmQtE47L5JamJ1WMgYh2ALgAwJ380HuJaC8RXU1EgwmvuZKI9hDRnrGxsXlfO84wiGyO5RpnmInsIRzHyLTvCjs+E+8SO9bk+VYRbq3hqZJ0bbkeg+sxmcWxkhWDK7KSzMbpqo8en0XONrFtbXfiOSqi51BflyUnunW9WYzPVWXcJi74HM1miiIUw4a+ro6sjg2DQu9jKO6YVibmczb346F/eBl2ru9d8FgEJh9Du64dyzDmFWNYSsT2pcIg2KGspHQLSJfUMBBRL4AfAPgAY2wWwJcB7ASwC76i+HTc6xhjVzHGdjPGdq9fv37e189XnJAkzVqm9M0u11qGmRZiDCMzvlI4kRArEf36JxdYCzFbduQf6OMn/FWzMARi9Vpd5nUMc+UajiUoq5rrwTKMpumq+47P4axT+lre21ooBnVyX9ebwXi+IrPO4lxJYcUQU+DGP/OFpqo2wjL8Wo1W77XTu/eZBsVmJLXyOnOlGQbb/9uL6w+1ahUDEdnwjcI3GWM/BADG2AnGmMsY8wB8FcBFaY4hX3ZCvtgMDz4DwNjc8gxAt5KVJBRBkmEQ/fonC/XGb3S2jHLNrTsex1yphmds8btz7jvup6yKP9SM5W+svtxdSZ/5xeO4/Au/iVUEssCtSbrq4yfyOHtj66tiYUzVyX1dbxYThaqMMaxpEGMgis+rF+6jTsUX4rBMkpsXLQWmQW3HF8TrVphdQNYyYRKhJ2siZ5shgy8U26qqYyBfn34NwKOMsc8oxzcpp/0RgIfSHMdcJWIYTAMDORuWQXjo2OyyzE4Sfv1GMQahGI7PlmN7qYh+/SK1UcAYw2Wfux1fumV/y2M5e2MfLIMwzLd9rLhBsaBtUuqupCOTxeYnNeDgeAHj+UpdnATgBW4m+emqCYah4vhN67YOtuZGAoIJXg3K+q4kJcYQowiE0ujvsmNX7KKxXpqKwTRoSffwXohhaFXlLBeylgHTJHRnLNz6Vy/EK8/bLJ8TwefVphieA+AKAJdGUlM/SUQPEtFeAC8C8L/TGkDF8fdgEAoBgJTIWwdz+P49w/ijL/028fVThSoePjbT8dYUk4UqvAarU6EYSjU3tEewyjEeOyjXvNg9rJNcSTOlGsbzFexXCtZmirXY8TDGMFtyMNCdweaBHIanfGMk/lCzvD1Emorhvx8cwfM+eTN+9tDIvN/jOHcZ3nmgPktLpKvaDdJVRZxm05rWC8rEBK+qgvV9WcxVHByfLaM3a8VOfiL4m1QX0LMIisE2jaU1DEq2TruvW4kxBuH+2tDXFbuD3apSDIyx2xljxBg7T01NZYxdwRh7Jj9+Oc9eSgXRDVPN9xZ/cF9/x8V488XbcHS6FNq/VuW1X/4t/vBzt+O1X042Hu2Srzh43r/8Ct/dcyTxnJmSEzo/jpHpkmxiF+dOSjIMwgUlspkm8hVc8omb8I5r766rtK44Hqquh/6chS0DOakYVFeSzTtANrrfpj2IGpzzH785CAD42A0Pz3vXPaEK7zgwETrOGOOuJAOmQYlpgeIz29xGdpdY7amZRaK30SPHZhM7o4qNapKa0gWGYWFVz40wDWqYqpo281UMa7rtRWlr0UmylpmocuzVWOC2HBA++o1K6wAR3Nk21I1Lz94AANg/nq97bbnm4qnxAnoyJp4aL7Tsk2/GU2MFFKou9hyaSjxntlSTmSFx7qSq42EsX8H5p/pte6OGgTGGCe5CmogYBuGCEt/3HZ9Dqebiln1j+Mvv3l83DsB3a2wdzEljIv5QM5avGJL+cGuuhxd+6mZ87fYDiffqegwv+NTN+PrvDoaOF6sOfvvkOO46OInLz9+ME7MV/Ocdh8EYa+t3UXFc+Rnc+dRkSBkJY2QZ/l7HScZJfFZtKYaYlb8ocrv/yLT83cVhGpTYe0gYl+jeCp3ENqgju7HNl/kGn9/zojPwn++8OIURpYcIPseRWaWupCVHrLaFP9Y2wz7InRv8YOL+0XrDcJj7tZ+9c13o54VyaNLvc7MvUjCmMlOqyTbIcYbhxGwZjAG7+OQSTUnNVxw5WU9EUnKFC2p0roKa62E/32jmDbtPxU2PjeLwRHCfItbRn7OxdbAbJ2YrKNdclERDQlMEn+Mn1H3H5zCer8pWw3GcmC1jPF/Fw8cC/z9jDK/54m/w5v93J7KWgX+4/FxsGcjhseOz+NH9R/Gsj/9Sjq0ZY3P+/V982lrMlGrYdyL43EXdgh9jSK5jGJGupNYn41zGhGVQaGW/TnH/qF09o1gGxdY4AP7eCj/889/Hs3cOxT7fCUyTYjfJWSx6s9a82lOsydk4tcV04uVC1kpOsZVZSVoxdBZhGIQ/NtrG99TBHGyTsH+sUPdasTPXC85aF/q5FVyPJW6ucYhPvI+fmItdoTquh3zFkUVpca4kMVHtSlAMwn2UMQ1MFqpwPb83T9XxZNomY/7r9o/m0ZMx8YGXnAmDCN+885B8H+HS6u+y5Ar1//zoIRmXCVxJ8Sv4vcMzAOqL41RE3EK4qVyP4fETeTx+Io+3XLwN3/qzSzDYk8HODb3YP5bHnQcmMVd2sPfIDBhL/pwF4rN5xTNOAQA8cGRaPicMgW34QfRoPEe8/7HpEga77ab7K6t0Zyx8/3/9Pl6/+1R5bJ2yx+8lpydP7D1Zq2EfpAu3DaZa4WsbSxtj+Njl5+JTrzt/ya6/mPR32Yl7TYv5qpaSYlhZTrcOkq+IfPFMqI++wDIN7Bjqwf6xQDEwxkBEUiE8/yy/hqJVxeB5DG/66h0YyNm46q27654/xDtjVhwPBycKdYVBQiFsGcwBB+NrGY5O+2M5bV0PBrptnIjUY4hMpNPX9+DAeAF/+h934bYnxrFpTRd27whWqiMzZewfy2Pnhl5sWpPDS8/ZiO/uOYIPvexs2KYRUgzC5/yDe4MWHN0Zkwef4ydnMQkfmSwhX3FiK3WFQRieKuEne0fwkesfxMvO3QgAeP+Lz5QdRHeu78Geg5MyUPfA8DQ+9fPH8Kwda/HRV55T977i9yg+m4tOG0J/l4UHhmfwRp4g7fJxmzyPPGqov3P3Efzrjfuwc31vW2pBsCviLhKupPV9WZzeoFL46rc9S+6PsBTkMuaS+uoXUqm/0vjgS89K7KJsa8WQDmes78PHXnUOTl3bjVzGjPVb7lzvr0SPTBbxmi/+Bh/4ju9nPzhRQH+Xhe1Dfq990epYMDxVxK7/eyPuOxx2k3zzrsO466lJ3PjICRwYq3dRHZooyn+6x0bmcGiigPP/4UY8dHQGn/r5Y7j8i7cDCP454lxJDx2dRdYysGNdD07p76or3hKK4YwNvag6Hm5/chw7hroxMlPGrftGZWHVsekSDowV5CT1mgu2YKpYw91P+dk7oRgDl+geAz5y2dPwhTdfgF2nDsA2kwvDHhielr7qJNeZUAwjM2Xc/uQYZko1fHfPMC7YNhBqK71zfS+KVRcPHvVVyI/uO4oHhmdw/X1H6yb0b/zuIH7vH3+JXzxyIpRRdN7WAewdDhRDjWch+S0x6rOrfnDvMMbzVdz51KTc+nIhdNkm1uRsXHL6UMMV/zO3rgltFL/YfOp15+OvX/a0Jbv+ycSmNbnENiuZlHslnbSGYdtQN97+nNOwtieDHr66jbJzQw8OTRRx+Rdux/1HpvGTvSOYKdVwaKIo+79sH+qWLiDBLfvGMF2s4dbHx/CFXz2BK752J8o1F5/82WO4YNsALIPwzTsP113v0EQRLzhrPUyD8NjxWfzqsVHMlPz3uenRURyZ9CdK4bqJa4uxd3ga527uh20aOHfzGtx3ZDoUVBVFbWdu8Bu+MQZ84A/OAuBXMl+4ze9Csn+sgKPTJalann/menTZBm585IQ8FwD6cxY29mVlkOxV52/GK8/bDMv0XUmVGKlbqrp4YjSPy57hl60kuZNUF9It+8bkP8NLztkYOk+M0WO+C+sJHheaKFRx7+Ep1FwPr/i32/DB79yPf/rpY8iXHfzZ1/fg+vuOImMZGOi2cf6pa7Dv+JwMXgfBZ6OuwG08XwklCMxHMcTxlSt+Dx9+xfKedM/Z3I9tQyvLV78aEUWScf9fneCkNQwquQTDcMaGXrgew/q+LD7z+vPheAy37BvFoYmi7IuzfainzpUkUh/3Ds/gx/cfw21PjON3+ycwV3bw7hfsxMufcQq+t+eIDNQCfqbT8dkyztrYh9PW9WDv8IzMrb/jwIRsOQFwVxKAb95xCH/4udvkatZxPTx0dFZuJH7J6WsxWajKiRIIXEln8Upd2yS87NxTcPp639CdubEPfVkLv31yHEAQhM9lTDzvzPW48eHjvIYhUAyWaWDzQA7P3LImNEnG1TEwxvCfdxyC6zG8/BmnoC9r4cu37MdLPnNrXU3I8FRJZmCNzJTxut1b8cnXnYcrLtkeOm/nhsD1IuIFZ27ohW0Sbnz4OPYdn8OjI7P44X1HQQT89weeh439WTx4dAYb+7MgIpy3dQCOx3D5F27HG77yO5llJVoSqCuzXz06CsaAZ2zxV3Odcu38/+3de3hU9ZnA8e+byWUSchkI5B4CCQgkQsJlIwoKrBAulsu6tCi1dl37oFR2bbu6tbWt1vaxdq21dV191MdbvaBdLdVaFnVjK0pFIEK5FTFyDXIVgiAhkOTdP87JMBMyZIEkE3Pez/PkyeR3Zs78fvOcOW/O7/Ke0YXpnuoqMedORIhvYzr4+fDsGEOoHgmxraZVnjY0m7oTTcwoyyEpzsfdizexeN1udtXWMaPUWYVY0CuJxet2c7KxiThfDKrKcveEvmLrweAAcfPAbVl+gEBiHK+t3c1LH9TwP+t2s2ZnLVnudMeC9CQmDsnksXe2BPvu360+gCoMzkph054jZKX6iY2RYLrr5Vs+5cG3qskNJFJ3spHSfCdNRfMg5pPLtvLORweoPXaCHgmxJMX7gvPuS/MCJMb7GF2Yzpb9n5MT8JMd8LPaHQMIHeeoKM7kzY17GXrnGzSpEh97aiDynn8cSqo/fBplQqyPd6sPMPGXb/PcNy4ikBTHj36/gRdX7WT8oD6MH5TBoKyU4H/f/121k3mXFQVfX3OojqG5afzVHaguzk4NG7Bt1ic5gRR/LEfrG5g9Mo9X1nzCrOG5wW675qu7n105lIL0JIr6JHN1eV9+9b8fBVNDl7rBdMfBY9QcquNKdxDdmR4ZQ+2xE2w98Dkp/lie+ss2cgOJLJgwkBufrbKTuYmKM3XVni8LDDg3OKlvZfZMQqyPuRf1Df49qTiDhSucxWfNl9MF6Uk0NimLVu/izY17OXLcWT1ckpMaNs2yctM+MlMTyEz1k5GSwKDMFO76wwZONipzRuWzZMMed389GNG3J48s/Zij9Q0UZ6eycbeznwfnjuDdj/YzICOZFH9sMOHaT17byOa9p64Kmk9y+b2SyA0k8sLKnfROjmdySRa/W72L/F6JwemRzcFjdGE6z7+/g+y0RHICiWzee5RZZTnBKwuALw3LYeehOpZVH6CqxVqLS9ypu6Hmjy+iX3oSzyzfzm0vr6W27iSrd9SyYMIAvj3pAnwxwg+/VMyez47z+DtbeXb5Dr4xtpAYd7D3k9o6KoozWbvrMHqG+x2ICEV9kjlcd5IxRb25Y3oxV47II5AUx+2L1vNSVQ2BpDiu+rv8YP/91eV9efCt6uBYRVaan1/NKWNIdir1DY3MeHAZ4IwxXF3elyXr9zD5/qXExDjdb7++qoyJQzK4Y3oxE4dktlovYzrSdWP6MywvrUP2bYEB+NbEgW2uwAW4cVwRCbE+4nxChdvPXVGcxZPZ2/j3l9YS74tBcfYz77JCbn5hDUnxPnr1iKfmUF2wi0dE+NrFBfzg9+u5Ylg2P589jG9OKOJPm/YxLDeNmBhhwqAM3tq0jxvGOfvJSfMzICOZAW7XToo/jkPHTlLYpweb9x6ld3I8DW7K637pp7pWRhem8/IHNfxoegkzSnOYNTyXxiYlJ83PHdOLuWJYttuOTL47ZTCXDuxNRkoCs8pymVmWEzYQmhjv4zuTLuCai/pSfndlm5/XyIKejCzoSSApjl+8sZmkeB8Pf3UEU4eeSolVmh+gFKev9F8Xrqb87kquH9ufmWU5NDQphX2SyUzxB7vZIrlt6mDqG5qIiRGuG9MfgElDMrl90XpW76jl0oG9w9qSmern/jllYZ/VrOGnbgkybWgWi9ft4URDE8PyAry6YCy/eW8bJxuV2SPzgskDm9/LmM52y+RBHbZvCwzARWeYNx6qIL0Hd84oCStLS4rj5fmX8OjSLYwb5ExfXVtTy5QLs4jzCaP69SKQGEfNobqwKYqzR+ax77PjXHtJv+C+/ynkJPP9aUMYM6A3FcVZ+GIkGFSaJSc46wfmjyvi1pfWMre8L6ML09l3pD5sod4N4woZkp3CdDcANE+xhfCTmj/Ox/zxTjdOaX7gjKtvM1L9PHN9+RkT+YVq7h6qKMmKeHKfemEWN44romr7Qe59fVNwkDy3ZyJ5PROJixVS/JHvBNba3P+MVD/D+wZYvaP2tOmh4AyUR3Lfl8sYnLUleDWQ3yuJ2684feqrMd2RtLUIqKsbNWqUrlq1KtrVaNXCFTu4IDOZNTsP85PXNvLM9eVcOvDs7x/x3PvbKclJCzu5LVm/m/jYGEYXpvNAZTU3XFZIzwg5dr5IPjt+ksvve5v9R+oZlpfGi/MuZsW2gxyrbwi70vj/eujP1fzHkg957NpRp81mMsbLRKRKVU9fUIUFhk6x/0g9jy79mFsmDzqnPC9e8/6WT1lWfYBvThhw3qtsPz1az0N//phbKgad1epkY7o7CwzGGGPCnCkw2DoGY4wxYSwwGGOMCWOBwRhjTJguGRhEZIqIfCgi1SJyW7TrY4wxXtLlAoOI+ID/AqYCxcDVImITyI0xppN0ucAAlAPVqrpFVU8ALwAzo1wnY4zxjK4YGHKBnSF/17hlQSIyT0RWiciq/fv3d2rljDGmu+uKgaFNqvqoqo5S1VF9+pz9SmJjjDGRdcVcSbuA0NzKeW5Zq6qqqg6IyPZI29vQGzhwjq/tLrz+GVj7rf1ebX9BpA1dbuWziMQCm4HLcQLCSmCuqm7ogPdaFWnln1d4/TOw9lv7vdz+SLrcFYOqNojIAuB1wAc80RFBwRhjTOu6XGAAUNXFwOJo18MYY7zoCzn43I4ejXYFugCvfwbWfm/zevtb1eXGGIwxxkSX168YjDHGtGCBwRhjTBjPBgYvJuoTkW0isk5E1ojIKresl4i8KSIfub97Rrue7UlEnhCRfSKyPqSs1TaL4wH3mFgrIiOiV/P2EaH9d4rILvc4WCMi00K2fc9t/4ciMjk6tW4/IpIvIn8SkY0iskFEbnbLPXMMnAtPBgaPJ+qboKplIXO3bwMqVXUgUOn+3Z08BUxpURapzVOBge7PPODhTqpjR3qK09sPcL97HJS5swBxvwNXASXuax5yvytfZA3Av6lqMTAauMltp5eOgbPmycCAJeoLNRN42n38NDArinVpd6q6FDjYojhSm2cCv1HHciAgItmdU9OOEaH9kcwEXlDVelXdClTjfFe+sFR1t6p+4D4+AvwNJ/eaZ46Bc+HVwNBmor5uSoE3RKRKROa5ZZmqutt9vAfIjE7VOlWkNnvpuFjgdpU8EdJ92K3bLyL9gOHA+9gxcEZeDQxeNVZVR+BcLt8kIpeFblRn7rKn5i97sc043SNFQBmwG7gvutXpeCKSDLwMfEtVPwvd5tFj4Iy8GhjOKlFfd6Gqu9zf+4BFON0Ee5svld3f+6JXw04Tqc2eOC5Uda+qNqpqE/AYp7qLumX7RSQOJyg8p6q/c4s9fQy0xauBYSUwUET6i0g8zoDbq1GuU4cSkR4iktL8GKgA1uP71x/vAAADNklEQVS0++vu074OvBKdGnaqSG1+FbjWnZkyGjgc0t3QbbToM/8HnOMAnPZfJSIJItIfZwB2RWfXrz2JiACPA39T1V+GbPL0MdCWLpkrqaN5NFFfJrDI+Z4QCzyvqktEZCXwWxG5HtgOfCWKdWx3IrIQGA/0FpEa4A7gHlpv82JgGs6g6zHguk6vcDuL0P7xIlKG032yDbgBQFU3iMhvgY04s3luUtXGaNS7HY0BvgasE5E1btn38dAxcC4sJYYxxpgwXu1KMsYYE4EFBmOMMWEsMBhjjAljgcEYY0wYCwzGGGPCWGAw5hyIyF0iMrEd9nO0PepjTHuy6arGRJGIHFXV5GjXw5hQdsVgjEtErhGRFe49Ch4REZ+IHBWR+91c/pUi0sd97lMiMtt9fI+b73+tiPzCLesnIm+5ZZUi0tct7y8i74lzX4yftnj/W0VkpfuaH7tlPUTkjyLyVxFZLyJzOvdTMV5kgcEYQESGAHOAMapaBjQCXwV6AKtUtQR4G2flcOjr0nHSSpSo6jCg+WT/n8DTbtlzwANu+a+Bh1V1KE4Cu+b9VOCkoCjHSW430k1yOAX4RFVLVfVCYEm7N96YFiwwGOO4HBgJrHRTJ1wOFAJNwIvuc54FxrZ43WHgOPC4iFyJk0YB4GLgeffxMyGvGwMsDClvVuH+rAY+AAbjBIp1wCQR+bmIXKqqh8+znca0yZO5koxpheD8h/+9sEKRH7Z4XtignJt3qxwnkMwGFgB/38Z7tTawJ8DPVPWR0zY4t5ecBvxURCpV9a429m/MebErBmMclcBsEcmA4D2BC3C+I7Pd58wF3g19kZvnP829Pea3gVJ3019wsvaC0yX1jvt4WYvyZq8D/+zuDxHJFZEMEckBjqnqs8C9gCfvQWw6l10xGAOo6kYR+QHOHe5igJPATcDnQLm7bR/OOESoFOAVEfHj/Nf/Hbf8X4AnReRWYD+nsnTeDDwvIt8lJMW5qr7hjnO852bAPQpcAwwA7hWRJrdO89u35caczqarGnMGNp3UeJF1JRljjAljVwzGGGPC2BWDMcaYMBYYjDHGhLHAYIwxJowFBmOMMWEsMBhjjAnzf+0oXmKNjquvAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 200.000, steps: 200\n",
            "Episode 6: reward: 200.000, steps: 200\n",
            "Episode 7: reward: 200.000, steps: 200\n",
            "Episode 8: reward: 176.000, steps: 176\n",
            "Episode 9: reward: 200.000, steps: 200\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 200.000, steps: 200\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 200.000, steps: 200\n",
            "Episode 15: reward: 200.000, steps: 200\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 200.000, steps: 200\n",
            "Episode 18: reward: 151.000, steps: 151\n",
            "Episode 19: reward: 200.000, steps: 200\n",
            "Episode 20: reward: 200.000, steps: 200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb1703b8a10>"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    }
  ]
}