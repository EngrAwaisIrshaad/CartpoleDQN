{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPoleDQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Megacity1/CartpoleDQN/blob/main/Week%203%20Deep%20RL%202/CartPoleDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Deep Q Network (DQN) for CartPole Using Boltzmann Q Policy\n",
        "This exercise implements a DQN for CartPole using a Boltzmann Q policy for selecting the actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGsC7cJ5jNcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda73cbf-3020-40f4-e45e-7b87053448f1"
      },
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.24.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (13.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.44.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.10.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMIHLgQ3Z-lF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5a8608-456f-4127-ce8a-f551e5f4e4f3"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0AMLzq08ap0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b380e1-6c3d-4bce-8233-a8ac01deaf26"
      },
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "!pip install Adam\n",
        "#from keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Adam in /usr/local/lib/python3.7/dist-packages (0.0.0.dev0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll6bNdUm54WS"
      },
      "source": [
        "Implementation of DQN for CartPole, applying policy BoltzmannQPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCrPKNy40PC"
      },
      "source": [
        "##Implement DQN with BoltzmannGumbelQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efM9jkXr5A3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bf6993bd-4dd6-49a8-a62c-a9a42544661f"
      },
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import BoltzmannGumbelQPolicy\n",
        "from rl.policy import LinearAnnealedPolicy\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n",
        "# setup experience replay buffer\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
        "                               attr='eps',            \n",
        "                               value_max=5.,\n",
        "                               value_min=.5, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=200)\n",
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "# add extra layers here\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=20,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy) \n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=8000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_54\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_52 (Flatten)        (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_105 (Dense)           (None, 16)                80        \n",
            "                                                                 \n",
            " dense_106 (Dense)           (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 8000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   38/8000: episode: 1, duration: 18.900s, episode steps:  38, steps per second:   2, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  loss: 0.509322, mae: 0.601407, mean_q: -0.097637, mean_eps: 4.347500\n",
            "   52/8000: episode: 2, duration: 0.452s, episode steps:  14, steps per second:  31, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.459584, mae: 0.600489, mean_q: -0.107048, mean_eps: 3.998750\n",
            "   62/8000: episode: 3, duration: 0.315s, episode steps:  10, steps per second:  32, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.412253, mae: 0.537117, mean_q: 0.022377, mean_eps: 3.728750\n",
            "   83/8000: episode: 4, duration: 0.697s, episode steps:  21, steps per second:  30, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 0.372485, mae: 0.510838, mean_q: 0.115477, mean_eps: 3.380000\n",
            "  109/8000: episode: 5, duration: 0.665s, episode steps:  26, steps per second:  39, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.332014, mae: 0.517122, mean_q: 0.235132, mean_eps: 2.851250\n",
            "  137/8000: episode: 6, duration: 0.671s, episode steps:  28, steps per second:  42, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 0.297174, mae: 0.529201, mean_q: 0.345387, mean_eps: 2.243750\n",
            "  171/8000: episode: 7, duration: 0.762s, episode steps:  34, steps per second:  45, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.618 [0.000, 1.000],  loss: 0.269964, mae: 0.558981, mean_q: 0.477056, mean_eps: 1.546250\n",
            "  181/8000: episode: 8, duration: 0.315s, episode steps:  10, steps per second:  32, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.239720, mae: 0.599462, mean_q: 0.615931, mean_eps: 1.051250\n",
            "  213/8000: episode: 9, duration: 1.012s, episode steps:  32, steps per second:  32, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.221847, mae: 0.649605, mean_q: 0.727423, mean_eps: 0.633594\n",
            "  237/8000: episode: 10, duration: 0.843s, episode steps:  24, steps per second:  28, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.189987, mae: 0.706208, mean_q: 0.891146, mean_eps: 0.500000\n",
            "  249/8000: episode: 11, duration: 0.448s, episode steps:  12, steps per second:  27, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.174760, mae: 0.745398, mean_q: 0.997981, mean_eps: 0.500000\n",
            "  267/8000: episode: 12, duration: 0.589s, episode steps:  18, steps per second:  31, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.722 [0.000, 1.000],  loss: 0.171361, mae: 0.796515, mean_q: 1.126834, mean_eps: 0.500000\n",
            "  281/8000: episode: 13, duration: 0.493s, episode steps:  14, steps per second:  28, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.160736, mae: 0.837121, mean_q: 1.252245, mean_eps: 0.500000\n",
            "  293/8000: episode: 14, duration: 0.386s, episode steps:  12, steps per second:  31, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.160943, mae: 0.895183, mean_q: 1.362899, mean_eps: 0.500000\n",
            "  321/8000: episode: 15, duration: 0.882s, episode steps:  28, steps per second:  32, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.679 [0.000, 1.000],  loss: 0.147335, mae: 0.919968, mean_q: 1.447112, mean_eps: 0.500000\n",
            "  331/8000: episode: 16, duration: 0.328s, episode steps:  10, steps per second:  30, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.150743, mae: 0.996947, mean_q: 1.628112, mean_eps: 0.500000\n",
            "  348/8000: episode: 17, duration: 0.569s, episode steps:  17, steps per second:  30, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.160471, mae: 1.046005, mean_q: 1.736822, mean_eps: 0.500000\n",
            "  363/8000: episode: 18, duration: 0.454s, episode steps:  15, steps per second:  33, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 0.157865, mae: 1.089278, mean_q: 1.823765, mean_eps: 0.500000\n",
            "  381/8000: episode: 19, duration: 0.622s, episode steps:  18, steps per second:  29, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.195492, mae: 1.179571, mean_q: 2.000527, mean_eps: 0.500000\n",
            "  407/8000: episode: 20, duration: 0.720s, episode steps:  26, steps per second:  36, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.190807, mae: 1.233280, mean_q: 2.134448, mean_eps: 0.500000\n",
            "  420/8000: episode: 21, duration: 0.340s, episode steps:  13, steps per second:  38, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.199796, mae: 1.302240, mean_q: 2.273938, mean_eps: 0.500000\n",
            "  434/8000: episode: 22, duration: 0.392s, episode steps:  14, steps per second:  36, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 0.226736, mae: 1.378378, mean_q: 2.425701, mean_eps: 0.500000\n",
            "  445/8000: episode: 23, duration: 0.334s, episode steps:  11, steps per second:  33, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.281165, mae: 1.419199, mean_q: 2.473524, mean_eps: 0.500000\n",
            "  466/8000: episode: 24, duration: 0.530s, episode steps:  21, steps per second:  40, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 0.265614, mae: 1.440547, mean_q: 2.554416, mean_eps: 0.500000\n",
            "  478/8000: episode: 25, duration: 0.306s, episode steps:  12, steps per second:  39, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.245336, mae: 1.486295, mean_q: 2.686566, mean_eps: 0.500000\n",
            "  490/8000: episode: 26, duration: 0.278s, episode steps:  12, steps per second:  43, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.249600, mae: 1.542116, mean_q: 2.808617, mean_eps: 0.500000\n",
            "  508/8000: episode: 27, duration: 0.424s, episode steps:  18, steps per second:  42, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.309374, mae: 1.636207, mean_q: 2.959014, mean_eps: 0.500000\n",
            "  519/8000: episode: 28, duration: 0.378s, episode steps:  11, steps per second:  29, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.296600, mae: 1.671486, mean_q: 3.097963, mean_eps: 0.500000\n",
            "  538/8000: episode: 29, duration: 0.549s, episode steps:  19, steps per second:  35, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.789 [0.000, 1.000],  loss: 0.360391, mae: 1.756491, mean_q: 3.218720, mean_eps: 0.500000\n",
            "  561/8000: episode: 30, duration: 0.727s, episode steps:  23, steps per second:  32, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.696 [0.000, 1.000],  loss: 0.327387, mae: 1.798363, mean_q: 3.298901, mean_eps: 0.500000\n",
            "  571/8000: episode: 31, duration: 0.298s, episode steps:  10, steps per second:  34, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.343319, mae: 1.880640, mean_q: 3.554826, mean_eps: 0.500000\n",
            "  580/8000: episode: 32, duration: 0.256s, episode steps:   9, steps per second:  35, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.287070, mae: 1.900756, mean_q: 3.652531, mean_eps: 0.500000\n",
            "  592/8000: episode: 33, duration: 0.317s, episode steps:  12, steps per second:  38, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.370172, mae: 1.926446, mean_q: 3.708513, mean_eps: 0.500000\n",
            "  601/8000: episode: 34, duration: 0.210s, episode steps:   9, steps per second:  43, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.528659, mae: 2.031880, mean_q: 3.777789, mean_eps: 0.500000\n",
            "  617/8000: episode: 35, duration: 0.443s, episode steps:  16, steps per second:  36, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.457557, mae: 2.067676, mean_q: 3.792208, mean_eps: 0.500000\n",
            "  627/8000: episode: 36, duration: 0.332s, episode steps:  10, steps per second:  30, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.460714, mae: 2.103480, mean_q: 3.907384, mean_eps: 0.500000\n",
            "  636/8000: episode: 37, duration: 0.265s, episode steps:   9, steps per second:  34, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.444816, mae: 2.133170, mean_q: 3.994947, mean_eps: 0.500000\n",
            "  647/8000: episode: 38, duration: 0.316s, episode steps:  11, steps per second:  35, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.458799, mae: 2.190646, mean_q: 4.109692, mean_eps: 0.500000\n",
            "  659/8000: episode: 39, duration: 0.363s, episode steps:  12, steps per second:  33, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.499768, mae: 2.234918, mean_q: 4.208261, mean_eps: 0.500000\n",
            "  673/8000: episode: 40, duration: 0.428s, episode steps:  14, steps per second:  33, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.441726, mae: 2.260075, mean_q: 4.270633, mean_eps: 0.500000\n",
            "  686/8000: episode: 41, duration: 0.390s, episode steps:  13, steps per second:  33, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.557772, mae: 2.337366, mean_q: 4.286002, mean_eps: 0.500000\n",
            "  698/8000: episode: 42, duration: 0.372s, episode steps:  12, steps per second:  32, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.393782, mae: 2.329295, mean_q: 4.314169, mean_eps: 0.500000\n",
            "  709/8000: episode: 43, duration: 0.382s, episode steps:  11, steps per second:  29, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.410740, mae: 2.383161, mean_q: 4.466661, mean_eps: 0.500000\n",
            "  723/8000: episode: 44, duration: 0.480s, episode steps:  14, steps per second:  29, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 0.449098, mae: 2.450226, mean_q: 4.540320, mean_eps: 0.500000\n",
            "  737/8000: episode: 45, duration: 0.431s, episode steps:  14, steps per second:  32, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.390432, mae: 2.482935, mean_q: 4.634889, mean_eps: 0.500000\n",
            "  748/8000: episode: 46, duration: 0.344s, episode steps:  11, steps per second:  32, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.304290, mae: 2.497829, mean_q: 4.726780, mean_eps: 0.500000\n",
            "  758/8000: episode: 47, duration: 0.341s, episode steps:  10, steps per second:  29, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.356608, mae: 2.567054, mean_q: 4.841962, mean_eps: 0.500000\n",
            "  769/8000: episode: 48, duration: 0.333s, episode steps:  11, steps per second:  33, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.470902, mae: 2.618693, mean_q: 4.913928, mean_eps: 0.500000\n",
            "  778/8000: episode: 49, duration: 0.315s, episode steps:   9, steps per second:  29, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.425596, mae: 2.631016, mean_q: 4.922938, mean_eps: 0.500000\n",
            "  789/8000: episode: 50, duration: 0.379s, episode steps:  11, steps per second:  29, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.564246, mae: 2.701694, mean_q: 4.887145, mean_eps: 0.500000\n",
            "  801/8000: episode: 51, duration: 0.385s, episode steps:  12, steps per second:  31, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.472767, mae: 2.697530, mean_q: 4.941179, mean_eps: 0.500000\n",
            "  829/8000: episode: 52, duration: 0.829s, episode steps:  28, steps per second:  34, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.607 [0.000, 1.000],  loss: 0.428741, mae: 2.733282, mean_q: 5.203077, mean_eps: 0.500000\n",
            "  840/8000: episode: 53, duration: 0.338s, episode steps:  11, steps per second:  33, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.517066, mae: 2.831208, mean_q: 5.237263, mean_eps: 0.500000\n",
            "  853/8000: episode: 54, duration: 0.429s, episode steps:  13, steps per second:  30, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.404888, mae: 2.841006, mean_q: 5.322035, mean_eps: 0.500000\n",
            "  864/8000: episode: 55, duration: 0.375s, episode steps:  11, steps per second:  29, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.563716, mae: 2.893960, mean_q: 5.392091, mean_eps: 0.500000\n",
            "  881/8000: episode: 56, duration: 0.546s, episode steps:  17, steps per second:  31, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.546458, mae: 2.922246, mean_q: 5.473816, mean_eps: 0.500000\n",
            "  895/8000: episode: 57, duration: 0.496s, episode steps:  14, steps per second:  28, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.490988, mae: 2.949040, mean_q: 5.491107, mean_eps: 0.500000\n",
            "  914/8000: episode: 58, duration: 0.666s, episode steps:  19, steps per second:  29, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.477036, mae: 2.988668, mean_q: 5.667199, mean_eps: 0.500000\n",
            "  932/8000: episode: 59, duration: 0.635s, episode steps:  18, steps per second:  28, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.514862, mae: 3.050060, mean_q: 5.688043, mean_eps: 0.500000\n",
            "  941/8000: episode: 60, duration: 0.323s, episode steps:   9, steps per second:  28, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.466504, mae: 3.097173, mean_q: 5.749669, mean_eps: 0.500000\n",
            "  973/8000: episode: 61, duration: 1.048s, episode steps:  32, steps per second:  31, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.421424, mae: 3.153588, mean_q: 5.944310, mean_eps: 0.500000\n",
            "  991/8000: episode: 62, duration: 0.585s, episode steps:  18, steps per second:  31, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 0.512476, mae: 3.232146, mean_q: 5.989716, mean_eps: 0.500000\n",
            " 1005/8000: episode: 63, duration: 0.534s, episode steps:  14, steps per second:  26, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.482084, mae: 3.288336, mean_q: 6.141095, mean_eps: 0.500000\n",
            " 1020/8000: episode: 64, duration: 0.495s, episode steps:  15, steps per second:  30, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.371876, mae: 3.269531, mean_q: 6.135692, mean_eps: 0.500000\n",
            " 1031/8000: episode: 65, duration: 0.271s, episode steps:  11, steps per second:  41, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.269749, mae: 3.278380, mean_q: 6.234893, mean_eps: 0.500000\n",
            " 1043/8000: episode: 66, duration: 0.301s, episode steps:  12, steps per second:  40, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.342112, mae: 3.363980, mean_q: 6.393007, mean_eps: 0.500000\n",
            " 1056/8000: episode: 67, duration: 0.343s, episode steps:  13, steps per second:  38, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.393235, mae: 3.419168, mean_q: 6.470287, mean_eps: 0.500000\n",
            " 1077/8000: episode: 68, duration: 0.635s, episode steps:  21, steps per second:  33, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.610443, mae: 3.505585, mean_q: 6.503286, mean_eps: 0.500000\n",
            " 1094/8000: episode: 69, duration: 0.560s, episode steps:  17, steps per second:  30, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: 0.586146, mae: 3.538005, mean_q: 6.608173, mean_eps: 0.500000\n",
            " 1108/8000: episode: 70, duration: 0.472s, episode steps:  14, steps per second:  30, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.582571, mae: 3.589677, mean_q: 6.710256, mean_eps: 0.500000\n",
            " 1125/8000: episode: 71, duration: 0.644s, episode steps:  17, steps per second:  26, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.514813, mae: 3.642939, mean_q: 6.830996, mean_eps: 0.500000\n",
            " 1158/8000: episode: 72, duration: 0.985s, episode steps:  33, steps per second:  34, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 0.475791, mae: 3.677018, mean_q: 6.903842, mean_eps: 0.500000\n",
            " 1173/8000: episode: 73, duration: 0.499s, episode steps:  15, steps per second:  30, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.426756, mae: 3.727323, mean_q: 7.053090, mean_eps: 0.500000\n",
            " 1182/8000: episode: 74, duration: 0.346s, episode steps:   9, steps per second:  26, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.773127, mae: 3.860847, mean_q: 7.210822, mean_eps: 0.500000\n",
            " 1197/8000: episode: 75, duration: 0.510s, episode steps:  15, steps per second:  29, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.508469, mae: 3.828280, mean_q: 7.226404, mean_eps: 0.500000\n",
            " 1211/8000: episode: 76, duration: 0.458s, episode steps:  14, steps per second:  31, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.617141, mae: 3.851045, mean_q: 7.207431, mean_eps: 0.500000\n",
            " 1224/8000: episode: 77, duration: 0.408s, episode steps:  13, steps per second:  32, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.763935, mae: 3.966642, mean_q: 7.425594, mean_eps: 0.500000\n",
            " 1283/8000: episode: 78, duration: 1.535s, episode steps:  59, steps per second:  38, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.390 [0.000, 1.000],  loss: 0.735121, mae: 4.044473, mean_q: 7.583086, mean_eps: 0.500000\n",
            " 1293/8000: episode: 79, duration: 0.252s, episode steps:  10, steps per second:  40, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.650838, mae: 4.077192, mean_q: 7.637896, mean_eps: 0.500000\n",
            " 1304/8000: episode: 80, duration: 0.292s, episode steps:  11, steps per second:  38, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.800737, mae: 4.195214, mean_q: 7.887085, mean_eps: 0.500000\n",
            " 1320/8000: episode: 81, duration: 0.386s, episode steps:  16, steps per second:  41, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.883767, mae: 4.238047, mean_q: 7.941377, mean_eps: 0.500000\n",
            " 1330/8000: episode: 82, duration: 0.283s, episode steps:  10, steps per second:  35, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.607318, mae: 4.282661, mean_q: 8.112386, mean_eps: 0.500000\n",
            " 1342/8000: episode: 83, duration: 0.268s, episode steps:  12, steps per second:  45, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 1.020708, mae: 4.321307, mean_q: 8.107302, mean_eps: 0.500000\n",
            " 1351/8000: episode: 84, duration: 0.239s, episode steps:   9, steps per second:  38, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.523298, mae: 4.311177, mean_q: 8.200957, mean_eps: 0.500000\n",
            " 1365/8000: episode: 85, duration: 0.338s, episode steps:  14, steps per second:  41, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.722755, mae: 4.395393, mean_q: 8.342682, mean_eps: 0.500000\n",
            " 1375/8000: episode: 86, duration: 0.330s, episode steps:  10, steps per second:  30, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.920713, mae: 4.374024, mean_q: 8.253526, mean_eps: 0.500000\n",
            " 1389/8000: episode: 87, duration: 0.476s, episode steps:  14, steps per second:  29, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.143 [0.000, 1.000],  loss: 1.152674, mae: 4.502560, mean_q: 8.363537, mean_eps: 0.500000\n",
            " 1402/8000: episode: 88, duration: 0.437s, episode steps:  13, steps per second:  30, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 1.156561, mae: 4.424555, mean_q: 8.127126, mean_eps: 0.500000\n",
            " 1431/8000: episode: 89, duration: 0.974s, episode steps:  29, steps per second:  30, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 0.861992, mae: 4.517527, mean_q: 8.499528, mean_eps: 0.500000\n",
            " 1441/8000: episode: 90, duration: 0.323s, episode steps:  10, steps per second:  31, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.997491, mae: 4.563859, mean_q: 8.598944, mean_eps: 0.500000\n",
            " 1459/8000: episode: 91, duration: 0.579s, episode steps:  18, steps per second:  31, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.069486, mae: 4.592976, mean_q: 8.669326, mean_eps: 0.500000\n",
            " 1473/8000: episode: 92, duration: 0.491s, episode steps:  14, steps per second:  29, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 1.157646, mae: 4.693727, mean_q: 8.762869, mean_eps: 0.500000\n",
            " 1490/8000: episode: 93, duration: 0.540s, episode steps:  17, steps per second:  31, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: 1.312584, mae: 4.710052, mean_q: 8.747615, mean_eps: 0.500000\n",
            " 1502/8000: episode: 94, duration: 0.403s, episode steps:  12, steps per second:  30, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 1.037359, mae: 4.772089, mean_q: 8.935069, mean_eps: 0.500000\n",
            " 1510/8000: episode: 95, duration: 0.293s, episode steps:   8, steps per second:  27, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.125 [0.000, 1.000],  loss: 1.386263, mae: 4.820410, mean_q: 9.021554, mean_eps: 0.500000\n",
            " 1524/8000: episode: 96, duration: 0.456s, episode steps:  14, steps per second:  31, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.143 [0.000, 1.000],  loss: 1.067512, mae: 4.841431, mean_q: 9.104356, mean_eps: 0.500000\n",
            " 1545/8000: episode: 97, duration: 0.666s, episode steps:  21, steps per second:  32, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.934859, mae: 4.827727, mean_q: 9.101603, mean_eps: 0.500000\n",
            " 1558/8000: episode: 98, duration: 0.398s, episode steps:  13, steps per second:  33, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.964433, mae: 4.880830, mean_q: 9.283453, mean_eps: 0.500000\n",
            " 1569/8000: episode: 99, duration: 0.363s, episode steps:  11, steps per second:  30, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.957500, mae: 4.941554, mean_q: 9.454077, mean_eps: 0.500000\n",
            " 1584/8000: episode: 100, duration: 0.418s, episode steps:  15, steps per second:  36, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 1.120194, mae: 4.936820, mean_q: 9.269206, mean_eps: 0.500000\n",
            " 1612/8000: episode: 101, duration: 0.826s, episode steps:  28, steps per second:  34, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 1.243884, mae: 5.011336, mean_q: 9.369745, mean_eps: 0.500000\n",
            " 1630/8000: episode: 102, duration: 0.547s, episode steps:  18, steps per second:  33, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 1.311338, mae: 5.026996, mean_q: 9.370813, mean_eps: 0.500000\n",
            " 1656/8000: episode: 103, duration: 0.759s, episode steps:  26, steps per second:  34, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1.087514, mae: 5.067387, mean_q: 9.514895, mean_eps: 0.500000\n",
            " 1676/8000: episode: 104, duration: 0.636s, episode steps:  20, steps per second:  31, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 1.392857, mae: 5.203700, mean_q: 9.745801, mean_eps: 0.500000\n",
            " 1698/8000: episode: 105, duration: 0.676s, episode steps:  22, steps per second:  33, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1.134296, mae: 5.208282, mean_q: 9.770274, mean_eps: 0.500000\n",
            " 1729/8000: episode: 106, duration: 0.971s, episode steps:  31, steps per second:  32, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 1.392977, mae: 5.277078, mean_q: 9.884117, mean_eps: 0.500000\n",
            " 1792/8000: episode: 107, duration: 1.954s, episode steps:  63, steps per second:  32, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 1.402237, mae: 5.321459, mean_q: 9.923038, mean_eps: 0.500000\n",
            " 1825/8000: episode: 108, duration: 1.021s, episode steps:  33, steps per second:  32, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 1.093587, mae: 5.391965, mean_q: 10.148889, mean_eps: 0.500000\n",
            " 1876/8000: episode: 109, duration: 1.521s, episode steps:  51, steps per second:  34, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 1.389719, mae: 5.513936, mean_q: 10.352161, mean_eps: 0.500000\n",
            " 1932/8000: episode: 110, duration: 1.717s, episode steps:  56, steps per second:  33, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 0.950702, mae: 5.575343, mean_q: 10.578278, mean_eps: 0.500000\n",
            " 1968/8000: episode: 111, duration: 1.080s, episode steps:  36, steps per second:  33, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 1.458943, mae: 5.717258, mean_q: 10.835657, mean_eps: 0.500000\n",
            " 1993/8000: episode: 112, duration: 0.816s, episode steps:  25, steps per second:  31, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 1.175138, mae: 5.833873, mean_q: 11.129850, mean_eps: 0.500000\n",
            " 2116/8000: episode: 113, duration: 3.124s, episode steps: 123, steps per second:  39, episode reward: 123.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 1.368925, mae: 5.969477, mean_q: 11.327711, mean_eps: 0.500000\n",
            " 2129/8000: episode: 114, duration: 0.320s, episode steps:  13, steps per second:  41, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 1.363388, mae: 6.179711, mean_q: 11.746117, mean_eps: 0.500000\n",
            " 2189/8000: episode: 115, duration: 1.616s, episode steps:  60, steps per second:  37, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1.335253, mae: 6.207620, mean_q: 11.831649, mean_eps: 0.500000\n",
            " 2309/8000: episode: 116, duration: 3.403s, episode steps: 120, steps per second:  35, episode reward: 120.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 1.327443, mae: 6.457553, mean_q: 12.389150, mean_eps: 0.500000\n",
            " 2392/8000: episode: 117, duration: 2.337s, episode steps:  83, steps per second:  36, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 1.531640, mae: 6.737774, mean_q: 12.918687, mean_eps: 0.500000\n",
            " 2445/8000: episode: 118, duration: 1.606s, episode steps:  53, steps per second:  33, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1.618225, mae: 6.868143, mean_q: 13.111555, mean_eps: 0.500000\n",
            " 2487/8000: episode: 119, duration: 1.294s, episode steps:  42, steps per second:  32, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 1.574559, mae: 7.026612, mean_q: 13.522336, mean_eps: 0.500000\n",
            " 2534/8000: episode: 120, duration: 1.446s, episode steps:  47, steps per second:  32, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.880768, mae: 7.113959, mean_q: 13.596940, mean_eps: 0.500000\n",
            " 2557/8000: episode: 121, duration: 0.708s, episode steps:  23, steps per second:  32, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 1.920135, mae: 7.149670, mean_q: 13.632593, mean_eps: 0.500000\n",
            " 2658/8000: episode: 122, duration: 2.734s, episode steps: 101, steps per second:  37, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 1.683173, mae: 7.286731, mean_q: 14.032104, mean_eps: 0.500000\n",
            " 2692/8000: episode: 123, duration: 0.814s, episode steps:  34, steps per second:  42, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.780418, mae: 7.469944, mean_q: 14.431333, mean_eps: 0.500000\n",
            " 2752/8000: episode: 124, duration: 1.532s, episode steps:  60, steps per second:  39, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.600239, mae: 7.571734, mean_q: 14.656273, mean_eps: 0.500000\n",
            " 2821/8000: episode: 125, duration: 1.658s, episode steps:  69, steps per second:  42, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  loss: 2.382484, mae: 7.797860, mean_q: 14.928940, mean_eps: 0.500000\n",
            " 2927/8000: episode: 126, duration: 2.572s, episode steps: 106, steps per second:  41, episode reward: 106.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 2.192145, mae: 8.018921, mean_q: 15.460220, mean_eps: 0.500000\n",
            " 2975/8000: episode: 127, duration: 1.399s, episode steps:  48, steps per second:  34, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 1.631893, mae: 8.247645, mean_q: 15.997755, mean_eps: 0.500000\n",
            " 3067/8000: episode: 128, duration: 2.547s, episode steps:  92, steps per second:  36, episode reward: 92.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.874834, mae: 8.389092, mean_q: 16.301820, mean_eps: 0.500000\n",
            " 3120/8000: episode: 129, duration: 1.330s, episode steps:  53, steps per second:  40, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 1.972684, mae: 8.545465, mean_q: 16.587163, mean_eps: 0.500000\n",
            " 3193/8000: episode: 130, duration: 1.768s, episode steps:  73, steps per second:  41, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 1.813193, mae: 8.766957, mean_q: 17.155618, mean_eps: 0.500000\n",
            " 3276/8000: episode: 131, duration: 2.086s, episode steps:  83, steps per second:  40, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 2.745053, mae: 8.973722, mean_q: 17.386227, mean_eps: 0.500000\n",
            " 3333/8000: episode: 132, duration: 1.275s, episode steps:  57, steps per second:  45, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 2.759075, mae: 9.132592, mean_q: 17.659124, mean_eps: 0.500000\n",
            " 3406/8000: episode: 133, duration: 1.495s, episode steps:  73, steps per second:  49, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 2.609149, mae: 9.287891, mean_q: 18.028624, mean_eps: 0.500000\n",
            " 3433/8000: episode: 134, duration: 0.602s, episode steps:  27, steps per second:  45, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 2.342388, mae: 9.386644, mean_q: 18.298406, mean_eps: 0.500000\n",
            " 3567/8000: episode: 135, duration: 2.876s, episode steps: 134, steps per second:  47, episode reward: 134.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 2.669404, mae: 9.510528, mean_q: 18.463496, mean_eps: 0.500000\n",
            " 3732/8000: episode: 136, duration: 4.110s, episode steps: 165, steps per second:  40, episode reward: 165.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 2.626205, mae: 9.843257, mean_q: 19.177572, mean_eps: 0.500000\n",
            " 3826/8000: episode: 137, duration: 2.167s, episode steps:  94, steps per second:  43, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 2.464480, mae: 10.189989, mean_q: 19.963220, mean_eps: 0.500000\n",
            " 3844/8000: episode: 138, duration: 0.511s, episode steps:  18, steps per second:  35, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 2.338343, mae: 10.291524, mean_q: 20.169574, mean_eps: 0.500000\n",
            " 3884/8000: episode: 139, duration: 1.049s, episode steps:  40, steps per second:  38, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.425 [0.000, 1.000],  loss: 2.837983, mae: 10.421594, mean_q: 20.396032, mean_eps: 0.500000\n",
            " 3951/8000: episode: 140, duration: 1.553s, episode steps:  67, steps per second:  43, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 3.143659, mae: 10.573039, mean_q: 20.675718, mean_eps: 0.500000\n",
            " 4030/8000: episode: 141, duration: 1.760s, episode steps:  79, steps per second:  45, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 2.537013, mae: 10.672792, mean_q: 20.948584, mean_eps: 0.500000\n",
            " 4053/8000: episode: 142, duration: 0.622s, episode steps:  23, steps per second:  37, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 3.343038, mae: 10.873833, mean_q: 21.363333, mean_eps: 0.500000\n",
            " 4097/8000: episode: 143, duration: 1.169s, episode steps:  44, steps per second:  38, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.880698, mae: 10.866248, mean_q: 21.380875, mean_eps: 0.500000\n",
            " 4190/8000: episode: 144, duration: 2.102s, episode steps:  93, steps per second:  44, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 3.321337, mae: 11.090603, mean_q: 21.691247, mean_eps: 0.500000\n",
            " 4247/8000: episode: 145, duration: 1.254s, episode steps:  57, steps per second:  45, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  loss: 3.363898, mae: 11.246866, mean_q: 22.019222, mean_eps: 0.500000\n",
            " 4306/8000: episode: 146, duration: 1.409s, episode steps:  59, steps per second:  42, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 2.772509, mae: 11.334106, mean_q: 22.359096, mean_eps: 0.500000\n",
            " 4351/8000: episode: 147, duration: 1.184s, episode steps:  45, steps per second:  38, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 2.122669, mae: 11.448004, mean_q: 22.722430, mean_eps: 0.500000\n",
            " 4388/8000: episode: 148, duration: 0.925s, episode steps:  37, steps per second:  40, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 3.532130, mae: 11.650851, mean_q: 22.980873, mean_eps: 0.500000\n",
            " 4436/8000: episode: 149, duration: 1.296s, episode steps:  48, steps per second:  37, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 2.821509, mae: 11.573595, mean_q: 22.862130, mean_eps: 0.500000\n",
            " 4563/8000: episode: 150, duration: 2.928s, episode steps: 127, steps per second:  43, episode reward: 127.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 3.686543, mae: 11.916368, mean_q: 23.396134, mean_eps: 0.500000\n",
            " 4672/8000: episode: 151, duration: 2.275s, episode steps: 109, steps per second:  48, episode reward: 109.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 4.010935, mae: 12.201479, mean_q: 23.921284, mean_eps: 0.500000\n",
            " 4780/8000: episode: 152, duration: 2.499s, episode steps: 108, steps per second:  43, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 3.416196, mae: 12.369380, mean_q: 24.373422, mean_eps: 0.500000\n",
            " 4897/8000: episode: 153, duration: 2.744s, episode steps: 117, steps per second:  43, episode reward: 117.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 4.114893, mae: 12.680830, mean_q: 24.942515, mean_eps: 0.500000\n",
            " 4975/8000: episode: 154, duration: 2.136s, episode steps:  78, steps per second:  37, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 4.862793, mae: 12.892905, mean_q: 25.309073, mean_eps: 0.500000\n",
            " 5022/8000: episode: 155, duration: 1.264s, episode steps:  47, steps per second:  37, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 3.924467, mae: 13.007559, mean_q: 25.649725, mean_eps: 0.500000\n",
            " 5117/8000: episode: 156, duration: 2.151s, episode steps:  95, steps per second:  44, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 4.270669, mae: 13.069537, mean_q: 25.752554, mean_eps: 0.500000\n",
            " 5137/8000: episode: 157, duration: 0.499s, episode steps:  20, steps per second:  40, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 3.024875, mae: 13.310636, mean_q: 26.450432, mean_eps: 0.500000\n",
            " 5178/8000: episode: 158, duration: 1.005s, episode steps:  41, steps per second:  41, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 4.776016, mae: 13.319957, mean_q: 26.273931, mean_eps: 0.500000\n",
            " 5258/8000: episode: 159, duration: 1.885s, episode steps:  80, steps per second:  42, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 4.474415, mae: 13.483745, mean_q: 26.586028, mean_eps: 0.500000\n",
            " 5355/8000: episode: 160, duration: 2.344s, episode steps:  97, steps per second:  41, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 4.069082, mae: 13.671829, mean_q: 27.046379, mean_eps: 0.500000\n",
            " 5466/8000: episode: 161, duration: 2.631s, episode steps: 111, steps per second:  42, episode reward: 111.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 5.032992, mae: 13.913822, mean_q: 27.444661, mean_eps: 0.500000\n",
            " 5592/8000: episode: 162, duration: 3.098s, episode steps: 126, steps per second:  41, episode reward: 126.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.013442, mae: 14.092800, mean_q: 27.811082, mean_eps: 0.500000\n",
            " 5732/8000: episode: 163, duration: 2.760s, episode steps: 140, steps per second:  51, episode reward: 140.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 4.831378, mae: 14.422836, mean_q: 28.480895, mean_eps: 0.500000\n",
            " 5877/8000: episode: 164, duration: 3.407s, episode steps: 145, steps per second:  43, episode reward: 145.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 5.098514, mae: 14.682849, mean_q: 28.952670, mean_eps: 0.500000\n",
            " 5980/8000: episode: 165, duration: 2.067s, episode steps: 103, steps per second:  50, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.408 [0.000, 1.000],  loss: 4.355054, mae: 14.808570, mean_q: 29.425711, mean_eps: 0.500000\n",
            " 6033/8000: episode: 166, duration: 1.386s, episode steps:  53, steps per second:  38, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 5.384471, mae: 14.948385, mean_q: 29.525154, mean_eps: 0.500000\n",
            " 6092/8000: episode: 167, duration: 1.558s, episode steps:  59, steps per second:  38, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 5.600218, mae: 15.064135, mean_q: 29.874543, mean_eps: 0.500000\n",
            " 6209/8000: episode: 168, duration: 3.099s, episode steps: 117, steps per second:  38, episode reward: 117.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 3.957053, mae: 15.330538, mean_q: 30.549951, mean_eps: 0.500000\n",
            " 6285/8000: episode: 169, duration: 1.572s, episode steps:  76, steps per second:  48, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 4.614378, mae: 15.539480, mean_q: 30.989864, mean_eps: 0.500000\n",
            " 6346/8000: episode: 170, duration: 1.501s, episode steps:  61, steps per second:  41, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 5.975535, mae: 15.658530, mean_q: 31.130244, mean_eps: 0.500000\n",
            " 6425/8000: episode: 171, duration: 2.108s, episode steps:  79, steps per second:  37, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 6.782229, mae: 15.903771, mean_q: 31.550479, mean_eps: 0.500000\n",
            " 6609/8000: episode: 172, duration: 4.898s, episode steps: 184, steps per second:  38, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 4.393878, mae: 16.134164, mean_q: 32.268438, mean_eps: 0.500000\n",
            " 6809/8000: episode: 173, duration: 4.651s, episode steps: 200, steps per second:  43, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 5.761945, mae: 16.709633, mean_q: 33.330885, mean_eps: 0.500000\n",
            " 7009/8000: episode: 174, duration: 4.364s, episode steps: 200, steps per second:  46, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.507650, mae: 17.179229, mean_q: 34.282053, mean_eps: 0.500000\n",
            " 7191/8000: episode: 175, duration: 3.830s, episode steps: 182, steps per second:  48, episode reward: 182.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 5.387302, mae: 17.620622, mean_q: 35.240015, mean_eps: 0.500000\n",
            " 7289/8000: episode: 176, duration: 2.127s, episode steps:  98, steps per second:  46, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 5.381134, mae: 17.911023, mean_q: 35.890616, mean_eps: 0.500000\n",
            " 7351/8000: episode: 177, duration: 1.757s, episode steps:  62, steps per second:  35, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 6.034398, mae: 18.164125, mean_q: 36.398760, mean_eps: 0.500000\n",
            " 7472/8000: episode: 178, duration: 2.680s, episode steps: 121, steps per second:  45, episode reward: 121.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 6.555058, mae: 18.405903, mean_q: 36.847156, mean_eps: 0.500000\n",
            " 7672/8000: episode: 179, duration: 4.088s, episode steps: 200, steps per second:  49, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 7.177162, mae: 18.754147, mean_q: 37.505720, mean_eps: 0.500000\n",
            " 7872/8000: episode: 180, duration: 4.819s, episode steps: 200, steps per second:  42, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 6.138308, mae: 19.302928, mean_q: 38.708760, mean_eps: 0.500000\n",
            "done, took 223.398 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 200.000, steps: 200\n",
            "Episode 6: reward: 200.000, steps: 200\n",
            "Episode 7: reward: 200.000, steps: 200\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 200.000, steps: 200\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 200.000, steps: 200\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 200.000, steps: 200\n",
            "Episode 15: reward: 200.000, steps: 200\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 200.000, steps: 200\n",
            "Episode 18: reward: 200.000, steps: 200\n",
            "Episode 19: reward: 200.000, steps: 200\n",
            "Episode 20: reward: 200.000, steps: 200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb16f5308d0>"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    }
  ]
}