{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPoleDQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Megacity1/CartpoleDQN/blob/main/Week%203%20Deep%20RL%202/CartPoleDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Deep Q Network (DQN) for CartPole Using Boltzmann Q Policy\n",
        "This exercise implements a DQN for CartPole using a Boltzmann Q policy for selecting the actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGsC7cJ5jNcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda73cbf-3020-40f4-e45e-7b87053448f1"
      },
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.24.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (13.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.44.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.10.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMIHLgQ3Z-lF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5a8608-456f-4127-ce8a-f551e5f4e4f3"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0AMLzq08ap0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b380e1-6c3d-4bce-8233-a8ac01deaf26"
      },
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "!pip install Adam\n",
        "#from keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Adam in /usr/local/lib/python3.7/dist-packages (0.0.0.dev0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll6bNdUm54WS"
      },
      "source": [
        "Implementation of DQN for CartPole, applying policy BoltzmannQPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCrPKNy40PC"
      },
      "source": [
        "##Implement DQN with BoltzmannGumbelQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efM9jkXr5A3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d5a30def-52ae-45ce-a5a5-df748081bd68"
      },
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import BoltzmannGumbelQPolicy\n",
        "from rl.policy import LinearAnnealedPolicy\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n",
        "# setup experience replay buffer\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
        "                               attr='eps',            \n",
        "                               value_max=5.,\n",
        "                               value_min=.5, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=20)\n",
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "# add extra layers here\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=20,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy) \n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=8000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_51\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_49 (Flatten)        (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_99 (Dense)            (None, 16)                80        \n",
            "                                                                 \n",
            " dense_100 (Dense)           (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 8000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   21/8000: episode: 1, duration: 16.211s, episode steps:  21, steps per second:   1, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   32/8000: episode: 2, duration: 0.276s, episode steps:  11, steps per second:  40, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.768732, mae: 0.691188, mean_q: 0.268512, mean_eps: 0.500000\n",
            "   49/8000: episode: 3, duration: 0.279s, episode steps:  17, steps per second:  61, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.765 [0.000, 1.000],  loss: 0.549060, mae: 0.610362, mean_q: 0.350501, mean_eps: 0.500000\n",
            "   63/8000: episode: 4, duration: 0.247s, episode steps:  14, steps per second:  57, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 0.427708, mae: 0.586997, mean_q: 0.499089, mean_eps: 0.500000\n",
            "   73/8000: episode: 5, duration: 0.274s, episode steps:  10, steps per second:  37, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.344303, mae: 0.599069, mean_q: 0.664660, mean_eps: 0.500000\n",
            "  137/8000: episode: 6, duration: 1.459s, episode steps:  64, steps per second:  44, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.453 [0.000, 1.000],  loss: 0.287315, mae: 0.705898, mean_q: 0.910903, mean_eps: 0.500000\n",
            "  150/8000: episode: 7, duration: 0.409s, episode steps:  13, steps per second:  32, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.237331, mae: 0.880168, mean_q: 1.291074, mean_eps: 0.500000\n",
            "  174/8000: episode: 8, duration: 0.657s, episode steps:  24, steps per second:  37, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.220486, mae: 0.930873, mean_q: 1.412495, mean_eps: 0.500000\n",
            "  183/8000: episode: 9, duration: 0.276s, episode steps:   9, steps per second:  33, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.204728, mae: 0.935818, mean_q: 1.433268, mean_eps: 0.500000\n",
            "  194/8000: episode: 10, duration: 0.308s, episode steps:  11, steps per second:  36, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.208153, mae: 0.989474, mean_q: 1.574907, mean_eps: 0.500000\n",
            "  205/8000: episode: 11, duration: 0.291s, episode steps:  11, steps per second:  38, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.202650, mae: 1.039306, mean_q: 1.746049, mean_eps: 0.500000\n",
            "  227/8000: episode: 12, duration: 0.552s, episode steps:  22, steps per second:  40, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.226167, mae: 1.099061, mean_q: 1.858051, mean_eps: 0.500000\n",
            "  245/8000: episode: 13, duration: 0.461s, episode steps:  18, steps per second:  39, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.722 [0.000, 1.000],  loss: 0.242657, mae: 1.140174, mean_q: 1.931984, mean_eps: 0.500000\n",
            "  257/8000: episode: 14, duration: 0.318s, episode steps:  12, steps per second:  38, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.230336, mae: 1.171997, mean_q: 2.022909, mean_eps: 0.500000\n",
            "  266/8000: episode: 15, duration: 0.236s, episode steps:   9, steps per second:  38, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.207424, mae: 1.206015, mean_q: 2.170229, mean_eps: 0.500000\n",
            "  274/8000: episode: 16, duration: 0.187s, episode steps:   8, steps per second:  43, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.233886, mae: 1.247694, mean_q: 2.272061, mean_eps: 0.500000\n",
            "  286/8000: episode: 17, duration: 0.278s, episode steps:  12, steps per second:  43, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.345884, mae: 1.376698, mean_q: 2.465476, mean_eps: 0.500000\n",
            "  296/8000: episode: 18, duration: 0.290s, episode steps:  10, steps per second:  35, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.332747, mae: 1.378388, mean_q: 2.408417, mean_eps: 0.500000\n",
            "  317/8000: episode: 19, duration: 0.527s, episode steps:  21, steps per second:  40, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.339520, mae: 1.424577, mean_q: 2.569481, mean_eps: 0.500000\n",
            "  330/8000: episode: 20, duration: 0.247s, episode steps:  13, steps per second:  53, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.304550, mae: 1.416001, mean_q: 2.621245, mean_eps: 0.500000\n",
            "  340/8000: episode: 21, duration: 0.211s, episode steps:  10, steps per second:  47, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.320057, mae: 1.484985, mean_q: 2.768689, mean_eps: 0.500000\n",
            "  353/8000: episode: 22, duration: 0.283s, episode steps:  13, steps per second:  46, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.846 [0.000, 1.000],  loss: 0.387268, mae: 1.520185, mean_q: 2.869528, mean_eps: 0.500000\n",
            "  365/8000: episode: 23, duration: 0.219s, episode steps:  12, steps per second:  55, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.373441, mae: 1.583370, mean_q: 3.044199, mean_eps: 0.500000\n",
            "  375/8000: episode: 24, duration: 0.171s, episode steps:  10, steps per second:  58, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.399134, mae: 1.619533, mean_q: 3.074158, mean_eps: 0.500000\n",
            "  385/8000: episode: 25, duration: 0.187s, episode steps:  10, steps per second:  53, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.443679, mae: 1.709834, mean_q: 3.249555, mean_eps: 0.500000\n",
            "  401/8000: episode: 26, duration: 0.285s, episode steps:  16, steps per second:  56, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.367793, mae: 1.674932, mean_q: 3.262104, mean_eps: 0.500000\n",
            "  413/8000: episode: 27, duration: 0.224s, episode steps:  12, steps per second:  54, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.572963, mae: 1.789160, mean_q: 3.354848, mean_eps: 0.500000\n",
            "  425/8000: episode: 28, duration: 0.219s, episode steps:  12, steps per second:  55, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.418489, mae: 1.780139, mean_q: 3.358824, mean_eps: 0.500000\n",
            "  440/8000: episode: 29, duration: 0.276s, episode steps:  15, steps per second:  54, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.449540, mae: 1.862480, mean_q: 3.584849, mean_eps: 0.500000\n",
            "  458/8000: episode: 30, duration: 0.338s, episode steps:  18, steps per second:  53, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 0.317001, mae: 1.855275, mean_q: 3.702522, mean_eps: 0.500000\n",
            "  470/8000: episode: 31, duration: 0.211s, episode steps:  12, steps per second:  57, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 0.505437, mae: 1.992030, mean_q: 3.894501, mean_eps: 0.500000\n",
            "  486/8000: episode: 32, duration: 0.269s, episode steps:  16, steps per second:  59, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.514914, mae: 2.070661, mean_q: 3.941650, mean_eps: 0.500000\n",
            "  498/8000: episode: 33, duration: 0.196s, episode steps:  12, steps per second:  61, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.407560, mae: 2.068819, mean_q: 3.994466, mean_eps: 0.500000\n",
            "  508/8000: episode: 34, duration: 0.181s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.513342, mae: 2.140044, mean_q: 4.107453, mean_eps: 0.500000\n",
            "  520/8000: episode: 35, duration: 0.192s, episode steps:  12, steps per second:  63, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.518718, mae: 2.162567, mean_q: 4.134232, mean_eps: 0.500000\n",
            "  532/8000: episode: 36, duration: 0.226s, episode steps:  12, steps per second:  53, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.577214, mae: 2.227712, mean_q: 4.255843, mean_eps: 0.500000\n",
            "  542/8000: episode: 37, duration: 0.179s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.630348, mae: 2.295580, mean_q: 4.346971, mean_eps: 0.500000\n",
            "  561/8000: episode: 38, duration: 0.325s, episode steps:  19, steps per second:  58, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.684 [0.000, 1.000],  loss: 0.500430, mae: 2.314995, mean_q: 4.353051, mean_eps: 0.500000\n",
            "  572/8000: episode: 39, duration: 0.187s, episode steps:  11, steps per second:  59, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.623585, mae: 2.370418, mean_q: 4.549435, mean_eps: 0.500000\n",
            "  584/8000: episode: 40, duration: 0.230s, episode steps:  12, steps per second:  52, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.594398, mae: 2.424504, mean_q: 4.640221, mean_eps: 0.500000\n",
            "  601/8000: episode: 41, duration: 0.301s, episode steps:  17, steps per second:  57, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.627687, mae: 2.447382, mean_q: 4.689204, mean_eps: 0.500000\n",
            "  611/8000: episode: 42, duration: 0.299s, episode steps:  10, steps per second:  33, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.501663, mae: 2.456817, mean_q: 4.684892, mean_eps: 0.500000\n",
            "  624/8000: episode: 43, duration: 0.385s, episode steps:  13, steps per second:  34, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.846 [0.000, 1.000],  loss: 0.579223, mae: 2.561258, mean_q: 4.826129, mean_eps: 0.500000\n",
            "  633/8000: episode: 44, duration: 0.269s, episode steps:   9, steps per second:  33, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.422287, mae: 2.559890, mean_q: 4.970141, mean_eps: 0.500000\n",
            "  646/8000: episode: 45, duration: 0.406s, episode steps:  13, steps per second:  32, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.846 [0.000, 1.000],  loss: 0.751404, mae: 2.673499, mean_q: 5.038686, mean_eps: 0.500000\n",
            "  656/8000: episode: 46, duration: 0.309s, episode steps:  10, steps per second:  32, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.658662, mae: 2.665734, mean_q: 4.992651, mean_eps: 0.500000\n",
            "  674/8000: episode: 47, duration: 0.407s, episode steps:  18, steps per second:  44, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.539750, mae: 2.711239, mean_q: 5.092064, mean_eps: 0.500000\n",
            "  683/8000: episode: 48, duration: 0.254s, episode steps:   9, steps per second:  35, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.609844, mae: 2.749186, mean_q: 5.220848, mean_eps: 0.500000\n",
            "  693/8000: episode: 49, duration: 0.286s, episode steps:  10, steps per second:  35, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.597807, mae: 2.753575, mean_q: 5.228619, mean_eps: 0.500000\n",
            "  714/8000: episode: 50, duration: 0.502s, episode steps:  21, steps per second:  42, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.592142, mae: 2.819693, mean_q: 5.314771, mean_eps: 0.500000\n",
            "  724/8000: episode: 51, duration: 0.243s, episode steps:  10, steps per second:  41, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.730376, mae: 2.870080, mean_q: 5.359374, mean_eps: 0.500000\n",
            "  736/8000: episode: 52, duration: 0.331s, episode steps:  12, steps per second:  36, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.609283, mae: 2.873308, mean_q: 5.342715, mean_eps: 0.500000\n",
            "  748/8000: episode: 53, duration: 0.351s, episode steps:  12, steps per second:  34, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.570498, mae: 2.950882, mean_q: 5.538258, mean_eps: 0.500000\n",
            "  778/8000: episode: 54, duration: 0.842s, episode steps:  30, steps per second:  36, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.595783, mae: 2.969991, mean_q: 5.575346, mean_eps: 0.500000\n",
            "  787/8000: episode: 55, duration: 0.245s, episode steps:   9, steps per second:  37, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.661685, mae: 3.055864, mean_q: 5.633019, mean_eps: 0.500000\n",
            "  800/8000: episode: 56, duration: 0.380s, episode steps:  13, steps per second:  34, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.479618, mae: 3.036357, mean_q: 5.612107, mean_eps: 0.500000\n",
            "  813/8000: episode: 57, duration: 0.356s, episode steps:  13, steps per second:  37, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.606022, mae: 3.119795, mean_q: 5.878186, mean_eps: 0.500000\n",
            "  834/8000: episode: 58, duration: 0.528s, episode steps:  21, steps per second:  40, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.621193, mae: 3.192871, mean_q: 5.897907, mean_eps: 0.500000\n",
            "  850/8000: episode: 59, duration: 0.386s, episode steps:  16, steps per second:  41, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.446140, mae: 3.200005, mean_q: 6.022172, mean_eps: 0.500000\n",
            "  932/8000: episode: 60, duration: 1.860s, episode steps:  82, steps per second:  44, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 0.615290, mae: 3.398149, mean_q: 6.341220, mean_eps: 0.500000\n",
            "  980/8000: episode: 61, duration: 0.915s, episode steps:  48, steps per second:  52, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 0.580334, mae: 3.612176, mean_q: 6.765590, mean_eps: 0.500000\n",
            " 1006/8000: episode: 62, duration: 0.447s, episode steps:  26, steps per second:  58, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.577 [0.000, 1.000],  loss: 0.557095, mae: 3.699826, mean_q: 6.913178, mean_eps: 0.500000\n",
            " 1035/8000: episode: 63, duration: 0.488s, episode steps:  29, steps per second:  59, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.379 [0.000, 1.000],  loss: 0.744858, mae: 3.808417, mean_q: 7.143555, mean_eps: 0.500000\n",
            " 1051/8000: episode: 64, duration: 0.263s, episode steps:  16, steps per second:  61, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.575922, mae: 3.886421, mean_q: 7.346397, mean_eps: 0.500000\n",
            " 1065/8000: episode: 65, duration: 0.241s, episode steps:  14, steps per second:  58, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.835721, mae: 3.905008, mean_q: 7.310208, mean_eps: 0.500000\n",
            " 1126/8000: episode: 66, duration: 1.060s, episode steps:  61, steps per second:  58, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.426 [0.000, 1.000],  loss: 0.609631, mae: 4.021020, mean_q: 7.612458, mean_eps: 0.500000\n",
            " 1140/8000: episode: 67, duration: 0.293s, episode steps:  14, steps per second:  48, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.214 [0.000, 1.000],  loss: 0.533491, mae: 4.173553, mean_q: 7.959210, mean_eps: 0.500000\n",
            " 1155/8000: episode: 68, duration: 0.264s, episode steps:  15, steps per second:  57, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.751425, mae: 4.259912, mean_q: 8.109850, mean_eps: 0.500000\n",
            " 1172/8000: episode: 69, duration: 0.311s, episode steps:  17, steps per second:  55, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.235 [0.000, 1.000],  loss: 0.964818, mae: 4.297290, mean_q: 8.076224, mean_eps: 0.500000\n",
            " 1189/8000: episode: 70, duration: 0.300s, episode steps:  17, steps per second:  57, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: 0.784872, mae: 4.259695, mean_q: 8.045608, mean_eps: 0.500000\n",
            " 1206/8000: episode: 71, duration: 0.330s, episode steps:  17, steps per second:  51, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.858934, mae: 4.382414, mean_q: 8.280987, mean_eps: 0.500000\n",
            " 1215/8000: episode: 72, duration: 0.174s, episode steps:   9, steps per second:  52, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.697589, mae: 4.399781, mean_q: 8.438801, mean_eps: 0.500000\n",
            " 1226/8000: episode: 73, duration: 0.223s, episode steps:  11, steps per second:  49, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 1.206018, mae: 4.527875, mean_q: 8.617348, mean_eps: 0.500000\n",
            " 1243/8000: episode: 74, duration: 0.309s, episode steps:  17, steps per second:  55, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.176 [0.000, 1.000],  loss: 1.237796, mae: 4.632681, mean_q: 8.773862, mean_eps: 0.500000\n",
            " 1253/8000: episode: 75, duration: 0.179s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.922990, mae: 4.677729, mean_q: 8.959985, mean_eps: 0.500000\n",
            " 1273/8000: episode: 76, duration: 0.343s, episode steps:  20, steps per second:  58, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.941270, mae: 4.700827, mean_q: 9.002008, mean_eps: 0.500000\n",
            " 1289/8000: episode: 77, duration: 0.284s, episode steps:  16, steps per second:  56, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 1.571856, mae: 4.825664, mean_q: 9.093300, mean_eps: 0.500000\n",
            " 1299/8000: episode: 78, duration: 0.180s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.926303, mae: 4.804919, mean_q: 9.148953, mean_eps: 0.500000\n",
            " 1312/8000: episode: 79, duration: 0.233s, episode steps:  13, steps per second:  56, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.077 [0.000, 1.000],  loss: 0.770252, mae: 4.857671, mean_q: 9.339816, mean_eps: 0.500000\n",
            " 1325/8000: episode: 80, duration: 0.250s, episode steps:  13, steps per second:  52, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 1.574843, mae: 4.941744, mean_q: 9.400877, mean_eps: 0.500000\n",
            " 1340/8000: episode: 81, duration: 0.346s, episode steps:  15, steps per second:  43, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.592946, mae: 4.957598, mean_q: 9.427052, mean_eps: 0.500000\n",
            " 1355/8000: episode: 82, duration: 0.293s, episode steps:  15, steps per second:  51, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.598467, mae: 5.008924, mean_q: 9.401518, mean_eps: 0.500000\n",
            " 1372/8000: episode: 83, duration: 0.293s, episode steps:  17, steps per second:  58, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: 1.927239, mae: 5.200994, mean_q: 9.709312, mean_eps: 0.500000\n",
            " 1384/8000: episode: 84, duration: 0.207s, episode steps:  12, steps per second:  58, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 1.825806, mae: 5.165122, mean_q: 9.679209, mean_eps: 0.500000\n",
            " 1393/8000: episode: 85, duration: 0.165s, episode steps:   9, steps per second:  55, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 1.739576, mae: 5.240343, mean_q: 9.807603, mean_eps: 0.500000\n",
            " 1402/8000: episode: 86, duration: 0.157s, episode steps:   9, steps per second:  57, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 1.143214, mae: 5.107830, mean_q: 9.700653, mean_eps: 0.500000\n",
            " 1418/8000: episode: 87, duration: 0.294s, episode steps:  16, steps per second:  54, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 1.678268, mae: 5.249999, mean_q: 9.972322, mean_eps: 0.500000\n",
            " 1431/8000: episode: 88, duration: 0.218s, episode steps:  13, steps per second:  60, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.154 [0.000, 1.000],  loss: 2.069006, mae: 5.350274, mean_q: 10.065790, mean_eps: 0.500000\n",
            " 1445/8000: episode: 89, duration: 0.235s, episode steps:  14, steps per second:  60, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.214 [0.000, 1.000],  loss: 2.211450, mae: 5.474793, mean_q: 10.206934, mean_eps: 0.500000\n",
            " 1458/8000: episode: 90, duration: 0.241s, episode steps:  13, steps per second:  54, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 1.499169, mae: 5.348695, mean_q: 10.019720, mean_eps: 0.500000\n",
            " 1478/8000: episode: 91, duration: 0.343s, episode steps:  20, steps per second:  58, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 2.128282, mae: 5.538665, mean_q: 10.289841, mean_eps: 0.500000\n",
            " 1488/8000: episode: 92, duration: 0.181s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 2.353166, mae: 5.606642, mean_q: 10.430822, mean_eps: 0.500000\n",
            " 1498/8000: episode: 93, duration: 0.187s, episode steps:  10, steps per second:  53, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 1.617031, mae: 5.589461, mean_q: 10.563751, mean_eps: 0.500000\n",
            " 1510/8000: episode: 94, duration: 0.243s, episode steps:  12, steps per second:  49, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 2.600698, mae: 5.696460, mean_q: 10.554704, mean_eps: 0.500000\n",
            " 1521/8000: episode: 95, duration: 0.216s, episode steps:  11, steps per second:  51, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 1.911027, mae: 5.628458, mean_q: 10.578519, mean_eps: 0.500000\n",
            " 1531/8000: episode: 96, duration: 0.246s, episode steps:  10, steps per second:  41, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 2.476109, mae: 5.710328, mean_q: 10.593138, mean_eps: 0.500000\n",
            " 1554/8000: episode: 97, duration: 0.563s, episode steps:  23, steps per second:  41, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  loss: 1.779105, mae: 5.670127, mean_q: 10.636423, mean_eps: 0.500000\n",
            " 1564/8000: episode: 98, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 3.086750, mae: 5.835960, mean_q: 10.727356, mean_eps: 0.500000\n",
            " 1573/8000: episode: 99, duration: 0.159s, episode steps:   9, steps per second:  57, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 1.832346, mae: 5.794842, mean_q: 10.769388, mean_eps: 0.500000\n",
            " 1587/8000: episode: 100, duration: 0.239s, episode steps:  14, steps per second:  58, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 2.032584, mae: 5.840824, mean_q: 10.908167, mean_eps: 0.500000\n",
            " 1597/8000: episode: 101, duration: 0.180s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.819514, mae: 5.956083, mean_q: 11.024131, mean_eps: 0.500000\n",
            " 1610/8000: episode: 102, duration: 0.249s, episode steps:  13, steps per second:  52, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 1.920078, mae: 5.889913, mean_q: 11.050026, mean_eps: 0.500000\n",
            " 1622/8000: episode: 103, duration: 0.196s, episode steps:  12, steps per second:  61, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 2.249514, mae: 5.918288, mean_q: 11.067148, mean_eps: 0.500000\n",
            " 1636/8000: episode: 104, duration: 0.227s, episode steps:  14, steps per second:  62, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.214 [0.000, 1.000],  loss: 2.208087, mae: 5.923366, mean_q: 11.031506, mean_eps: 0.500000\n",
            " 1650/8000: episode: 105, duration: 0.242s, episode steps:  14, steps per second:  58, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 2.555144, mae: 5.991703, mean_q: 11.022426, mean_eps: 0.500000\n",
            " 1666/8000: episode: 106, duration: 0.300s, episode steps:  16, steps per second:  53, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 2.089406, mae: 5.980364, mean_q: 11.109724, mean_eps: 0.500000\n",
            " 1675/8000: episode: 107, duration: 0.156s, episode steps:   9, steps per second:  58, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.725269, mae: 5.952781, mean_q: 11.221525, mean_eps: 0.500000\n",
            " 1686/8000: episode: 108, duration: 0.209s, episode steps:  11, steps per second:  53, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 1.881721, mae: 6.045033, mean_q: 11.353228, mean_eps: 0.500000\n",
            " 1698/8000: episode: 109, duration: 0.238s, episode steps:  12, steps per second:  50, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 2.601207, mae: 6.133454, mean_q: 11.355640, mean_eps: 0.500000\n",
            " 1708/8000: episode: 110, duration: 0.180s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 2.755062, mae: 6.184483, mean_q: 11.396783, mean_eps: 0.500000\n",
            " 1724/8000: episode: 111, duration: 0.275s, episode steps:  16, steps per second:  58, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.644867, mae: 6.151214, mean_q: 11.283859, mean_eps: 0.500000\n",
            " 1738/8000: episode: 112, duration: 0.266s, episode steps:  14, steps per second:  53, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 1.948911, mae: 6.102857, mean_q: 11.445478, mean_eps: 0.500000\n",
            " 1753/8000: episode: 113, duration: 0.349s, episode steps:  15, steps per second:  43, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 2.202960, mae: 6.218011, mean_q: 11.723324, mean_eps: 0.500000\n",
            " 1764/8000: episode: 114, duration: 0.345s, episode steps:  11, steps per second:  32, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 2.572053, mae: 6.226408, mean_q: 11.538873, mean_eps: 0.500000\n",
            " 1784/8000: episode: 115, duration: 0.519s, episode steps:  20, steps per second:  39, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 2.174060, mae: 6.234511, mean_q: 11.604181, mean_eps: 0.500000\n",
            " 1792/8000: episode: 116, duration: 0.223s, episode steps:   8, steps per second:  36, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.125 [0.000, 1.000],  loss: 2.175722, mae: 6.298234, mean_q: 11.745506, mean_eps: 0.500000\n",
            " 1849/8000: episode: 117, duration: 1.085s, episode steps:  57, steps per second:  53, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  loss: 2.362703, mae: 6.345194, mean_q: 11.811585, mean_eps: 0.500000\n",
            " 1900/8000: episode: 118, duration: 0.845s, episode steps:  51, steps per second:  60, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1.961852, mae: 6.362075, mean_q: 11.923226, mean_eps: 0.500000\n",
            " 1966/8000: episode: 119, duration: 1.429s, episode steps:  66, steps per second:  46, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 2.044611, mae: 6.481771, mean_q: 12.148253, mean_eps: 0.500000\n",
            " 2034/8000: episode: 120, duration: 1.412s, episode steps:  68, steps per second:  48, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.346028, mae: 6.558966, mean_q: 12.163903, mean_eps: 0.500000\n",
            " 2075/8000: episode: 121, duration: 1.072s, episode steps:  41, steps per second:  38, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 2.025537, mae: 6.666428, mean_q: 12.494367, mean_eps: 0.500000\n",
            " 2091/8000: episode: 122, duration: 0.464s, episode steps:  16, steps per second:  34, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.181658, mae: 6.698790, mean_q: 12.574942, mean_eps: 0.500000\n",
            " 2119/8000: episode: 123, duration: 0.697s, episode steps:  28, steps per second:  40, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 1.911387, mae: 6.712255, mean_q: 12.681341, mean_eps: 0.500000\n",
            " 2132/8000: episode: 124, duration: 0.317s, episode steps:  13, steps per second:  41, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 2.002049, mae: 6.856495, mean_q: 12.961810, mean_eps: 0.500000\n",
            " 2208/8000: episode: 125, duration: 1.952s, episode steps:  76, steps per second:  39, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.256006, mae: 6.879898, mean_q: 12.961320, mean_eps: 0.500000\n",
            " 2219/8000: episode: 126, duration: 0.317s, episode steps:  11, steps per second:  35, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 2.296767, mae: 6.968461, mean_q: 13.028239, mean_eps: 0.500000\n",
            " 2279/8000: episode: 127, duration: 1.217s, episode steps:  60, steps per second:  49, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 2.170804, mae: 6.969200, mean_q: 13.134610, mean_eps: 0.500000\n",
            " 2335/8000: episode: 128, duration: 1.018s, episode steps:  56, steps per second:  55, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 2.000718, mae: 7.111796, mean_q: 13.472143, mean_eps: 0.500000\n",
            " 2421/8000: episode: 129, duration: 2.139s, episode steps:  86, steps per second:  40, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 2.452505, mae: 7.259701, mean_q: 13.712906, mean_eps: 0.500000\n",
            " 2460/8000: episode: 130, duration: 0.910s, episode steps:  39, steps per second:  43, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 2.282413, mae: 7.405622, mean_q: 14.023166, mean_eps: 0.500000\n",
            " 2493/8000: episode: 131, duration: 0.924s, episode steps:  33, steps per second:  36, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 2.676662, mae: 7.518005, mean_q: 14.236176, mean_eps: 0.500000\n",
            " 2527/8000: episode: 132, duration: 0.990s, episode steps:  34, steps per second:  34, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.441 [0.000, 1.000],  loss: 2.053998, mae: 7.603206, mean_q: 14.515335, mean_eps: 0.500000\n",
            " 2571/8000: episode: 133, duration: 1.019s, episode steps:  44, steps per second:  43, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.683351, mae: 7.678466, mean_q: 14.552820, mean_eps: 0.500000\n",
            " 2605/8000: episode: 134, duration: 0.627s, episode steps:  34, steps per second:  54, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.846856, mae: 7.724113, mean_q: 14.560308, mean_eps: 0.500000\n",
            " 2631/8000: episode: 135, duration: 0.471s, episode steps:  26, steps per second:  55, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.423 [0.000, 1.000],  loss: 2.384917, mae: 7.770549, mean_q: 14.781684, mean_eps: 0.500000\n",
            " 2648/8000: episode: 136, duration: 0.312s, episode steps:  17, steps per second:  54, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 2.782238, mae: 7.933613, mean_q: 15.080064, mean_eps: 0.500000\n",
            " 2685/8000: episode: 137, duration: 0.652s, episode steps:  37, steps per second:  57, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 2.951794, mae: 7.950139, mean_q: 15.006427, mean_eps: 0.500000\n",
            " 2715/8000: episode: 138, duration: 0.828s, episode steps:  30, steps per second:  36, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 2.691552, mae: 8.041406, mean_q: 15.263433, mean_eps: 0.500000\n",
            " 2797/8000: episode: 139, duration: 2.010s, episode steps:  82, steps per second:  41, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 2.824920, mae: 8.169106, mean_q: 15.520222, mean_eps: 0.500000\n",
            " 2890/8000: episode: 140, duration: 2.084s, episode steps:  93, steps per second:  45, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 2.798262, mae: 8.336732, mean_q: 15.868970, mean_eps: 0.500000\n",
            " 2944/8000: episode: 141, duration: 1.104s, episode steps:  54, steps per second:  49, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 2.592083, mae: 8.516303, mean_q: 16.279595, mean_eps: 0.500000\n",
            " 2984/8000: episode: 142, duration: 0.754s, episode steps:  40, steps per second:  53, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 2.208169, mae: 8.573320, mean_q: 16.478823, mean_eps: 0.500000\n",
            " 3013/8000: episode: 143, duration: 0.648s, episode steps:  29, steps per second:  45, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 2.577987, mae: 8.730939, mean_q: 16.778230, mean_eps: 0.500000\n",
            " 3038/8000: episode: 144, duration: 0.639s, episode steps:  25, steps per second:  39, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 3.065035, mae: 8.839660, mean_q: 16.896415, mean_eps: 0.500000\n",
            " 3106/8000: episode: 145, duration: 1.496s, episode steps:  68, steps per second:  45, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.516501, mae: 8.913856, mean_q: 16.924450, mean_eps: 0.500000\n",
            " 3165/8000: episode: 146, duration: 1.228s, episode steps:  59, steps per second:  48, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 3.796111, mae: 8.997235, mean_q: 17.035211, mean_eps: 0.500000\n",
            " 3220/8000: episode: 147, duration: 1.170s, episode steps:  55, steps per second:  47, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 2.708674, mae: 9.105720, mean_q: 17.409632, mean_eps: 0.500000\n",
            " 3256/8000: episode: 148, duration: 0.739s, episode steps:  36, steps per second:  49, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.522004, mae: 9.209957, mean_q: 17.693261, mean_eps: 0.500000\n",
            " 3345/8000: episode: 149, duration: 1.500s, episode steps:  89, steps per second:  59, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 3.376859, mae: 9.393456, mean_q: 17.971053, mean_eps: 0.500000\n",
            " 3399/8000: episode: 150, duration: 1.123s, episode steps:  54, steps per second:  48, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 4.341934, mae: 9.524654, mean_q: 18.091813, mean_eps: 0.500000\n",
            " 3467/8000: episode: 151, duration: 1.562s, episode steps:  68, steps per second:  44, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 4.058785, mae: 9.671130, mean_q: 18.483710, mean_eps: 0.500000\n",
            " 3503/8000: episode: 152, duration: 0.913s, episode steps:  36, steps per second:  39, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 3.488774, mae: 9.736574, mean_q: 18.658672, mean_eps: 0.500000\n",
            " 3586/8000: episode: 153, duration: 2.132s, episode steps:  83, steps per second:  39, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 4.242036, mae: 9.920220, mean_q: 18.922239, mean_eps: 0.500000\n",
            " 3664/8000: episode: 154, duration: 1.804s, episode steps:  78, steps per second:  43, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 3.895083, mae: 9.970137, mean_q: 19.021201, mean_eps: 0.500000\n",
            " 3708/8000: episode: 155, duration: 0.712s, episode steps:  44, steps per second:  62, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 3.749578, mae: 10.093897, mean_q: 19.397569, mean_eps: 0.500000\n",
            " 3736/8000: episode: 156, duration: 0.608s, episode steps:  28, steps per second:  46, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 3.045288, mae: 10.136978, mean_q: 19.519624, mean_eps: 0.500000\n",
            " 3850/8000: episode: 157, duration: 2.587s, episode steps: 114, steps per second:  44, episode reward: 114.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 3.504862, mae: 10.316256, mean_q: 19.827302, mean_eps: 0.500000\n",
            " 3895/8000: episode: 158, duration: 0.715s, episode steps:  45, steps per second:  63, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 3.558337, mae: 10.479754, mean_q: 20.181440, mean_eps: 0.500000\n",
            " 3911/8000: episode: 159, duration: 0.273s, episode steps:  16, steps per second:  59, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 2.695445, mae: 10.565499, mean_q: 20.417564, mean_eps: 0.500000\n",
            " 3970/8000: episode: 160, duration: 1.317s, episode steps:  59, steps per second:  45, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 4.985766, mae: 10.729004, mean_q: 20.500262, mean_eps: 0.500000\n",
            " 3986/8000: episode: 161, duration: 0.449s, episode steps:  16, steps per second:  36, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 4.108739, mae: 10.684121, mean_q: 20.434199, mean_eps: 0.500000\n",
            " 4121/8000: episode: 162, duration: 3.167s, episode steps: 135, steps per second:  43, episode reward: 135.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 4.403561, mae: 10.848028, mean_q: 20.794851, mean_eps: 0.500000\n",
            " 4180/8000: episode: 163, duration: 1.307s, episode steps:  59, steps per second:  45, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 3.827898, mae: 11.050985, mean_q: 21.291582, mean_eps: 0.500000\n",
            " 4223/8000: episode: 164, duration: 0.946s, episode steps:  43, steps per second:  45, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 4.112738, mae: 11.110963, mean_q: 21.413417, mean_eps: 0.500000\n",
            " 4269/8000: episode: 165, duration: 1.150s, episode steps:  46, steps per second:  40, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 5.414744, mae: 11.207246, mean_q: 21.410446, mean_eps: 0.500000\n",
            " 4332/8000: episode: 166, duration: 1.663s, episode steps:  63, steps per second:  38, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 5.440854, mae: 11.365007, mean_q: 21.686162, mean_eps: 0.500000\n",
            " 4395/8000: episode: 167, duration: 1.612s, episode steps:  63, steps per second:  39, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 4.128203, mae: 11.398260, mean_q: 21.919577, mean_eps: 0.500000\n",
            " 4406/8000: episode: 168, duration: 0.320s, episode steps:  11, steps per second:  34, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 2.259567, mae: 11.518988, mean_q: 22.401503, mean_eps: 0.500000\n",
            " 4459/8000: episode: 169, duration: 1.005s, episode steps:  53, steps per second:  53, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 5.114166, mae: 11.513941, mean_q: 22.083715, mean_eps: 0.500000\n",
            " 4500/8000: episode: 170, duration: 0.664s, episode steps:  41, steps per second:  62, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 4.782540, mae: 11.541234, mean_q: 22.190690, mean_eps: 0.500000\n",
            " 4578/8000: episode: 171, duration: 1.535s, episode steps:  78, steps per second:  51, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 4.210996, mae: 11.740818, mean_q: 22.658350, mean_eps: 0.500000\n",
            " 4631/8000: episode: 172, duration: 0.884s, episode steps:  53, steps per second:  60, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 5.133183, mae: 11.842580, mean_q: 22.822630, mean_eps: 0.500000\n",
            " 4687/8000: episode: 173, duration: 0.881s, episode steps:  56, steps per second:  64, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 4.099338, mae: 11.916512, mean_q: 23.071187, mean_eps: 0.500000\n",
            " 4738/8000: episode: 174, duration: 0.772s, episode steps:  51, steps per second:  66, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 5.641222, mae: 12.068550, mean_q: 23.207229, mean_eps: 0.500000\n",
            " 4775/8000: episode: 175, duration: 0.552s, episode steps:  37, steps per second:  67, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 4.058759, mae: 12.066666, mean_q: 23.419794, mean_eps: 0.500000\n",
            " 4803/8000: episode: 176, duration: 0.434s, episode steps:  28, steps per second:  64, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.333365, mae: 12.321111, mean_q: 23.775745, mean_eps: 0.500000\n",
            " 4869/8000: episode: 177, duration: 1.089s, episode steps:  66, steps per second:  61, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 5.060702, mae: 12.332177, mean_q: 23.790949, mean_eps: 0.500000\n",
            " 4982/8000: episode: 178, duration: 1.850s, episode steps: 113, steps per second:  61, episode reward: 113.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 5.866021, mae: 12.473048, mean_q: 23.996052, mean_eps: 0.500000\n",
            " 5069/8000: episode: 179, duration: 1.798s, episode steps:  87, steps per second:  48, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 5.431024, mae: 12.588710, mean_q: 24.254857, mean_eps: 0.500000\n",
            " 5107/8000: episode: 180, duration: 0.733s, episode steps:  38, steps per second:  52, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  loss: 8.278849, mae: 12.853671, mean_q: 24.558126, mean_eps: 0.500000\n",
            " 5152/8000: episode: 181, duration: 0.757s, episode steps:  45, steps per second:  59, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 4.818813, mae: 12.666923, mean_q: 24.410744, mean_eps: 0.500000\n",
            " 5199/8000: episode: 182, duration: 0.869s, episode steps:  47, steps per second:  54, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 5.508279, mae: 12.850166, mean_q: 24.759455, mean_eps: 0.500000\n",
            " 5240/8000: episode: 183, duration: 1.097s, episode steps:  41, steps per second:  37, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 5.763591, mae: 12.900553, mean_q: 24.797430, mean_eps: 0.500000\n",
            " 5325/8000: episode: 184, duration: 2.002s, episode steps:  85, steps per second:  42, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 5.073868, mae: 12.986378, mean_q: 25.102010, mean_eps: 0.500000\n",
            " 5371/8000: episode: 185, duration: 0.733s, episode steps:  46, steps per second:  63, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 4.705087, mae: 13.177853, mean_q: 25.556928, mean_eps: 0.500000\n",
            " 5428/8000: episode: 186, duration: 0.968s, episode steps:  57, steps per second:  59, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 5.593633, mae: 13.225833, mean_q: 25.516531, mean_eps: 0.500000\n",
            " 5462/8000: episode: 187, duration: 0.698s, episode steps:  34, steps per second:  49, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 4.633567, mae: 13.281221, mean_q: 25.741168, mean_eps: 0.500000\n",
            " 5506/8000: episode: 188, duration: 1.212s, episode steps:  44, steps per second:  36, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 4.652470, mae: 13.382035, mean_q: 25.963933, mean_eps: 0.500000\n",
            " 5580/8000: episode: 189, duration: 1.162s, episode steps:  74, steps per second:  64, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.457391, mae: 13.503754, mean_q: 26.019929, mean_eps: 0.500000\n",
            " 5632/8000: episode: 190, duration: 0.892s, episode steps:  52, steps per second:  58, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 5.393500, mae: 13.545734, mean_q: 26.176294, mean_eps: 0.500000\n",
            " 5665/8000: episode: 191, duration: 0.532s, episode steps:  33, steps per second:  62, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 6.499130, mae: 13.604695, mean_q: 26.321442, mean_eps: 0.500000\n",
            " 5716/8000: episode: 192, duration: 0.825s, episode steps:  51, steps per second:  62, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 5.543754, mae: 13.676029, mean_q: 26.493587, mean_eps: 0.500000\n",
            " 5778/8000: episode: 193, duration: 1.006s, episode steps:  62, steps per second:  62, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.137636, mae: 13.829177, mean_q: 26.790717, mean_eps: 0.500000\n",
            " 5812/8000: episode: 194, duration: 0.531s, episode steps:  34, steps per second:  64, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 5.131588, mae: 13.795932, mean_q: 26.802248, mean_eps: 0.500000\n",
            " 5852/8000: episode: 195, duration: 0.662s, episode steps:  40, steps per second:  60, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 5.723191, mae: 13.959260, mean_q: 27.075679, mean_eps: 0.500000\n",
            " 5916/8000: episode: 196, duration: 1.100s, episode steps:  64, steps per second:  58, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 5.824318, mae: 13.990534, mean_q: 27.128225, mean_eps: 0.500000\n",
            " 5976/8000: episode: 197, duration: 1.004s, episode steps:  60, steps per second:  60, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 5.563795, mae: 14.121875, mean_q: 27.448011, mean_eps: 0.500000\n",
            " 6028/8000: episode: 198, duration: 0.919s, episode steps:  52, steps per second:  57, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 5.560464, mae: 14.213005, mean_q: 27.559525, mean_eps: 0.500000\n",
            " 6069/8000: episode: 199, duration: 0.713s, episode steps:  41, steps per second:  57, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.439 [0.000, 1.000],  loss: 6.641130, mae: 14.301045, mean_q: 27.696145, mean_eps: 0.500000\n",
            " 6115/8000: episode: 200, duration: 1.011s, episode steps:  46, steps per second:  45, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 6.784034, mae: 14.315601, mean_q: 27.666212, mean_eps: 0.500000\n",
            " 6155/8000: episode: 201, duration: 0.691s, episode steps:  40, steps per second:  58, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 6.359421, mae: 14.322693, mean_q: 27.797218, mean_eps: 0.500000\n",
            " 6186/8000: episode: 202, duration: 0.500s, episode steps:  31, steps per second:  62, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 9.016443, mae: 14.546525, mean_q: 28.027396, mean_eps: 0.500000\n",
            " 6294/8000: episode: 203, duration: 1.701s, episode steps: 108, steps per second:  63, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 7.168286, mae: 14.485572, mean_q: 28.001554, mean_eps: 0.500000\n",
            " 6333/8000: episode: 204, duration: 0.617s, episode steps:  39, steps per second:  63, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 5.731155, mae: 14.585536, mean_q: 28.398745, mean_eps: 0.500000\n",
            " 6398/8000: episode: 205, duration: 1.036s, episode steps:  65, steps per second:  63, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 4.215032, mae: 14.687931, mean_q: 28.682456, mean_eps: 0.500000\n",
            " 6488/8000: episode: 206, duration: 1.494s, episode steps:  90, steps per second:  60, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.514388, mae: 14.843874, mean_q: 29.006386, mean_eps: 0.500000\n",
            " 6538/8000: episode: 207, duration: 0.923s, episode steps:  50, steps per second:  54, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 6.907000, mae: 15.046358, mean_q: 29.227191, mean_eps: 0.500000\n",
            " 6607/8000: episode: 208, duration: 1.126s, episode steps:  69, steps per second:  61, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 5.450123, mae: 15.004341, mean_q: 29.310336, mean_eps: 0.500000\n",
            " 6672/8000: episode: 209, duration: 1.430s, episode steps:  65, steps per second:  45, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 6.050796, mae: 15.149927, mean_q: 29.502984, mean_eps: 0.500000\n",
            " 6802/8000: episode: 210, duration: 2.141s, episode steps: 130, steps per second:  61, episode reward: 130.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 7.873870, mae: 15.299865, mean_q: 29.621755, mean_eps: 0.500000\n",
            " 6879/8000: episode: 211, duration: 1.204s, episode steps:  77, steps per second:  64, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 7.177671, mae: 15.416874, mean_q: 29.985092, mean_eps: 0.500000\n",
            " 6928/8000: episode: 212, duration: 0.765s, episode steps:  49, steps per second:  64, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 6.975968, mae: 15.409158, mean_q: 29.987979, mean_eps: 0.500000\n",
            " 7050/8000: episode: 213, duration: 1.833s, episode steps: 122, steps per second:  67, episode reward: 122.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 6.621434, mae: 15.558170, mean_q: 30.310589, mean_eps: 0.500000\n",
            " 7162/8000: episode: 214, duration: 1.740s, episode steps: 112, steps per second:  64, episode reward: 112.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 6.719739, mae: 15.757777, mean_q: 30.669463, mean_eps: 0.500000\n",
            " 7247/8000: episode: 215, duration: 1.338s, episode steps:  85, steps per second:  64, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 6.971674, mae: 15.950098, mean_q: 31.052896, mean_eps: 0.500000\n",
            " 7417/8000: episode: 216, duration: 2.658s, episode steps: 170, steps per second:  64, episode reward: 170.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 7.231731, mae: 15.989748, mean_q: 31.191155, mean_eps: 0.500000\n",
            " 7446/8000: episode: 217, duration: 0.477s, episode steps:  29, steps per second:  61, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 6.746682, mae: 16.137505, mean_q: 31.441702, mean_eps: 0.500000\n",
            " 7545/8000: episode: 218, duration: 2.021s, episode steps:  99, steps per second:  49, episode reward: 99.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 6.936605, mae: 16.260236, mean_q: 31.812068, mean_eps: 0.500000\n",
            " 7593/8000: episode: 219, duration: 0.870s, episode steps:  48, steps per second:  55, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 6.508208, mae: 16.267368, mean_q: 31.956663, mean_eps: 0.500000\n",
            " 7686/8000: episode: 220, duration: 1.949s, episode steps:  93, steps per second:  48, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 7.861648, mae: 16.488592, mean_q: 32.202957, mean_eps: 0.500000\n",
            " 7847/8000: episode: 221, duration: 2.943s, episode steps: 161, steps per second:  55, episode reward: 161.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 7.573500, mae: 16.645509, mean_q: 32.545326, mean_eps: 0.500000\n",
            " 7927/8000: episode: 222, duration: 1.662s, episode steps:  80, steps per second:  48, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 7.315059, mae: 16.744766, mean_q: 32.762927, mean_eps: 0.500000\n",
            "done, took 175.719 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 147.000, steps: 147\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 124.000, steps: 124\n",
            "Episode 6: reward: 200.000, steps: 200\n",
            "Episode 7: reward: 200.000, steps: 200\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 142.000, steps: 142\n",
            "Episode 10: reward: 103.000, steps: 103\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 105.000, steps: 105\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 200.000, steps: 200\n",
            "Episode 15: reward: 154.000, steps: 154\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 200.000, steps: 200\n",
            "Episode 18: reward: 131.000, steps: 131\n",
            "Episode 19: reward: 101.000, steps: 101\n",
            "Episode 20: reward: 200.000, steps: 200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb1701184d0>"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    }
  ]
}