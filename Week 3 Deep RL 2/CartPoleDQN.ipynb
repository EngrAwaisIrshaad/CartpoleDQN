{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPoleDQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Megacity1/CartpoleDQN/blob/main/Week%203%20Deep%20RL%202/CartPoleDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Deep Q Network (DQN) for CartPole Using Boltzmann Q Policy\n",
        "This exercise implements a DQN for CartPole using a Boltzmann Q policy for selecting the actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGsC7cJ5jNcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda73cbf-3020-40f4-e45e-7b87053448f1"
      },
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.24.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (13.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.44.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.10.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMIHLgQ3Z-lF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5a8608-456f-4127-ce8a-f551e5f4e4f3"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0AMLzq08ap0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b380e1-6c3d-4bce-8233-a8ac01deaf26"
      },
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "!pip install Adam\n",
        "#from keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Adam in /usr/local/lib/python3.7/dist-packages (0.0.0.dev0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll6bNdUm54WS"
      },
      "source": [
        "Implementation of DQN for CartPole, applying policy BoltzmannQPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCrPKNy40PC"
      },
      "source": [
        "##Implement DQN with BoltzmannGumbelQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efM9jkXr5A3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "30eb94a9-b984-4589-e3c5-1f05548cd5cb"
      },
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import BoltzmannGumbelQPolicy\n",
        "from rl.policy import LinearAnnealedPolicy\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n",
        "# setup experience replay buffer\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
        "                               attr='eps',            \n",
        "                               value_max=10.,\n",
        "                               value_min=.10, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=20)\n",
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "# add extra layers here\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=20,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy) \n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_41\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_39 (Flatten)        (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_79 (Dense)            (None, 16)                80        \n",
            "                                                                 \n",
            " dense_80 (Dense)            (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 10000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   26/10000: episode: 1, duration: 12.332s, episode steps:  26, steps per second:   2, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.423 [0.000, 1.000],  loss: 0.523824, mae: 0.531833, mean_q: 0.072765, mean_eps: 0.100000\n",
            "   36/10000: episode: 2, duration: 0.168s, episode steps:  10, steps per second:  60, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.471592, mae: 0.524708, mean_q: 0.149646, mean_eps: 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   46/10000: episode: 3, duration: 0.173s, episode steps:  10, steps per second:  58, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.389267, mae: 0.522164, mean_q: 0.305601, mean_eps: 0.100000\n",
            "   57/10000: episode: 4, duration: 0.185s, episode steps:  11, steps per second:  59, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.306291, mae: 0.507297, mean_q: 0.484589, mean_eps: 0.100000\n",
            "   67/10000: episode: 5, duration: 0.186s, episode steps:  10, steps per second:  54, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.257241, mae: 0.515897, mean_q: 0.653094, mean_eps: 0.100000\n",
            "   77/10000: episode: 6, duration: 0.175s, episode steps:  10, steps per second:  57, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.246495, mae: 0.538059, mean_q: 0.748234, mean_eps: 0.100000\n",
            "   87/10000: episode: 7, duration: 0.203s, episode steps:  10, steps per second:  49, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.207166, mae: 0.561796, mean_q: 0.987814, mean_eps: 0.100000\n",
            "   98/10000: episode: 8, duration: 0.238s, episode steps:  11, steps per second:  46, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.212944, mae: 0.601737, mean_q: 1.025587, mean_eps: 0.100000\n",
            "  108/10000: episode: 9, duration: 0.264s, episode steps:  10, steps per second:  38, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.235673, mae: 0.682022, mean_q: 1.149818, mean_eps: 0.100000\n",
            "  118/10000: episode: 10, duration: 0.275s, episode steps:  10, steps per second:  36, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.216592, mae: 0.690235, mean_q: 1.192847, mean_eps: 0.100000\n",
            "  127/10000: episode: 11, duration: 0.270s, episode steps:   9, steps per second:  33, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.206125, mae: 0.744052, mean_q: 1.354901, mean_eps: 0.100000\n",
            "  136/10000: episode: 12, duration: 0.212s, episode steps:   9, steps per second:  42, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.222841, mae: 0.780207, mean_q: 1.412106, mean_eps: 0.100000\n",
            "  147/10000: episode: 13, duration: 0.233s, episode steps:  11, steps per second:  47, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.231746, mae: 0.805489, mean_q: 1.495647, mean_eps: 0.100000\n",
            "  156/10000: episode: 14, duration: 0.144s, episode steps:   9, steps per second:  62, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.264762, mae: 0.873489, mean_q: 1.602803, mean_eps: 0.100000\n",
            "  166/10000: episode: 15, duration: 0.147s, episode steps:  10, steps per second:  68, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.281134, mae: 0.905324, mean_q: 1.603886, mean_eps: 0.100000\n",
            "  175/10000: episode: 16, duration: 0.146s, episode steps:   9, steps per second:  62, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.232078, mae: 0.935237, mean_q: 1.720885, mean_eps: 0.100000\n",
            "  185/10000: episode: 17, duration: 0.155s, episode steps:  10, steps per second:  64, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.227166, mae: 0.935741, mean_q: 1.819697, mean_eps: 0.100000\n",
            "  194/10000: episode: 18, duration: 0.134s, episode steps:   9, steps per second:  67, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.266638, mae: 0.970224, mean_q: 1.873945, mean_eps: 0.100000\n",
            "  203/10000: episode: 19, duration: 0.162s, episode steps:   9, steps per second:  56, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.256244, mae: 1.002549, mean_q: 1.932315, mean_eps: 0.100000\n",
            "  215/10000: episode: 20, duration: 0.228s, episode steps:  12, steps per second:  53, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.083 [0.000, 1.000],  loss: 0.253963, mae: 1.059878, mean_q: 2.028627, mean_eps: 0.100000\n",
            "  226/10000: episode: 21, duration: 0.199s, episode steps:  11, steps per second:  55, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.263572, mae: 1.074685, mean_q: 2.058398, mean_eps: 0.100000\n",
            "  235/10000: episode: 22, duration: 0.168s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.268284, mae: 1.112995, mean_q: 2.134464, mean_eps: 0.100000\n",
            "  244/10000: episode: 23, duration: 0.172s, episode steps:   9, steps per second:  52, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.236195, mae: 1.093880, mean_q: 2.156577, mean_eps: 0.100000\n",
            "  255/10000: episode: 24, duration: 0.208s, episode steps:  11, steps per second:  53, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.321639, mae: 1.196168, mean_q: 2.317676, mean_eps: 0.100000\n",
            "  263/10000: episode: 25, duration: 0.130s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.318748, mae: 1.214325, mean_q: 2.394158, mean_eps: 0.100000\n",
            "  272/10000: episode: 26, duration: 0.178s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.306334, mae: 1.238734, mean_q: 2.414979, mean_eps: 0.100000\n",
            "  282/10000: episode: 27, duration: 0.160s, episode steps:  10, steps per second:  62, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.328387, mae: 1.258131, mean_q: 2.427165, mean_eps: 0.100000\n",
            "  292/10000: episode: 28, duration: 0.156s, episode steps:  10, steps per second:  64, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.254231, mae: 1.264984, mean_q: 2.514832, mean_eps: 0.100000\n",
            "  301/10000: episode: 29, duration: 0.144s, episode steps:   9, steps per second:  63, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.298830, mae: 1.300649, mean_q: 2.635556, mean_eps: 0.100000\n",
            "  311/10000: episode: 30, duration: 0.162s, episode steps:  10, steps per second:  62, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.335362, mae: 1.367956, mean_q: 2.719350, mean_eps: 0.100000\n",
            "  323/10000: episode: 31, duration: 0.223s, episode steps:  12, steps per second:  54, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.253421, mae: 1.334535, mean_q: 2.721974, mean_eps: 0.100000\n",
            "  334/10000: episode: 32, duration: 0.296s, episode steps:  11, steps per second:  37, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.345642, mae: 1.404928, mean_q: 2.801263, mean_eps: 0.100000\n",
            "  343/10000: episode: 33, duration: 0.229s, episode steps:   9, steps per second:  39, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.289483, mae: 1.406895, mean_q: 2.797124, mean_eps: 0.100000\n",
            "  353/10000: episode: 34, duration: 0.268s, episode steps:  10, steps per second:  37, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.232878, mae: 1.401817, mean_q: 2.916510, mean_eps: 0.100000\n",
            "  363/10000: episode: 35, duration: 0.202s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.281649, mae: 1.458294, mean_q: 3.074957, mean_eps: 0.100000\n",
            "  373/10000: episode: 36, duration: 0.167s, episode steps:  10, steps per second:  60, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.282424, mae: 1.441758, mean_q: 3.095192, mean_eps: 0.100000\n",
            "  381/10000: episode: 37, duration: 0.136s, episode steps:   8, steps per second:  59, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.243484, mae: 1.428062, mean_q: 3.136475, mean_eps: 0.100000\n",
            "  390/10000: episode: 38, duration: 0.157s, episode steps:   9, steps per second:  57, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.238239, mae: 1.423995, mean_q: 3.213929, mean_eps: 0.100000\n",
            "  398/10000: episode: 39, duration: 0.149s, episode steps:   8, steps per second:  54, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.297915, mae: 1.461810, mean_q: 3.272945, mean_eps: 0.100000\n",
            "  408/10000: episode: 40, duration: 0.183s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.310326, mae: 1.501628, mean_q: 3.291864, mean_eps: 0.100000\n",
            "  418/10000: episode: 41, duration: 0.176s, episode steps:  10, steps per second:  57, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.262095, mae: 1.502527, mean_q: 3.328827, mean_eps: 0.100000\n",
            "  427/10000: episode: 42, duration: 0.177s, episode steps:   9, steps per second:  51, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.269031, mae: 1.510471, mean_q: 3.460366, mean_eps: 0.100000\n",
            "  437/10000: episode: 43, duration: 0.201s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.306750, mae: 1.548045, mean_q: 3.455146, mean_eps: 0.100000\n",
            "  449/10000: episode: 44, duration: 0.201s, episode steps:  12, steps per second:  60, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.083 [0.000, 1.000],  loss: 0.294596, mae: 1.543891, mean_q: 3.441190, mean_eps: 0.100000\n",
            "  461/10000: episode: 45, duration: 0.223s, episode steps:  12, steps per second:  54, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.249026, mae: 1.556741, mean_q: 3.573531, mean_eps: 0.100000\n",
            "  473/10000: episode: 46, duration: 0.249s, episode steps:  12, steps per second:  48, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.309739, mae: 1.627951, mean_q: 3.645065, mean_eps: 0.100000\n",
            "  483/10000: episode: 47, duration: 0.299s, episode steps:  10, steps per second:  33, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.256193, mae: 1.596774, mean_q: 3.679837, mean_eps: 0.100000\n",
            "  492/10000: episode: 48, duration: 0.225s, episode steps:   9, steps per second:  40, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.243674, mae: 1.621634, mean_q: 3.713727, mean_eps: 0.100000\n",
            "  501/10000: episode: 49, duration: 0.202s, episode steps:   9, steps per second:  44, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.257205, mae: 1.657442, mean_q: 3.813686, mean_eps: 0.100000\n",
            "  514/10000: episode: 50, duration: 0.318s, episode steps:  13, steps per second:  41, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.309424, mae: 1.730567, mean_q: 3.787510, mean_eps: 0.100000\n",
            "  523/10000: episode: 51, duration: 0.263s, episode steps:   9, steps per second:  34, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.223298, mae: 1.729754, mean_q: 3.879017, mean_eps: 0.100000\n",
            "  532/10000: episode: 52, duration: 0.199s, episode steps:   9, steps per second:  45, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.251872, mae: 1.742241, mean_q: 3.893778, mean_eps: 0.100000\n",
            "  542/10000: episode: 53, duration: 0.248s, episode steps:  10, steps per second:  40, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.246162, mae: 1.779687, mean_q: 4.006840, mean_eps: 0.100000\n",
            "  551/10000: episode: 54, duration: 0.254s, episode steps:   9, steps per second:  35, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.288885, mae: 1.833047, mean_q: 3.987137, mean_eps: 0.100000\n",
            "  564/10000: episode: 55, duration: 0.324s, episode steps:  13, steps per second:  40, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.202386, mae: 1.820264, mean_q: 4.116824, mean_eps: 0.100000\n",
            "  573/10000: episode: 56, duration: 0.228s, episode steps:   9, steps per second:  39, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.284261, mae: 1.908416, mean_q: 4.105013, mean_eps: 0.100000\n",
            "  583/10000: episode: 57, duration: 0.254s, episode steps:  10, steps per second:  39, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.210344, mae: 1.911422, mean_q: 4.202483, mean_eps: 0.100000\n",
            "  595/10000: episode: 58, duration: 0.259s, episode steps:  12, steps per second:  46, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.260064, mae: 1.941942, mean_q: 4.239937, mean_eps: 0.100000\n",
            "  605/10000: episode: 59, duration: 0.177s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.253714, mae: 1.911640, mean_q: 4.219349, mean_eps: 0.100000\n",
            "  614/10000: episode: 60, duration: 0.145s, episode steps:   9, steps per second:  62, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.219981, mae: 1.938957, mean_q: 4.304082, mean_eps: 0.100000\n",
            "  623/10000: episode: 61, duration: 0.145s, episode steps:   9, steps per second:  62, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.236406, mae: 1.933262, mean_q: 4.349026, mean_eps: 0.100000\n",
            "  633/10000: episode: 62, duration: 0.193s, episode steps:  10, steps per second:  52, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.204815, mae: 1.960184, mean_q: 4.473003, mean_eps: 0.100000\n",
            "  644/10000: episode: 63, duration: 0.205s, episode steps:  11, steps per second:  54, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.184300, mae: 1.998488, mean_q: 4.622533, mean_eps: 0.100000\n",
            "  654/10000: episode: 64, duration: 0.216s, episode steps:  10, steps per second:  46, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.200298, mae: 2.024922, mean_q: 4.623612, mean_eps: 0.100000\n",
            "  662/10000: episode: 65, duration: 0.172s, episode steps:   8, steps per second:  46, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.231140, mae: 2.047961, mean_q: 4.486965, mean_eps: 0.100000\n",
            "  672/10000: episode: 66, duration: 0.162s, episode steps:  10, steps per second:  62, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.181781, mae: 2.049734, mean_q: 4.553960, mean_eps: 0.100000\n",
            "  680/10000: episode: 67, duration: 0.131s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.258631, mae: 2.147217, mean_q: 4.684054, mean_eps: 0.100000\n",
            "  689/10000: episode: 68, duration: 0.162s, episode steps:   9, steps per second:  55, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.212983, mae: 2.109488, mean_q: 4.570292, mean_eps: 0.100000\n",
            "  699/10000: episode: 69, duration: 0.168s, episode steps:  10, steps per second:  59, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.183424, mae: 2.110718, mean_q: 4.614347, mean_eps: 0.100000\n",
            "  708/10000: episode: 70, duration: 0.165s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.168116, mae: 2.173633, mean_q: 4.828667, mean_eps: 0.100000\n",
            "  717/10000: episode: 71, duration: 0.195s, episode steps:   9, steps per second:  46, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.219391, mae: 2.200149, mean_q: 4.790483, mean_eps: 0.100000\n",
            "  727/10000: episode: 72, duration: 0.213s, episode steps:  10, steps per second:  47, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.231157, mae: 2.223205, mean_q: 4.760836, mean_eps: 0.100000\n",
            "  736/10000: episode: 73, duration: 0.167s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.181966, mae: 2.239227, mean_q: 4.765668, mean_eps: 0.100000\n",
            "  744/10000: episode: 74, duration: 0.171s, episode steps:   8, steps per second:  47, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.125 [0.000, 1.000],  loss: 0.152978, mae: 2.300235, mean_q: 5.032624, mean_eps: 0.100000\n",
            "  758/10000: episode: 75, duration: 0.272s, episode steps:  14, steps per second:  51, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.206862, mae: 2.303181, mean_q: 4.901151, mean_eps: 0.100000\n",
            "  770/10000: episode: 76, duration: 0.203s, episode steps:  12, steps per second:  59, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.146614, mae: 2.329505, mean_q: 5.005866, mean_eps: 0.100000\n",
            "  780/10000: episode: 77, duration: 0.144s, episode steps:  10, steps per second:  69, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.187216, mae: 2.353612, mean_q: 5.016244, mean_eps: 0.100000\n",
            "  791/10000: episode: 78, duration: 0.162s, episode steps:  11, steps per second:  68, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.192281, mae: 2.383675, mean_q: 5.045928, mean_eps: 0.100000\n",
            "  803/10000: episode: 79, duration: 0.178s, episode steps:  12, steps per second:  67, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.203021, mae: 2.347663, mean_q: 4.919622, mean_eps: 0.100000\n",
            "  814/10000: episode: 80, duration: 0.201s, episode steps:  11, steps per second:  55, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.143676, mae: 2.430854, mean_q: 5.199737, mean_eps: 0.100000\n",
            "  826/10000: episode: 81, duration: 0.219s, episode steps:  12, steps per second:  55, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.152441, mae: 2.475894, mean_q: 5.240053, mean_eps: 0.100000\n",
            "  837/10000: episode: 82, duration: 0.166s, episode steps:  11, steps per second:  66, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.171704, mae: 2.519379, mean_q: 5.260123, mean_eps: 0.100000\n",
            "  850/10000: episode: 83, duration: 0.199s, episode steps:  13, steps per second:  65, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.159502, mae: 2.497326, mean_q: 5.150408, mean_eps: 0.100000\n",
            "  859/10000: episode: 84, duration: 0.138s, episode steps:   9, steps per second:  65, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.170652, mae: 2.485655, mean_q: 5.159789, mean_eps: 0.100000\n",
            "  869/10000: episode: 85, duration: 0.143s, episode steps:  10, steps per second:  70, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.178750, mae: 2.456077, mean_q: 5.042760, mean_eps: 0.100000\n",
            "  878/10000: episode: 86, duration: 0.140s, episode steps:   9, steps per second:  64, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.147208, mae: 2.517085, mean_q: 5.209276, mean_eps: 0.100000\n",
            "  887/10000: episode: 87, duration: 0.184s, episode steps:   9, steps per second:  49, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.165601, mae: 2.535382, mean_q: 5.239099, mean_eps: 0.100000\n",
            "  896/10000: episode: 88, duration: 0.195s, episode steps:   9, steps per second:  46, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.170669, mae: 2.584793, mean_q: 5.304307, mean_eps: 0.100000\n",
            "  907/10000: episode: 89, duration: 0.225s, episode steps:  11, steps per second:  49, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.157892, mae: 2.628778, mean_q: 5.361009, mean_eps: 0.100000\n",
            "  916/10000: episode: 90, duration: 0.227s, episode steps:   9, steps per second:  40, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.149724, mae: 2.639911, mean_q: 5.385120, mean_eps: 0.100000\n",
            "  925/10000: episode: 91, duration: 0.202s, episode steps:   9, steps per second:  44, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.134404, mae: 2.735126, mean_q: 5.583477, mean_eps: 0.100000\n",
            "  936/10000: episode: 92, duration: 0.274s, episode steps:  11, steps per second:  40, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.125589, mae: 2.698066, mean_q: 5.494072, mean_eps: 0.100000\n",
            "  945/10000: episode: 93, duration: 0.207s, episode steps:   9, steps per second:  44, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.143640, mae: 2.744903, mean_q: 5.547753, mean_eps: 0.100000\n",
            "  955/10000: episode: 94, duration: 0.206s, episode steps:  10, steps per second:  48, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.127558, mae: 2.694833, mean_q: 5.458164, mean_eps: 0.100000\n",
            "  966/10000: episode: 95, duration: 0.273s, episode steps:  11, steps per second:  40, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.119316, mae: 2.718846, mean_q: 5.506652, mean_eps: 0.100000\n",
            "  978/10000: episode: 96, duration: 0.309s, episode steps:  12, steps per second:  39, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.116390, mae: 2.724315, mean_q: 5.509720, mean_eps: 0.100000\n",
            "  988/10000: episode: 97, duration: 0.233s, episode steps:  10, steps per second:  43, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.154167, mae: 2.708136, mean_q: 5.397688, mean_eps: 0.100000\n",
            "  998/10000: episode: 98, duration: 0.236s, episode steps:  10, steps per second:  42, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.159718, mae: 2.775763, mean_q: 5.518388, mean_eps: 0.100000\n",
            " 1007/10000: episode: 99, duration: 0.190s, episode steps:   9, steps per second:  47, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.153383, mae: 2.759884, mean_q: 5.450374, mean_eps: 0.100000\n",
            " 1017/10000: episode: 100, duration: 0.231s, episode steps:  10, steps per second:  43, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.127637, mae: 2.894081, mean_q: 5.739799, mean_eps: 0.100000\n",
            " 1028/10000: episode: 101, duration: 0.257s, episode steps:  11, steps per second:  43, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.163367, mae: 2.931032, mean_q: 5.754985, mean_eps: 0.100000\n",
            " 1036/10000: episode: 102, duration: 0.173s, episode steps:   8, steps per second:  46, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.163711, mae: 2.820188, mean_q: 5.506622, mean_eps: 0.100000\n",
            " 1050/10000: episode: 103, duration: 0.271s, episode steps:  14, steps per second:  52, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.170444, mae: 2.952906, mean_q: 5.787698, mean_eps: 0.100000\n",
            " 1061/10000: episode: 104, duration: 0.243s, episode steps:  11, steps per second:  45, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.148748, mae: 2.892018, mean_q: 5.621743, mean_eps: 0.100000\n",
            " 1076/10000: episode: 105, duration: 0.255s, episode steps:  15, steps per second:  59, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.177333, mae: 2.935745, mean_q: 5.673910, mean_eps: 0.100000\n",
            " 1104/10000: episode: 106, duration: 0.427s, episode steps:  28, steps per second:  66, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.173432, mae: 3.046762, mean_q: 5.871509, mean_eps: 0.100000\n",
            " 1115/10000: episode: 107, duration: 0.189s, episode steps:  11, steps per second:  58, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.488996, mae: 3.174610, mean_q: 6.054038, mean_eps: 0.100000\n",
            " 1138/10000: episode: 108, duration: 0.377s, episode steps:  23, steps per second:  61, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.783 [0.000, 1.000],  loss: 0.293402, mae: 3.169574, mean_q: 6.056586, mean_eps: 0.100000\n",
            " 1152/10000: episode: 109, duration: 0.213s, episode steps:  14, steps per second:  66, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.595710, mae: 3.252777, mean_q: 6.166089, mean_eps: 0.100000\n",
            " 1174/10000: episode: 110, duration: 0.329s, episode steps:  22, steps per second:  67, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.448774, mae: 3.267520, mean_q: 6.233547, mean_eps: 0.100000\n",
            " 1198/10000: episode: 111, duration: 0.457s, episode steps:  24, steps per second:  53, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.499454, mae: 3.294157, mean_q: 6.219887, mean_eps: 0.100000\n",
            " 1224/10000: episode: 112, duration: 0.627s, episode steps:  26, steps per second:  41, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.731 [0.000, 1.000],  loss: 0.645074, mae: 3.324091, mean_q: 6.262877, mean_eps: 0.100000\n",
            " 1254/10000: episode: 113, duration: 0.581s, episode steps:  30, steps per second:  52, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.650590, mae: 3.392999, mean_q: 6.401597, mean_eps: 0.100000\n",
            " 1263/10000: episode: 114, duration: 0.194s, episode steps:   9, steps per second:  46, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.838600, mae: 3.568418, mean_q: 6.736082, mean_eps: 0.100000\n",
            " 1276/10000: episode: 115, duration: 0.258s, episode steps:  13, steps per second:  50, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.846 [0.000, 1.000],  loss: 0.836426, mae: 3.702270, mean_q: 7.017493, mean_eps: 0.100000\n",
            " 1287/10000: episode: 116, duration: 0.223s, episode steps:  11, steps per second:  49, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 1.333656, mae: 3.801989, mean_q: 7.138762, mean_eps: 0.100000\n",
            " 1302/10000: episode: 117, duration: 0.333s, episode steps:  15, steps per second:  45, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.807208, mae: 3.738637, mean_q: 7.119279, mean_eps: 0.100000\n",
            " 1311/10000: episode: 118, duration: 0.180s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.823243, mae: 3.725001, mean_q: 7.047423, mean_eps: 0.100000\n",
            " 1322/10000: episode: 119, duration: 0.228s, episode steps:  11, steps per second:  48, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.005842, mae: 3.722952, mean_q: 6.921537, mean_eps: 0.100000\n",
            " 1332/10000: episode: 120, duration: 0.190s, episode steps:  10, steps per second:  53, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.956166, mae: 3.862278, mean_q: 7.078471, mean_eps: 0.100000\n",
            " 1343/10000: episode: 121, duration: 0.179s, episode steps:  11, steps per second:  61, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.607661, mae: 3.970162, mean_q: 7.308958, mean_eps: 0.100000\n",
            " 1354/10000: episode: 122, duration: 0.187s, episode steps:  11, steps per second:  59, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 1.320230, mae: 3.948071, mean_q: 7.262992, mean_eps: 0.100000\n",
            " 1365/10000: episode: 123, duration: 0.162s, episode steps:  11, steps per second:  68, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 1.288289, mae: 3.990961, mean_q: 7.426912, mean_eps: 0.100000\n",
            " 1375/10000: episode: 124, duration: 0.145s, episode steps:  10, steps per second:  69, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.933244, mae: 3.989584, mean_q: 7.493449, mean_eps: 0.100000\n",
            " 1385/10000: episode: 125, duration: 0.151s, episode steps:  10, steps per second:  66, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.068365, mae: 3.949537, mean_q: 7.460380, mean_eps: 0.100000\n",
            " 1395/10000: episode: 126, duration: 0.155s, episode steps:  10, steps per second:  64, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.685624, mae: 4.131893, mean_q: 7.781354, mean_eps: 0.100000\n",
            " 1404/10000: episode: 127, duration: 0.149s, episode steps:   9, steps per second:  60, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.505199, mae: 4.077691, mean_q: 7.700468, mean_eps: 0.100000\n",
            " 1414/10000: episode: 128, duration: 0.194s, episode steps:  10, steps per second:  51, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.667131, mae: 4.160207, mean_q: 7.784210, mean_eps: 0.100000\n",
            " 1423/10000: episode: 129, duration: 0.179s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.667223, mae: 4.074998, mean_q: 7.560016, mean_eps: 0.100000\n",
            " 1433/10000: episode: 130, duration: 0.199s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.949041, mae: 4.179041, mean_q: 7.896270, mean_eps: 0.100000\n",
            " 1442/10000: episode: 131, duration: 0.252s, episode steps:   9, steps per second:  36, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.661170, mae: 4.294185, mean_q: 8.003369, mean_eps: 0.100000\n",
            " 1452/10000: episode: 132, duration: 0.179s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.009690, mae: 4.170267, mean_q: 7.876326, mean_eps: 0.100000\n",
            " 1462/10000: episode: 133, duration: 0.179s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 1.691793, mae: 4.264553, mean_q: 7.988424, mean_eps: 0.100000\n",
            " 1471/10000: episode: 134, duration: 0.140s, episode steps:   9, steps per second:  64, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.651964, mae: 4.406556, mean_q: 8.299057, mean_eps: 0.100000\n",
            " 1482/10000: episode: 135, duration: 0.162s, episode steps:  11, steps per second:  68, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 2.554132, mae: 4.375599, mean_q: 8.023758, mean_eps: 0.100000\n",
            " 1492/10000: episode: 136, duration: 0.163s, episode steps:  10, steps per second:  61, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 1.522457, mae: 4.255327, mean_q: 7.877489, mean_eps: 0.100000\n",
            " 1501/10000: episode: 137, duration: 0.142s, episode steps:   9, steps per second:  64, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.724817, mae: 4.347446, mean_q: 7.993763, mean_eps: 0.100000\n",
            " 1513/10000: episode: 138, duration: 0.180s, episode steps:  12, steps per second:  67, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 2.397520, mae: 4.433620, mean_q: 8.094413, mean_eps: 0.100000\n",
            " 1521/10000: episode: 139, duration: 0.131s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 2.325513, mae: 4.512698, mean_q: 8.159631, mean_eps: 0.100000\n",
            " 1531/10000: episode: 140, duration: 0.173s, episode steps:  10, steps per second:  58, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.559087, mae: 4.346397, mean_q: 7.958016, mean_eps: 0.100000\n",
            " 1542/10000: episode: 141, duration: 0.170s, episode steps:  11, steps per second:  65, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 1.319921, mae: 4.409284, mean_q: 8.186444, mean_eps: 0.100000\n",
            " 1556/10000: episode: 142, duration: 0.236s, episode steps:  14, steps per second:  59, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.857 [0.000, 1.000],  loss: 2.108526, mae: 4.439272, mean_q: 8.043378, mean_eps: 0.100000\n",
            " 1566/10000: episode: 143, duration: 0.160s, episode steps:  10, steps per second:  62, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 1.651972, mae: 4.389498, mean_q: 8.062366, mean_eps: 0.100000\n",
            " 1575/10000: episode: 144, duration: 0.186s, episode steps:   9, steps per second:  49, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.984788, mae: 4.563200, mean_q: 8.345536, mean_eps: 0.100000\n",
            " 1584/10000: episode: 145, duration: 0.214s, episode steps:   9, steps per second:  42, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.279020, mae: 4.280247, mean_q: 7.954438, mean_eps: 0.100000\n",
            " 1594/10000: episode: 146, duration: 0.197s, episode steps:  10, steps per second:  51, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.985708, mae: 4.465194, mean_q: 8.155459, mean_eps: 0.100000\n",
            " 1602/10000: episode: 147, duration: 0.177s, episode steps:   8, steps per second:  45, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.425071, mae: 4.524164, mean_q: 8.352981, mean_eps: 0.100000\n",
            " 1612/10000: episode: 148, duration: 0.198s, episode steps:  10, steps per second:  51, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.553532, mae: 4.515623, mean_q: 8.354576, mean_eps: 0.100000\n",
            " 1622/10000: episode: 149, duration: 0.224s, episode steps:  10, steps per second:  45, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.915764, mae: 4.524586, mean_q: 8.337259, mean_eps: 0.100000\n",
            " 1631/10000: episode: 150, duration: 0.191s, episode steps:   9, steps per second:  47, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.845863, mae: 4.373505, mean_q: 8.270263, mean_eps: 0.100000\n",
            " 1641/10000: episode: 151, duration: 0.209s, episode steps:  10, steps per second:  48, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.372481, mae: 4.536523, mean_q: 8.483424, mean_eps: 0.100000\n",
            " 1651/10000: episode: 152, duration: 0.228s, episode steps:  10, steps per second:  44, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.834950, mae: 4.485509, mean_q: 8.501335, mean_eps: 0.100000\n",
            " 1660/10000: episode: 153, duration: 0.237s, episode steps:   9, steps per second:  38, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.572648, mae: 4.501750, mean_q: 8.403085, mean_eps: 0.100000\n",
            " 1670/10000: episode: 154, duration: 0.212s, episode steps:  10, steps per second:  47, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.398452, mae: 4.513482, mean_q: 8.471260, mean_eps: 0.100000\n",
            " 1680/10000: episode: 155, duration: 0.198s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 2.299010, mae: 4.643961, mean_q: 8.434855, mean_eps: 0.100000\n",
            " 1688/10000: episode: 156, duration: 0.136s, episode steps:   8, steps per second:  59, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.010831, mae: 4.349711, mean_q: 8.043308, mean_eps: 0.100000\n",
            " 1699/10000: episode: 157, duration: 0.208s, episode steps:  11, steps per second:  53, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 1.344291, mae: 4.629551, mean_q: 8.607600, mean_eps: 0.100000\n",
            " 1708/10000: episode: 158, duration: 0.199s, episode steps:   9, steps per second:  45, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.623498, mae: 4.550769, mean_q: 8.428169, mean_eps: 0.100000\n",
            " 1721/10000: episode: 159, duration: 0.305s, episode steps:  13, steps per second:  43, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.846 [0.000, 1.000],  loss: 1.440351, mae: 4.629172, mean_q: 8.666094, mean_eps: 0.100000\n",
            " 1730/10000: episode: 160, duration: 0.210s, episode steps:   9, steps per second:  43, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.283470, mae: 4.617755, mean_q: 8.631219, mean_eps: 0.100000\n",
            " 1740/10000: episode: 161, duration: 0.272s, episode steps:  10, steps per second:  37, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.567467, mae: 4.725550, mean_q: 8.766331, mean_eps: 0.100000\n",
            " 1749/10000: episode: 162, duration: 0.241s, episode steps:   9, steps per second:  37, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.165853, mae: 4.641118, mean_q: 8.670766, mean_eps: 0.100000\n",
            " 1759/10000: episode: 163, duration: 0.199s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.356581, mae: 4.873373, mean_q: 9.157116, mean_eps: 0.100000\n",
            " 1769/10000: episode: 164, duration: 0.180s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 1.587801, mae: 4.711204, mean_q: 8.785595, mean_eps: 0.100000\n",
            " 1779/10000: episode: 165, duration: 0.168s, episode steps:  10, steps per second:  60, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 1.533795, mae: 4.726387, mean_q: 8.808792, mean_eps: 0.100000\n",
            " 1788/10000: episode: 166, duration: 0.143s, episode steps:   9, steps per second:  63, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.068841, mae: 4.690014, mean_q: 8.858760, mean_eps: 0.100000\n",
            " 1801/10000: episode: 167, duration: 0.204s, episode steps:  13, steps per second:  64, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.846 [0.000, 1.000],  loss: 1.620578, mae: 4.727490, mean_q: 8.778744, mean_eps: 0.100000\n",
            " 1810/10000: episode: 168, duration: 0.142s, episode steps:   9, steps per second:  63, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.896067, mae: 4.716074, mean_q: 8.902734, mean_eps: 0.100000\n",
            " 1820/10000: episode: 169, duration: 0.154s, episode steps:  10, steps per second:  65, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.388235, mae: 4.637475, mean_q: 8.657959, mean_eps: 0.100000\n",
            " 1830/10000: episode: 170, duration: 0.167s, episode steps:  10, steps per second:  60, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.599435, mae: 4.679919, mean_q: 8.637312, mean_eps: 0.100000\n",
            " 1839/10000: episode: 171, duration: 0.142s, episode steps:   9, steps per second:  63, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.528003, mae: 4.735436, mean_q: 8.752630, mean_eps: 0.100000\n",
            " 1849/10000: episode: 172, duration: 0.180s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.142952, mae: 4.718850, mean_q: 8.799840, mean_eps: 0.100000\n",
            " 1858/10000: episode: 173, duration: 0.144s, episode steps:   9, steps per second:  63, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.422058, mae: 4.735462, mean_q: 8.758886, mean_eps: 0.100000\n",
            " 1869/10000: episode: 174, duration: 0.169s, episode steps:  11, steps per second:  65, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 1.391671, mae: 4.748985, mean_q: 8.838966, mean_eps: 0.100000\n",
            " 1879/10000: episode: 175, duration: 0.144s, episode steps:  10, steps per second:  69, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.383240, mae: 4.641397, mean_q: 8.549774, mean_eps: 0.100000\n",
            " 1890/10000: episode: 176, duration: 0.228s, episode steps:  11, steps per second:  48, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 1.132875, mae: 4.718846, mean_q: 8.794048, mean_eps: 0.100000\n",
            " 1904/10000: episode: 177, duration: 0.289s, episode steps:  14, steps per second:  48, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 1.435496, mae: 4.819067, mean_q: 8.859039, mean_eps: 0.100000\n",
            " 1913/10000: episode: 178, duration: 0.189s, episode steps:   9, steps per second:  48, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.424650, mae: 4.793270, mean_q: 8.827054, mean_eps: 0.100000\n",
            " 1925/10000: episode: 179, duration: 0.278s, episode steps:  12, steps per second:  43, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 1.373332, mae: 4.838111, mean_q: 8.883512, mean_eps: 0.100000\n",
            " 1936/10000: episode: 180, duration: 0.190s, episode steps:  11, steps per second:  58, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 1.082238, mae: 4.682990, mean_q: 8.657381, mean_eps: 0.100000\n",
            " 1946/10000: episode: 181, duration: 0.173s, episode steps:  10, steps per second:  58, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 1.021764, mae: 4.606066, mean_q: 8.546036, mean_eps: 0.100000\n",
            " 1957/10000: episode: 182, duration: 0.160s, episode steps:  11, steps per second:  69, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 1.188500, mae: 4.841144, mean_q: 9.036253, mean_eps: 0.100000\n",
            " 1967/10000: episode: 183, duration: 0.168s, episode steps:  10, steps per second:  60, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.335505, mae: 4.893835, mean_q: 9.107896, mean_eps: 0.100000\n",
            " 1977/10000: episode: 184, duration: 0.158s, episode steps:  10, steps per second:  63, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.162190, mae: 4.808107, mean_q: 8.955868, mean_eps: 0.100000\n",
            " 1988/10000: episode: 185, duration: 0.175s, episode steps:  11, steps per second:  63, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 1.403547, mae: 4.810945, mean_q: 8.903916, mean_eps: 0.100000\n",
            " 1997/10000: episode: 186, duration: 0.154s, episode steps:   9, steps per second:  59, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.019393, mae: 4.766117, mean_q: 8.875944, mean_eps: 0.100000\n",
            " 2009/10000: episode: 187, duration: 0.262s, episode steps:  12, steps per second:  46, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 1.245669, mae: 4.728333, mean_q: 8.768255, mean_eps: 0.100000\n",
            " 2020/10000: episode: 188, duration: 0.246s, episode steps:  11, steps per second:  45, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 1.077515, mae: 4.781830, mean_q: 9.024381, mean_eps: 0.100000\n",
            " 2032/10000: episode: 189, duration: 0.299s, episode steps:  12, steps per second:  40, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 1.545023, mae: 4.901756, mean_q: 9.122726, mean_eps: 0.100000\n",
            " 2043/10000: episode: 190, duration: 0.262s, episode steps:  11, steps per second:  42, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 1.219589, mae: 4.773170, mean_q: 8.893928, mean_eps: 0.100000\n",
            " 2056/10000: episode: 191, duration: 0.240s, episode steps:  13, steps per second:  54, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 1.389613, mae: 4.860744, mean_q: 9.064365, mean_eps: 0.100000\n",
            " 2065/10000: episode: 192, duration: 0.161s, episode steps:   9, steps per second:  56, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.735852, mae: 4.785253, mean_q: 9.028132, mean_eps: 0.100000\n",
            " 2077/10000: episode: 193, duration: 0.196s, episode steps:  12, steps per second:  61, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 1.262147, mae: 4.735587, mean_q: 8.789172, mean_eps: 0.100000\n",
            " 2090/10000: episode: 194, duration: 0.243s, episode steps:  13, steps per second:  53, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 1.644598, mae: 4.985793, mean_q: 9.265277, mean_eps: 0.100000\n",
            " 2103/10000: episode: 195, duration: 0.298s, episode steps:  13, steps per second:  44, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1.005105, mae: 4.874406, mean_q: 9.120590, mean_eps: 0.100000\n",
            " 2170/10000: episode: 196, duration: 1.228s, episode steps:  67, steps per second:  55, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 1.107511, mae: 4.871744, mean_q: 9.124352, mean_eps: 0.100000\n",
            " 2181/10000: episode: 197, duration: 0.255s, episode steps:  11, steps per second:  43, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 1.199397, mae: 4.886178, mean_q: 9.086366, mean_eps: 0.100000\n",
            " 2201/10000: episode: 198, duration: 0.430s, episode steps:  20, steps per second:  47, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 1.278017, mae: 4.852145, mean_q: 9.058792, mean_eps: 0.100000\n",
            " 2284/10000: episode: 199, duration: 1.290s, episode steps:  83, steps per second:  64, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 1.104667, mae: 4.990515, mean_q: 9.350502, mean_eps: 0.100000\n",
            " 2363/10000: episode: 200, duration: 1.263s, episode steps:  79, steps per second:  63, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 1.054854, mae: 5.051876, mean_q: 9.466036, mean_eps: 0.100000\n",
            " 2437/10000: episode: 201, duration: 1.081s, episode steps:  74, steps per second:  68, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.446 [0.000, 1.000],  loss: 1.260564, mae: 5.131187, mean_q: 9.562417, mean_eps: 0.100000\n",
            " 2473/10000: episode: 202, duration: 0.591s, episode steps:  36, steps per second:  61, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 1.101995, mae: 5.175803, mean_q: 9.689871, mean_eps: 0.100000\n",
            " 2496/10000: episode: 203, duration: 0.437s, episode steps:  23, steps per second:  53, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 1.119809, mae: 5.273114, mean_q: 9.964130, mean_eps: 0.100000\n",
            " 2561/10000: episode: 204, duration: 1.204s, episode steps:  65, steps per second:  54, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.446 [0.000, 1.000],  loss: 1.218884, mae: 5.337589, mean_q: 9.973134, mean_eps: 0.100000\n",
            " 2632/10000: episode: 205, duration: 1.440s, episode steps:  71, steps per second:  49, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 1.223909, mae: 5.392521, mean_q: 10.076775, mean_eps: 0.100000\n",
            " 2679/10000: episode: 206, duration: 0.699s, episode steps:  47, steps per second:  67, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  loss: 1.337746, mae: 5.552218, mean_q: 10.408227, mean_eps: 0.100000\n",
            " 2755/10000: episode: 207, duration: 1.130s, episode steps:  76, steps per second:  67, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 1.277983, mae: 5.635370, mean_q: 10.573808, mean_eps: 0.100000\n",
            " 2827/10000: episode: 208, duration: 1.474s, episode steps:  72, steps per second:  49, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 1.481281, mae: 5.750617, mean_q: 10.790242, mean_eps: 0.100000\n",
            " 2884/10000: episode: 209, duration: 1.237s, episode steps:  57, steps per second:  46, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.274465, mae: 5.873431, mean_q: 11.077974, mean_eps: 0.100000\n",
            " 2959/10000: episode: 210, duration: 1.469s, episode steps:  75, steps per second:  51, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 1.447280, mae: 6.026456, mean_q: 11.350291, mean_eps: 0.100000\n",
            " 3020/10000: episode: 211, duration: 0.937s, episode steps:  61, steps per second:  65, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 1.672815, mae: 6.172873, mean_q: 11.603545, mean_eps: 0.100000\n",
            " 3053/10000: episode: 212, duration: 0.619s, episode steps:  33, steps per second:  53, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 1.848416, mae: 6.262124, mean_q: 11.751600, mean_eps: 0.100000\n",
            " 3178/10000: episode: 213, duration: 2.151s, episode steps: 125, steps per second:  58, episode reward: 125.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 1.625601, mae: 6.428998, mean_q: 12.129662, mean_eps: 0.100000\n",
            " 3253/10000: episode: 214, duration: 1.184s, episode steps:  75, steps per second:  63, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 1.502698, mae: 6.642934, mean_q: 12.600866, mean_eps: 0.100000\n",
            " 3310/10000: episode: 215, duration: 0.911s, episode steps:  57, steps per second:  63, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.840151, mae: 6.730899, mean_q: 12.683095, mean_eps: 0.100000\n",
            " 3352/10000: episode: 216, duration: 0.687s, episode steps:  42, steps per second:  61, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 1.894974, mae: 6.836389, mean_q: 12.914851, mean_eps: 0.100000\n",
            " 3404/10000: episode: 217, duration: 0.830s, episode steps:  52, steps per second:  63, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.634739, mae: 6.852868, mean_q: 13.006373, mean_eps: 0.100000\n",
            " 3469/10000: episode: 218, duration: 1.094s, episode steps:  65, steps per second:  59, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 2.004807, mae: 7.099487, mean_q: 13.426632, mean_eps: 0.100000\n",
            " 3574/10000: episode: 219, duration: 1.583s, episode steps: 105, steps per second:  66, episode reward: 105.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 2.341885, mae: 7.133178, mean_q: 13.458282, mean_eps: 0.100000\n",
            " 3640/10000: episode: 220, duration: 1.211s, episode steps:  66, steps per second:  54, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 2.249049, mae: 7.267287, mean_q: 13.736161, mean_eps: 0.100000\n",
            " 3683/10000: episode: 221, duration: 0.631s, episode steps:  43, steps per second:  68, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 2.149203, mae: 7.328540, mean_q: 13.880121, mean_eps: 0.100000\n",
            " 3725/10000: episode: 222, duration: 0.638s, episode steps:  42, steps per second:  66, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 1.840303, mae: 7.474457, mean_q: 14.265740, mean_eps: 0.100000\n",
            " 3861/10000: episode: 223, duration: 2.088s, episode steps: 136, steps per second:  65, episode reward: 136.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 2.148054, mae: 7.715477, mean_q: 14.673417, mean_eps: 0.100000\n",
            " 3942/10000: episode: 224, duration: 1.703s, episode steps:  81, steps per second:  48, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 2.455843, mae: 7.843105, mean_q: 14.874607, mean_eps: 0.100000\n",
            " 4051/10000: episode: 225, duration: 2.124s, episode steps: 109, steps per second:  51, episode reward: 109.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 2.332091, mae: 7.968758, mean_q: 15.176608, mean_eps: 0.100000\n",
            " 4101/10000: episode: 226, duration: 1.044s, episode steps:  50, steps per second:  48, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 2.568950, mae: 8.142302, mean_q: 15.496598, mean_eps: 0.100000\n",
            " 4181/10000: episode: 227, duration: 1.306s, episode steps:  80, steps per second:  61, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 2.891142, mae: 8.373449, mean_q: 15.910169, mean_eps: 0.100000\n",
            " 4230/10000: episode: 228, duration: 0.795s, episode steps:  49, steps per second:  62, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 2.763301, mae: 8.386283, mean_q: 15.952358, mean_eps: 0.100000\n",
            " 4326/10000: episode: 229, duration: 1.921s, episode steps:  96, steps per second:  50, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 2.403999, mae: 8.528959, mean_q: 16.287704, mean_eps: 0.100000\n",
            " 4421/10000: episode: 230, duration: 1.997s, episode steps:  95, steps per second:  48, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 2.138801, mae: 8.757317, mean_q: 16.836220, mean_eps: 0.100000\n",
            " 4469/10000: episode: 231, duration: 1.089s, episode steps:  48, steps per second:  44, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 3.451018, mae: 8.857565, mean_q: 16.847268, mean_eps: 0.100000\n",
            " 4527/10000: episode: 232, duration: 1.334s, episode steps:  58, steps per second:  43, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 2.832972, mae: 8.946871, mean_q: 17.125893, mean_eps: 0.100000\n",
            " 4603/10000: episode: 233, duration: 1.356s, episode steps:  76, steps per second:  56, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 3.525378, mae: 9.031317, mean_q: 17.173656, mean_eps: 0.100000\n",
            " 4654/10000: episode: 234, duration: 1.168s, episode steps:  51, steps per second:  44, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 3.044929, mae: 9.247737, mean_q: 17.703133, mean_eps: 0.100000\n",
            " 4712/10000: episode: 235, duration: 0.960s, episode steps:  58, steps per second:  60, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 2.821415, mae: 9.249703, mean_q: 17.740305, mean_eps: 0.100000\n",
            " 4775/10000: episode: 236, duration: 1.054s, episode steps:  63, steps per second:  60, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 2.198083, mae: 9.460718, mean_q: 18.219257, mean_eps: 0.100000\n",
            " 4820/10000: episode: 237, duration: 0.828s, episode steps:  45, steps per second:  54, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 3.582677, mae: 9.559081, mean_q: 18.243499, mean_eps: 0.100000\n",
            " 4899/10000: episode: 238, duration: 1.570s, episode steps:  79, steps per second:  50, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 3.249556, mae: 9.601273, mean_q: 18.401435, mean_eps: 0.100000\n",
            " 4953/10000: episode: 239, duration: 1.154s, episode steps:  54, steps per second:  47, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 3.483276, mae: 9.693112, mean_q: 18.546752, mean_eps: 0.100000\n",
            " 5088/10000: episode: 240, duration: 2.859s, episode steps: 135, steps per second:  47, episode reward: 135.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 2.528416, mae: 9.925833, mean_q: 19.178062, mean_eps: 0.100000\n",
            " 5141/10000: episode: 241, duration: 0.813s, episode steps:  53, steps per second:  65, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 2.603500, mae: 10.158953, mean_q: 19.634981, mean_eps: 0.100000\n",
            " 5200/10000: episode: 242, duration: 1.311s, episode steps:  59, steps per second:  45, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 3.199053, mae: 10.260242, mean_q: 19.731743, mean_eps: 0.100000\n",
            " 5277/10000: episode: 243, duration: 1.344s, episode steps:  77, steps per second:  57, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 3.124240, mae: 10.388919, mean_q: 20.047336, mean_eps: 0.100000\n",
            " 5337/10000: episode: 244, duration: 0.965s, episode steps:  60, steps per second:  62, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 3.560276, mae: 10.458427, mean_q: 20.115824, mean_eps: 0.100000\n",
            " 5386/10000: episode: 245, duration: 0.874s, episode steps:  49, steps per second:  56, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 3.846390, mae: 10.416283, mean_q: 19.993070, mean_eps: 0.100000\n",
            " 5494/10000: episode: 246, duration: 1.645s, episode steps: 108, steps per second:  66, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 4.162432, mae: 10.739621, mean_q: 20.633844, mean_eps: 0.100000\n",
            " 5544/10000: episode: 247, duration: 0.744s, episode steps:  50, steps per second:  67, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 4.151357, mae: 10.607745, mean_q: 20.316828, mean_eps: 0.100000\n",
            " 5610/10000: episode: 248, duration: 0.990s, episode steps:  66, steps per second:  67, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 3.328862, mae: 10.802743, mean_q: 20.869816, mean_eps: 0.100000\n",
            " 5658/10000: episode: 249, duration: 0.752s, episode steps:  48, steps per second:  64, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 3.665116, mae: 10.933007, mean_q: 21.096334, mean_eps: 0.100000\n",
            " 5756/10000: episode: 250, duration: 1.634s, episode steps:  98, steps per second:  60, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 3.700928, mae: 11.090061, mean_q: 21.353155, mean_eps: 0.100000\n",
            " 5823/10000: episode: 251, duration: 1.036s, episode steps:  67, steps per second:  65, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 3.989992, mae: 11.062309, mean_q: 21.319067, mean_eps: 0.100000\n",
            " 5885/10000: episode: 252, duration: 1.080s, episode steps:  62, steps per second:  57, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 3.479778, mae: 11.344411, mean_q: 21.956289, mean_eps: 0.100000\n",
            " 5934/10000: episode: 253, duration: 1.052s, episode steps:  49, steps per second:  47, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 3.270371, mae: 11.281359, mean_q: 21.816670, mean_eps: 0.100000\n",
            " 5988/10000: episode: 254, duration: 1.009s, episode steps:  54, steps per second:  54, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 3.522053, mae: 11.457027, mean_q: 22.201312, mean_eps: 0.100000\n",
            " 6073/10000: episode: 255, duration: 1.511s, episode steps:  85, steps per second:  56, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 3.556546, mae: 11.551307, mean_q: 22.394366, mean_eps: 0.100000\n",
            " 6136/10000: episode: 256, duration: 1.005s, episode steps:  63, steps per second:  63, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 4.126350, mae: 11.637807, mean_q: 22.478994, mean_eps: 0.100000\n",
            " 6220/10000: episode: 257, duration: 1.176s, episode steps:  84, steps per second:  71, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 3.631513, mae: 11.649010, mean_q: 22.608776, mean_eps: 0.100000\n",
            " 6288/10000: episode: 258, duration: 1.247s, episode steps:  68, steps per second:  55, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 4.089464, mae: 11.890969, mean_q: 23.039137, mean_eps: 0.100000\n",
            " 6371/10000: episode: 259, duration: 1.557s, episode steps:  83, steps per second:  53, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 4.740871, mae: 11.943235, mean_q: 23.054846, mean_eps: 0.100000\n",
            " 6455/10000: episode: 260, duration: 1.469s, episode steps:  84, steps per second:  57, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 4.186555, mae: 12.063310, mean_q: 23.370890, mean_eps: 0.100000\n",
            " 6507/10000: episode: 261, duration: 1.043s, episode steps:  52, steps per second:  50, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 4.230492, mae: 12.284961, mean_q: 23.843551, mean_eps: 0.100000\n",
            " 6588/10000: episode: 262, duration: 1.149s, episode steps:  81, steps per second:  70, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 3.581839, mae: 12.367134, mean_q: 24.052814, mean_eps: 0.100000\n",
            " 6636/10000: episode: 263, duration: 0.953s, episode steps:  48, steps per second:  50, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 5.027526, mae: 12.601574, mean_q: 24.381015, mean_eps: 0.100000\n",
            " 6713/10000: episode: 264, duration: 1.304s, episode steps:  77, steps per second:  59, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 4.309685, mae: 12.596238, mean_q: 24.408322, mean_eps: 0.100000\n",
            " 6802/10000: episode: 265, duration: 2.006s, episode steps:  89, steps per second:  44, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 4.958611, mae: 12.629215, mean_q: 24.444178, mean_eps: 0.100000\n",
            " 6870/10000: episode: 266, duration: 1.215s, episode steps:  68, steps per second:  56, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.456 [0.000, 1.000],  loss: 4.190800, mae: 12.600663, mean_q: 24.501704, mean_eps: 0.100000\n",
            " 6966/10000: episode: 267, duration: 1.905s, episode steps:  96, steps per second:  50, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 5.119674, mae: 12.966745, mean_q: 25.168685, mean_eps: 0.100000\n",
            " 7035/10000: episode: 268, duration: 1.353s, episode steps:  69, steps per second:  51, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 4.858697, mae: 13.077646, mean_q: 25.401178, mean_eps: 0.100000\n",
            " 7115/10000: episode: 269, duration: 1.560s, episode steps:  80, steps per second:  51, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 5.177759, mae: 13.058766, mean_q: 25.326786, mean_eps: 0.100000\n",
            " 7246/10000: episode: 270, duration: 2.778s, episode steps: 131, steps per second:  47, episode reward: 131.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 5.240539, mae: 13.162283, mean_q: 25.500809, mean_eps: 0.100000\n",
            " 7336/10000: episode: 271, duration: 1.515s, episode steps:  90, steps per second:  59, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 4.383553, mae: 13.340645, mean_q: 25.986264, mean_eps: 0.100000\n",
            " 7457/10000: episode: 272, duration: 2.538s, episode steps: 121, steps per second:  48, episode reward: 121.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 3.743877, mae: 13.511559, mean_q: 26.407834, mean_eps: 0.100000\n",
            " 7657/10000: episode: 273, duration: 3.648s, episode steps: 200, steps per second:  55, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 4.997532, mae: 13.807760, mean_q: 26.922972, mean_eps: 0.100000\n",
            " 7768/10000: episode: 274, duration: 1.934s, episode steps: 111, steps per second:  57, episode reward: 111.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 5.821703, mae: 13.943511, mean_q: 27.090632, mean_eps: 0.100000\n",
            " 7883/10000: episode: 275, duration: 1.828s, episode steps: 115, steps per second:  63, episode reward: 115.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 5.212867, mae: 14.015731, mean_q: 27.313844, mean_eps: 0.100000\n",
            " 7964/10000: episode: 276, duration: 1.680s, episode steps:  81, steps per second:  48, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 5.093113, mae: 14.200964, mean_q: 27.687022, mean_eps: 0.100000\n",
            " 8059/10000: episode: 277, duration: 1.967s, episode steps:  95, steps per second:  48, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 4.984336, mae: 14.372893, mean_q: 28.074328, mean_eps: 0.100000\n",
            " 8259/10000: episode: 278, duration: 3.003s, episode steps: 200, steps per second:  67, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 5.355795, mae: 14.515951, mean_q: 28.338382, mean_eps: 0.100000\n",
            " 8379/10000: episode: 279, duration: 2.274s, episode steps: 120, steps per second:  53, episode reward: 120.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 5.083776, mae: 14.638571, mean_q: 28.612203, mean_eps: 0.100000\n",
            " 8454/10000: episode: 280, duration: 1.253s, episode steps:  75, steps per second:  60, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 6.562634, mae: 14.875960, mean_q: 29.027308, mean_eps: 0.100000\n",
            " 8572/10000: episode: 281, duration: 2.434s, episode steps: 118, steps per second:  48, episode reward: 118.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 5.613428, mae: 14.798635, mean_q: 28.934663, mean_eps: 0.100000\n",
            " 8772/10000: episode: 282, duration: 4.145s, episode steps: 200, steps per second:  48, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.282958, mae: 15.101172, mean_q: 29.600115, mean_eps: 0.100000\n",
            " 8972/10000: episode: 283, duration: 3.327s, episode steps: 200, steps per second:  60, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 5.049256, mae: 15.348103, mean_q: 30.173345, mean_eps: 0.100000\n",
            " 9101/10000: episode: 284, duration: 2.299s, episode steps: 129, steps per second:  56, episode reward: 129.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 5.493521, mae: 15.579026, mean_q: 30.570025, mean_eps: 0.100000\n",
            " 9301/10000: episode: 285, duration: 3.218s, episode steps: 200, steps per second:  62, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 5.948530, mae: 15.807408, mean_q: 31.026643, mean_eps: 0.100000\n",
            " 9411/10000: episode: 286, duration: 2.011s, episode steps: 110, steps per second:  55, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 4.355952, mae: 16.063273, mean_q: 31.690031, mean_eps: 0.100000\n",
            " 9572/10000: episode: 287, duration: 2.738s, episode steps: 161, steps per second:  59, episode reward: 161.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 5.708603, mae: 16.186787, mean_q: 31.837456, mean_eps: 0.100000\n",
            " 9771/10000: episode: 288, duration: 3.343s, episode steps: 199, steps per second:  60, episode reward: 199.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 5.581187, mae: 16.362741, mean_q: 32.243667, mean_eps: 0.100000\n",
            " 9899/10000: episode: 289, duration: 1.842s, episode steps: 128, steps per second:  69, episode reward: 128.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.445 [0.000, 1.000],  loss: 5.923899, mae: 16.631831, mean_q: 32.772948, mean_eps: 0.100000\n",
            "done, took 194.200 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 140.000, steps: 140\n",
            "Episode 3: reward: 145.000, steps: 145\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 187.000, steps: 187\n",
            "Episode 6: reward: 150.000, steps: 150\n",
            "Episode 7: reward: 152.000, steps: 152\n",
            "Episode 8: reward: 155.000, steps: 155\n",
            "Episode 9: reward: 200.000, steps: 200\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 147.000, steps: 147\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 170.000, steps: 170\n",
            "Episode 15: reward: 200.000, steps: 200\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 198.000, steps: 198\n",
            "Episode 18: reward: 133.000, steps: 133\n",
            "Episode 19: reward: 200.000, steps: 200\n",
            "Episode 20: reward: 200.000, steps: 200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb171618c10>"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    }
  ]
}