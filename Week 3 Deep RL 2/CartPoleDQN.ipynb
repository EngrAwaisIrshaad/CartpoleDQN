{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPoleDQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Megacity1/CartpoleDQN/blob/main/Week%203%20Deep%20RL%202/CartPoleDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Deep Q Network (DQN) for CartPole Using Boltzmann Q Policy\n",
        "This exercise implements a DQN for CartPole using a Boltzmann Q policy for selecting the actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGsC7cJ5jNcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda73cbf-3020-40f4-e45e-7b87053448f1"
      },
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.24.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (13.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.44.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.10.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMIHLgQ3Z-lF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5a8608-456f-4127-ce8a-f551e5f4e4f3"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0AMLzq08ap0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b380e1-6c3d-4bce-8233-a8ac01deaf26"
      },
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "!pip install Adam\n",
        "#from keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Adam in /usr/local/lib/python3.7/dist-packages (0.0.0.dev0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll6bNdUm54WS"
      },
      "source": [
        "Implementation of DQN for CartPole, applying policy BoltzmannQPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCrPKNy40PC"
      },
      "source": [
        "##Implement DQN with BoltzmannGumbelQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efM9jkXr5A3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0db9f804-e5b1-453e-a47a-909b3367b8b7"
      },
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import BoltzmannGumbelQPolicy\n",
        "from rl.policy import LinearAnnealedPolicy\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n",
        "# setup experience replay buffer\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
        "                               attr='eps',            \n",
        "                               value_max=4.,\n",
        "                               value_min=.4, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=20)\n",
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "# add extra layers here\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=20,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy) \n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=6000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_36\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_34 (Flatten)        (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_69 (Dense)            (None, 16)                80        \n",
            "                                                                 \n",
            " dense_70 (Dense)            (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 6000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   21/6000: episode: 1, duration: 8.238s, episode steps:  21, steps per second:   3, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.381 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   50/6000: episode: 2, duration: 0.364s, episode steps:  29, steps per second:  80, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 0.546896, mae: 0.652534, mean_q: 0.414838, mean_eps: 0.400000\n",
            "   65/6000: episode: 3, duration: 0.194s, episode steps:  15, steps per second:  77, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.133 [0.000, 1.000],  loss: 0.427334, mae: 0.595762, mean_q: 0.423149, mean_eps: 0.400000\n",
            "   75/6000: episode: 4, duration: 0.162s, episode steps:  10, steps per second:  62, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.429660, mae: 0.654927, mean_q: 0.680041, mean_eps: 0.400000\n",
            "   85/6000: episode: 5, duration: 0.132s, episode steps:  10, steps per second:  76, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.426140, mae: 0.710891, mean_q: 0.876172, mean_eps: 0.400000\n",
            "   97/6000: episode: 6, duration: 0.161s, episode steps:  12, steps per second:  75, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.420865, mae: 0.775012, mean_q: 1.053384, mean_eps: 0.400000\n",
            "  111/6000: episode: 7, duration: 0.172s, episode steps:  14, steps per second:  81, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.214 [0.000, 1.000],  loss: 0.398350, mae: 0.800033, mean_q: 1.103455, mean_eps: 0.400000\n",
            "  132/6000: episode: 8, duration: 0.261s, episode steps:  21, steps per second:  80, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.362665, mae: 0.851785, mean_q: 1.247904, mean_eps: 0.400000\n",
            "  146/6000: episode: 9, duration: 0.215s, episode steps:  14, steps per second:  65, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.143 [0.000, 1.000],  loss: 0.388319, mae: 0.907269, mean_q: 1.395155, mean_eps: 0.400000\n",
            "  156/6000: episode: 10, duration: 0.155s, episode steps:  10, steps per second:  64, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.432417, mae: 0.987788, mean_q: 1.504797, mean_eps: 0.400000\n",
            "  167/6000: episode: 11, duration: 0.184s, episode steps:  11, steps per second:  60, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.343062, mae: 0.995148, mean_q: 1.537878, mean_eps: 0.400000\n",
            "  179/6000: episode: 12, duration: 0.233s, episode steps:  12, steps per second:  51, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.502708, mae: 1.121406, mean_q: 1.790237, mean_eps: 0.400000\n",
            "  190/6000: episode: 13, duration: 0.200s, episode steps:  11, steps per second:  55, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.497398, mae: 1.125619, mean_q: 1.713004, mean_eps: 0.400000\n",
            "  199/6000: episode: 14, duration: 0.186s, episode steps:   9, steps per second:  48, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.446094, mae: 1.155575, mean_q: 1.786333, mean_eps: 0.400000\n",
            "  211/6000: episode: 15, duration: 0.248s, episode steps:  12, steps per second:  48, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.418657, mae: 1.180086, mean_q: 1.838319, mean_eps: 0.400000\n",
            "  222/6000: episode: 16, duration: 0.194s, episode steps:  11, steps per second:  57, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.573874, mae: 1.302790, mean_q: 2.051586, mean_eps: 0.400000\n",
            "  233/6000: episode: 17, duration: 0.193s, episode steps:  11, steps per second:  57, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.493827, mae: 1.333512, mean_q: 2.142196, mean_eps: 0.400000\n",
            "  247/6000: episode: 18, duration: 0.181s, episode steps:  14, steps per second:  77, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.214 [0.000, 1.000],  loss: 0.472315, mae: 1.361193, mean_q: 2.199456, mean_eps: 0.400000\n",
            "  265/6000: episode: 19, duration: 0.236s, episode steps:  18, steps per second:  76, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.515837, mae: 1.399351, mean_q: 2.316779, mean_eps: 0.400000\n",
            "  275/6000: episode: 20, duration: 0.145s, episode steps:  10, steps per second:  69, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.614906, mae: 1.505618, mean_q: 2.488524, mean_eps: 0.400000\n",
            "  285/6000: episode: 21, duration: 0.145s, episode steps:  10, steps per second:  69, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.639168, mae: 1.569049, mean_q: 2.560577, mean_eps: 0.400000\n",
            "  297/6000: episode: 22, duration: 0.160s, episode steps:  12, steps per second:  75, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.618265, mae: 1.600320, mean_q: 2.613533, mean_eps: 0.400000\n",
            "  310/6000: episode: 23, duration: 0.171s, episode steps:  13, steps per second:  76, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.077 [0.000, 1.000],  loss: 0.554126, mae: 1.605350, mean_q: 2.635749, mean_eps: 0.400000\n",
            "  322/6000: episode: 24, duration: 0.162s, episode steps:  12, steps per second:  74, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.469865, mae: 1.621843, mean_q: 2.763663, mean_eps: 0.400000\n",
            "  344/6000: episode: 25, duration: 0.305s, episode steps:  22, steps per second:  72, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.227 [0.000, 1.000],  loss: 0.627074, mae: 1.707614, mean_q: 2.909642, mean_eps: 0.400000\n",
            "  353/6000: episode: 26, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.575868, mae: 1.731222, mean_q: 2.979853, mean_eps: 0.400000\n",
            "  372/6000: episode: 27, duration: 0.232s, episode steps:  19, steps per second:  82, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.263 [0.000, 1.000],  loss: 0.705695, mae: 1.794435, mean_q: 3.069280, mean_eps: 0.400000\n",
            "  381/6000: episode: 28, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.556502, mae: 1.811125, mean_q: 3.202378, mean_eps: 0.400000\n",
            "  396/6000: episode: 29, duration: 0.198s, episode steps:  15, steps per second:  76, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 0.549787, mae: 1.790905, mean_q: 3.243903, mean_eps: 0.400000\n",
            "  405/6000: episode: 30, duration: 0.124s, episode steps:   9, steps per second:  73, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.682676, mae: 1.880258, mean_q: 3.363742, mean_eps: 0.400000\n",
            "  417/6000: episode: 31, duration: 0.182s, episode steps:  12, steps per second:  66, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.454814, mae: 1.861800, mean_q: 3.426268, mean_eps: 0.400000\n",
            "  427/6000: episode: 32, duration: 0.146s, episode steps:  10, steps per second:  68, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.583925, mae: 1.909255, mean_q: 3.505774, mean_eps: 0.400000\n",
            "  438/6000: episode: 33, duration: 0.174s, episode steps:  11, steps per second:  63, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.582161, mae: 1.959107, mean_q: 3.574273, mean_eps: 0.400000\n",
            "  448/6000: episode: 34, duration: 0.145s, episode steps:  10, steps per second:  69, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.686833, mae: 2.011420, mean_q: 3.660051, mean_eps: 0.400000\n",
            "  460/6000: episode: 35, duration: 0.168s, episode steps:  12, steps per second:  72, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.525258, mae: 2.008751, mean_q: 3.683712, mean_eps: 0.400000\n",
            "  474/6000: episode: 36, duration: 0.194s, episode steps:  14, steps per second:  72, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.566587, mae: 2.090815, mean_q: 3.851289, mean_eps: 0.400000\n",
            "  485/6000: episode: 37, duration: 0.179s, episode steps:  11, steps per second:  61, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.547099, mae: 2.105591, mean_q: 3.914585, mean_eps: 0.400000\n",
            "  502/6000: episode: 38, duration: 0.274s, episode steps:  17, steps per second:  62, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 0.511597, mae: 2.164808, mean_q: 3.970210, mean_eps: 0.400000\n",
            "  512/6000: episode: 39, duration: 0.169s, episode steps:  10, steps per second:  59, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.553339, mae: 2.206930, mean_q: 4.004180, mean_eps: 0.400000\n",
            "  523/6000: episode: 40, duration: 0.200s, episode steps:  11, steps per second:  55, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.557870, mae: 2.299020, mean_q: 4.125731, mean_eps: 0.400000\n",
            "  534/6000: episode: 41, duration: 0.153s, episode steps:  11, steps per second:  72, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.570539, mae: 2.264206, mean_q: 4.179376, mean_eps: 0.400000\n",
            "  548/6000: episode: 42, duration: 0.199s, episode steps:  14, steps per second:  70, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.443252, mae: 2.264269, mean_q: 4.252003, mean_eps: 0.400000\n",
            "  558/6000: episode: 43, duration: 0.154s, episode steps:  10, steps per second:  65, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.662492, mae: 2.379642, mean_q: 4.393722, mean_eps: 0.400000\n",
            "  567/6000: episode: 44, duration: 0.147s, episode steps:   9, steps per second:  61, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.470434, mae: 2.348477, mean_q: 4.378721, mean_eps: 0.400000\n",
            "  582/6000: episode: 45, duration: 0.220s, episode steps:  15, steps per second:  68, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 0.488171, mae: 2.385261, mean_q: 4.445930, mean_eps: 0.400000\n",
            "  591/6000: episode: 46, duration: 0.146s, episode steps:   9, steps per second:  61, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.578673, mae: 2.408107, mean_q: 4.519289, mean_eps: 0.400000\n",
            "  601/6000: episode: 47, duration: 0.153s, episode steps:  10, steps per second:  65, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.393834, mae: 2.402170, mean_q: 4.605106, mean_eps: 0.400000\n",
            "  614/6000: episode: 48, duration: 0.188s, episode steps:  13, steps per second:  69, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.632342, mae: 2.517752, mean_q: 4.705943, mean_eps: 0.400000\n",
            "  623/6000: episode: 49, duration: 0.137s, episode steps:   9, steps per second:  65, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.561022, mae: 2.505802, mean_q: 4.662786, mean_eps: 0.400000\n",
            "  633/6000: episode: 50, duration: 0.146s, episode steps:  10, steps per second:  68, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.631295, mae: 2.552041, mean_q: 4.653469, mean_eps: 0.400000\n",
            "  642/6000: episode: 51, duration: 0.133s, episode steps:   9, steps per second:  68, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.469145, mae: 2.543568, mean_q: 4.741904, mean_eps: 0.400000\n",
            "  654/6000: episode: 52, duration: 0.156s, episode steps:  12, steps per second:  77, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.425420, mae: 2.561551, mean_q: 4.932236, mean_eps: 0.400000\n",
            "  664/6000: episode: 53, duration: 0.153s, episode steps:  10, steps per second:  65, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.405352, mae: 2.598954, mean_q: 5.051636, mean_eps: 0.400000\n",
            "  676/6000: episode: 54, duration: 0.158s, episode steps:  12, steps per second:  76, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.508019, mae: 2.684412, mean_q: 5.087638, mean_eps: 0.400000\n",
            "  689/6000: episode: 55, duration: 0.193s, episode steps:  13, steps per second:  67, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.536463, mae: 2.743840, mean_q: 5.072471, mean_eps: 0.400000\n",
            "  699/6000: episode: 56, duration: 0.135s, episode steps:  10, steps per second:  74, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.596099, mae: 2.782603, mean_q: 5.078196, mean_eps: 0.400000\n",
            "  711/6000: episode: 57, duration: 0.165s, episode steps:  12, steps per second:  73, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.554148, mae: 2.781096, mean_q: 5.130500, mean_eps: 0.400000\n",
            "  730/6000: episode: 58, duration: 0.253s, episode steps:  19, steps per second:  75, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 0.535938, mae: 2.820066, mean_q: 5.292028, mean_eps: 0.400000\n",
            "  740/6000: episode: 59, duration: 0.137s, episode steps:  10, steps per second:  73, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.433900, mae: 2.827032, mean_q: 5.326239, mean_eps: 0.400000\n",
            "  749/6000: episode: 60, duration: 0.133s, episode steps:   9, steps per second:  68, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.330774, mae: 2.835103, mean_q: 5.397380, mean_eps: 0.400000\n",
            "  760/6000: episode: 61, duration: 0.146s, episode steps:  11, steps per second:  75, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.737994, mae: 2.940222, mean_q: 5.401612, mean_eps: 0.400000\n",
            "  769/6000: episode: 62, duration: 0.153s, episode steps:   9, steps per second:  59, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.472021, mae: 2.906168, mean_q: 5.409308, mean_eps: 0.400000\n",
            "  779/6000: episode: 63, duration: 0.158s, episode steps:  10, steps per second:  63, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.551949, mae: 2.945721, mean_q: 5.421302, mean_eps: 0.400000\n",
            "  794/6000: episode: 64, duration: 0.214s, episode steps:  15, steps per second:  70, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 0.571114, mae: 2.976593, mean_q: 5.477916, mean_eps: 0.400000\n",
            "  806/6000: episode: 65, duration: 0.155s, episode steps:  12, steps per second:  78, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.526198, mae: 3.006740, mean_q: 5.466695, mean_eps: 0.400000\n",
            "  816/6000: episode: 66, duration: 0.141s, episode steps:  10, steps per second:  71, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.500535, mae: 2.997130, mean_q: 5.586398, mean_eps: 0.400000\n",
            "  827/6000: episode: 67, duration: 0.143s, episode steps:  11, steps per second:  77, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.489434, mae: 3.027902, mean_q: 5.687922, mean_eps: 0.400000\n",
            "  838/6000: episode: 68, duration: 0.165s, episode steps:  11, steps per second:  67, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.461545, mae: 3.096850, mean_q: 5.767780, mean_eps: 0.400000\n",
            "  851/6000: episode: 69, duration: 0.162s, episode steps:  13, steps per second:  80, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.343121, mae: 3.115878, mean_q: 5.859821, mean_eps: 0.400000\n",
            "  865/6000: episode: 70, duration: 0.183s, episode steps:  14, steps per second:  76, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.486664, mae: 3.185331, mean_q: 5.914138, mean_eps: 0.400000\n",
            "  889/6000: episode: 71, duration: 0.335s, episode steps:  24, steps per second:  72, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.388225, mae: 3.218350, mean_q: 5.989250, mean_eps: 0.400000\n",
            "  904/6000: episode: 72, duration: 0.211s, episode steps:  15, steps per second:  71, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.402108, mae: 3.262928, mean_q: 6.069399, mean_eps: 0.400000\n",
            "  926/6000: episode: 73, duration: 0.289s, episode steps:  22, steps per second:  76, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.494561, mae: 3.313426, mean_q: 6.152621, mean_eps: 0.400000\n",
            "  950/6000: episode: 74, duration: 0.307s, episode steps:  24, steps per second:  78, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 0.451767, mae: 3.381091, mean_q: 6.274972, mean_eps: 0.400000\n",
            "  961/6000: episode: 75, duration: 0.192s, episode steps:  11, steps per second:  57, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.459489, mae: 3.422894, mean_q: 6.324419, mean_eps: 0.400000\n",
            "  971/6000: episode: 76, duration: 0.165s, episode steps:  10, steps per second:  61, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.417647, mae: 3.487640, mean_q: 6.577616, mean_eps: 0.400000\n",
            "  980/6000: episode: 77, duration: 0.150s, episode steps:   9, steps per second:  60, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.421533, mae: 3.542063, mean_q: 6.789065, mean_eps: 0.400000\n",
            "  998/6000: episode: 78, duration: 0.255s, episode steps:  18, steps per second:  71, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.563842, mae: 3.550955, mean_q: 6.657996, mean_eps: 0.400000\n",
            " 1016/6000: episode: 79, duration: 0.243s, episode steps:  18, steps per second:  74, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.729905, mae: 3.622492, mean_q: 6.730309, mean_eps: 0.400000\n",
            " 1032/6000: episode: 80, duration: 0.215s, episode steps:  16, steps per second:  75, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.832266, mae: 3.683129, mean_q: 6.953799, mean_eps: 0.400000\n",
            " 1053/6000: episode: 81, duration: 0.290s, episode steps:  21, steps per second:  72, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.969711, mae: 3.740041, mean_q: 6.896905, mean_eps: 0.400000\n",
            " 1073/6000: episode: 82, duration: 0.266s, episode steps:  20, steps per second:  75, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.790175, mae: 3.692965, mean_q: 6.725434, mean_eps: 0.400000\n",
            " 1100/6000: episode: 83, duration: 0.357s, episode steps:  27, steps per second:  76, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 0.586564, mae: 3.753281, mean_q: 7.023548, mean_eps: 0.400000\n",
            " 1120/6000: episode: 84, duration: 0.264s, episode steps:  20, steps per second:  76, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.617256, mae: 3.820474, mean_q: 7.171104, mean_eps: 0.400000\n",
            " 1137/6000: episode: 85, duration: 0.232s, episode steps:  17, steps per second:  73, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.759468, mae: 3.865582, mean_q: 7.222863, mean_eps: 0.400000\n",
            " 1165/6000: episode: 86, duration: 0.340s, episode steps:  28, steps per second:  82, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 0.808268, mae: 3.903228, mean_q: 7.248208, mean_eps: 0.400000\n",
            " 1185/6000: episode: 87, duration: 0.262s, episode steps:  20, steps per second:  76, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.548228, mae: 3.987283, mean_q: 7.557735, mean_eps: 0.400000\n",
            " 1202/6000: episode: 88, duration: 0.258s, episode steps:  17, steps per second:  66, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.730813, mae: 4.054797, mean_q: 7.630784, mean_eps: 0.400000\n",
            " 1259/6000: episode: 89, duration: 0.974s, episode steps:  57, steps per second:  59, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.745452, mae: 4.172874, mean_q: 7.884396, mean_eps: 0.400000\n",
            " 1321/6000: episode: 90, duration: 0.920s, episode steps:  62, steps per second:  67, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 0.766447, mae: 4.340093, mean_q: 8.240385, mean_eps: 0.400000\n",
            " 1351/6000: episode: 91, duration: 0.405s, episode steps:  30, steps per second:  74, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.643040, mae: 4.424036, mean_q: 8.335555, mean_eps: 0.400000\n",
            " 1408/6000: episode: 92, duration: 0.732s, episode steps:  57, steps per second:  78, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 0.900883, mae: 4.597038, mean_q: 8.758931, mean_eps: 0.400000\n",
            " 1420/6000: episode: 93, duration: 0.192s, episode steps:  12, steps per second:  62, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 1.118456, mae: 4.766830, mean_q: 8.984122, mean_eps: 0.400000\n",
            " 1436/6000: episode: 94, duration: 0.220s, episode steps:  16, steps per second:  73, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 1.021937, mae: 4.767283, mean_q: 8.944705, mean_eps: 0.400000\n",
            " 1459/6000: episode: 95, duration: 0.298s, episode steps:  23, steps per second:  77, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 1.003402, mae: 4.742856, mean_q: 8.909704, mean_eps: 0.400000\n",
            " 1551/6000: episode: 96, duration: 1.275s, episode steps:  92, steps per second:  72, episode reward: 92.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.587 [0.000, 1.000],  loss: 1.162083, mae: 4.976660, mean_q: 9.458866, mean_eps: 0.400000\n",
            " 1588/6000: episode: 97, duration: 0.489s, episode steps:  37, steps per second:  76, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 1.133023, mae: 5.151796, mean_q: 9.807909, mean_eps: 0.400000\n",
            " 1625/6000: episode: 98, duration: 0.477s, episode steps:  37, steps per second:  78, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 1.353271, mae: 5.239645, mean_q: 9.855505, mean_eps: 0.400000\n",
            " 1676/6000: episode: 99, duration: 0.674s, episode steps:  51, steps per second:  76, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1.479962, mae: 5.397420, mean_q: 10.235918, mean_eps: 0.400000\n",
            " 1725/6000: episode: 100, duration: 0.660s, episode steps:  49, steps per second:  74, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 1.258827, mae: 5.471532, mean_q: 10.467968, mean_eps: 0.400000\n",
            " 1761/6000: episode: 101, duration: 0.459s, episode steps:  36, steps per second:  78, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.786130, mae: 5.686866, mean_q: 10.759601, mean_eps: 0.400000\n",
            " 1828/6000: episode: 102, duration: 0.852s, episode steps:  67, steps per second:  79, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 1.225605, mae: 5.771983, mean_q: 11.085560, mean_eps: 0.400000\n",
            " 1861/6000: episode: 103, duration: 0.433s, episode steps:  33, steps per second:  76, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 1.735097, mae: 5.996906, mean_q: 11.444379, mean_eps: 0.400000\n",
            " 1888/6000: episode: 104, duration: 0.385s, episode steps:  27, steps per second:  70, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 1.430854, mae: 6.104229, mean_q: 11.705260, mean_eps: 0.400000\n",
            " 2024/6000: episode: 105, duration: 1.701s, episode steps: 136, steps per second:  80, episode reward: 136.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.551 [0.000, 1.000],  loss: 1.847604, mae: 6.289780, mean_q: 11.989378, mean_eps: 0.400000\n",
            " 2056/6000: episode: 106, duration: 0.417s, episode steps:  32, steps per second:  77, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.714935, mae: 6.539141, mean_q: 12.558410, mean_eps: 0.400000\n",
            " 2137/6000: episode: 107, duration: 1.018s, episode steps:  81, steps per second:  80, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.580 [0.000, 1.000],  loss: 2.004975, mae: 6.660520, mean_q: 12.745681, mean_eps: 0.400000\n",
            " 2202/6000: episode: 108, duration: 0.820s, episode steps:  65, steps per second:  79, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 1.671811, mae: 6.843594, mean_q: 13.207827, mean_eps: 0.400000\n",
            " 2240/6000: episode: 109, duration: 0.464s, episode steps:  38, steps per second:  82, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2.286798, mae: 7.087391, mean_q: 13.672176, mean_eps: 0.400000\n",
            " 2266/6000: episode: 110, duration: 0.327s, episode steps:  26, steps per second:  80, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 2.328678, mae: 7.169807, mean_q: 13.822469, mean_eps: 0.400000\n",
            " 2290/6000: episode: 111, duration: 0.319s, episode steps:  24, steps per second:  75, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.022837, mae: 7.234503, mean_q: 13.981733, mean_eps: 0.400000\n",
            " 2317/6000: episode: 112, duration: 0.392s, episode steps:  27, steps per second:  69, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 3.000083, mae: 7.353269, mean_q: 14.109879, mean_eps: 0.400000\n",
            " 2352/6000: episode: 113, duration: 0.450s, episode steps:  35, steps per second:  78, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 2.772330, mae: 7.488557, mean_q: 14.336949, mean_eps: 0.400000\n",
            " 2364/6000: episode: 114, duration: 0.154s, episode steps:  12, steps per second:  78, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 3.591885, mae: 7.663794, mean_q: 14.594577, mean_eps: 0.400000\n",
            " 2387/6000: episode: 115, duration: 0.287s, episode steps:  23, steps per second:  80, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 1.396197, mae: 7.511379, mean_q: 14.629242, mean_eps: 0.400000\n",
            " 2405/6000: episode: 116, duration: 0.234s, episode steps:  18, steps per second:  77, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.079017, mae: 7.789924, mean_q: 14.895671, mean_eps: 0.400000\n",
            " 2448/6000: episode: 117, duration: 0.527s, episode steps:  43, steps per second:  82, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 3.580532, mae: 7.761927, mean_q: 14.690242, mean_eps: 0.400000\n",
            " 2466/6000: episode: 118, duration: 0.237s, episode steps:  18, steps per second:  76, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 2.618007, mae: 7.824293, mean_q: 14.931237, mean_eps: 0.400000\n",
            " 2495/6000: episode: 119, duration: 0.400s, episode steps:  29, steps per second:  72, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 3.643216, mae: 8.040289, mean_q: 15.400388, mean_eps: 0.400000\n",
            " 2515/6000: episode: 120, duration: 0.260s, episode steps:  20, steps per second:  77, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 3.287565, mae: 7.898494, mean_q: 15.131817, mean_eps: 0.400000\n",
            " 2537/6000: episode: 121, duration: 0.283s, episode steps:  22, steps per second:  78, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.691531, mae: 8.068831, mean_q: 15.318063, mean_eps: 0.400000\n",
            " 2587/6000: episode: 122, duration: 0.616s, episode steps:  50, steps per second:  81, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.596015, mae: 8.215118, mean_q: 15.597429, mean_eps: 0.400000\n",
            " 2615/6000: episode: 123, duration: 0.366s, episode steps:  28, steps per second:  76, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 4.303933, mae: 8.283934, mean_q: 15.614947, mean_eps: 0.400000\n",
            " 2640/6000: episode: 124, duration: 0.329s, episode steps:  25, steps per second:  76, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 2.760727, mae: 8.129723, mean_q: 15.646017, mean_eps: 0.400000\n",
            " 2669/6000: episode: 125, duration: 0.378s, episode steps:  29, steps per second:  77, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 3.850778, mae: 8.347849, mean_q: 15.930307, mean_eps: 0.400000\n",
            " 2693/6000: episode: 126, duration: 0.311s, episode steps:  24, steps per second:  77, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.731588, mae: 8.297945, mean_q: 15.887157, mean_eps: 0.400000\n",
            " 2723/6000: episode: 127, duration: 0.392s, episode steps:  30, steps per second:  77, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 3.066193, mae: 8.354057, mean_q: 16.032506, mean_eps: 0.400000\n",
            " 2772/6000: episode: 128, duration: 0.657s, episode steps:  49, steps per second:  75, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 3.959333, mae: 8.536011, mean_q: 16.318911, mean_eps: 0.400000\n",
            " 2823/6000: episode: 129, duration: 0.787s, episode steps:  51, steps per second:  65, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 3.898440, mae: 8.563020, mean_q: 16.252421, mean_eps: 0.400000\n",
            " 2909/6000: episode: 130, duration: 1.130s, episode steps:  86, steps per second:  76, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 3.964005, mae: 8.742117, mean_q: 16.625997, mean_eps: 0.400000\n",
            " 2936/6000: episode: 131, duration: 0.335s, episode steps:  27, steps per second:  81, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 3.879886, mae: 8.847274, mean_q: 16.918955, mean_eps: 0.400000\n",
            " 2984/6000: episode: 132, duration: 0.606s, episode steps:  48, steps per second:  79, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.636482, mae: 8.883503, mean_q: 17.012640, mean_eps: 0.400000\n",
            " 3014/6000: episode: 133, duration: 0.404s, episode steps:  30, steps per second:  74, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.567 [0.000, 1.000],  loss: 3.524062, mae: 8.936691, mean_q: 17.217269, mean_eps: 0.400000\n",
            " 3062/6000: episode: 134, duration: 0.598s, episode steps:  48, steps per second:  80, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 3.632630, mae: 8.989220, mean_q: 17.237269, mean_eps: 0.400000\n",
            " 3130/6000: episode: 135, duration: 0.903s, episode steps:  68, steps per second:  75, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 4.040567, mae: 9.158105, mean_q: 17.499998, mean_eps: 0.400000\n",
            " 3153/6000: episode: 136, duration: 0.302s, episode steps:  23, steps per second:  76, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 4.219835, mae: 9.298366, mean_q: 17.860521, mean_eps: 0.400000\n",
            " 3197/6000: episode: 137, duration: 0.547s, episode steps:  44, steps per second:  80, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 4.076176, mae: 9.371994, mean_q: 17.893799, mean_eps: 0.400000\n",
            " 3256/6000: episode: 138, duration: 0.759s, episode steps:  59, steps per second:  78, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 3.928900, mae: 9.441513, mean_q: 18.173445, mean_eps: 0.400000\n",
            " 3282/6000: episode: 139, duration: 0.357s, episode steps:  26, steps per second:  73, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.798765, mae: 9.551932, mean_q: 18.522145, mean_eps: 0.400000\n",
            " 3309/6000: episode: 140, duration: 0.365s, episode steps:  27, steps per second:  74, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 4.427450, mae: 9.607851, mean_q: 18.428083, mean_eps: 0.400000\n",
            " 3339/6000: episode: 141, duration: 0.424s, episode steps:  30, steps per second:  71, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 5.128413, mae: 9.731434, mean_q: 18.591006, mean_eps: 0.400000\n",
            " 3375/6000: episode: 142, duration: 0.502s, episode steps:  36, steps per second:  72, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 3.831246, mae: 9.775960, mean_q: 18.875338, mean_eps: 0.400000\n",
            " 3475/6000: episode: 143, duration: 1.294s, episode steps: 100, steps per second:  77, episode reward: 100.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 4.186700, mae: 9.858502, mean_q: 18.941780, mean_eps: 0.400000\n",
            " 3565/6000: episode: 144, duration: 1.310s, episode steps:  90, steps per second:  69, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 4.546340, mae: 10.039258, mean_q: 19.266962, mean_eps: 0.400000\n",
            " 3622/6000: episode: 145, duration: 0.952s, episode steps:  57, steps per second:  60, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 4.136419, mae: 10.179852, mean_q: 19.588306, mean_eps: 0.400000\n",
            " 3722/6000: episode: 146, duration: 1.263s, episode steps: 100, steps per second:  79, episode reward: 100.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.430 [0.000, 1.000],  loss: 4.582667, mae: 10.332927, mean_q: 19.825937, mean_eps: 0.400000\n",
            " 3772/6000: episode: 147, duration: 0.717s, episode steps:  50, steps per second:  70, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.211310, mae: 10.437871, mean_q: 20.099614, mean_eps: 0.400000\n",
            " 3841/6000: episode: 148, duration: 0.917s, episode steps:  69, steps per second:  75, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 5.054601, mae: 10.564609, mean_q: 20.287988, mean_eps: 0.400000\n",
            " 3964/6000: episode: 149, duration: 1.774s, episode steps: 123, steps per second:  69, episode reward: 123.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 4.208289, mae: 10.617535, mean_q: 20.453510, mean_eps: 0.400000\n",
            " 4100/6000: episode: 150, duration: 1.801s, episode steps: 136, steps per second:  76, episode reward: 136.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  loss: 5.074364, mae: 10.935573, mean_q: 21.065096, mean_eps: 0.400000\n",
            " 4237/6000: episode: 151, duration: 1.890s, episode steps: 137, steps per second:  72, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 4.821073, mae: 11.182786, mean_q: 21.636939, mean_eps: 0.400000\n",
            " 4317/6000: episode: 152, duration: 1.030s, episode steps:  80, steps per second:  78, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 4.674848, mae: 11.409573, mean_q: 22.059374, mean_eps: 0.400000\n",
            " 4363/6000: episode: 153, duration: 0.587s, episode steps:  46, steps per second:  78, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.501965, mae: 11.535258, mean_q: 22.103432, mean_eps: 0.400000\n",
            " 4403/6000: episode: 154, duration: 0.513s, episode steps:  40, steps per second:  78, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.829227, mae: 11.626888, mean_q: 22.542271, mean_eps: 0.400000\n",
            " 4504/6000: episode: 155, duration: 1.269s, episode steps: 101, steps per second:  80, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.446 [0.000, 1.000],  loss: 5.077370, mae: 11.751901, mean_q: 22.752674, mean_eps: 0.400000\n",
            " 4535/6000: episode: 156, duration: 0.379s, episode steps:  31, steps per second:  82, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 6.395244, mae: 11.851771, mean_q: 22.779369, mean_eps: 0.400000\n",
            " 4614/6000: episode: 157, duration: 0.963s, episode steps:  79, steps per second:  82, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 5.199113, mae: 11.943589, mean_q: 23.098380, mean_eps: 0.400000\n",
            " 4705/6000: episode: 158, duration: 1.142s, episode steps:  91, steps per second:  80, episode reward: 91.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 4.749436, mae: 12.085108, mean_q: 23.496350, mean_eps: 0.400000\n",
            " 4804/6000: episode: 159, duration: 1.224s, episode steps:  99, steps per second:  81, episode reward: 99.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 5.083376, mae: 12.271316, mean_q: 23.815996, mean_eps: 0.400000\n",
            " 4894/6000: episode: 160, duration: 1.075s, episode steps:  90, steps per second:  84, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 6.102672, mae: 12.486262, mean_q: 24.065171, mean_eps: 0.400000\n",
            " 4992/6000: episode: 161, duration: 1.276s, episode steps:  98, steps per second:  77, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 6.005233, mae: 12.616480, mean_q: 24.390361, mean_eps: 0.400000\n",
            " 5055/6000: episode: 162, duration: 0.780s, episode steps:  63, steps per second:  81, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 4.978881, mae: 12.669959, mean_q: 24.560092, mean_eps: 0.400000\n",
            " 5105/6000: episode: 163, duration: 0.658s, episode steps:  50, steps per second:  76, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.213931, mae: 12.868866, mean_q: 24.960529, mean_eps: 0.400000\n",
            " 5142/6000: episode: 164, duration: 0.497s, episode steps:  37, steps per second:  74, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 6.084166, mae: 12.889363, mean_q: 24.937028, mean_eps: 0.400000\n",
            " 5203/6000: episode: 165, duration: 0.775s, episode steps:  61, steps per second:  79, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 4.707779, mae: 12.970757, mean_q: 25.284249, mean_eps: 0.400000\n",
            " 5289/6000: episode: 166, duration: 1.148s, episode steps:  86, steps per second:  75, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 5.288253, mae: 13.077608, mean_q: 25.445398, mean_eps: 0.400000\n",
            " 5418/6000: episode: 167, duration: 1.604s, episode steps: 129, steps per second:  80, episode reward: 129.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 5.107580, mae: 13.313087, mean_q: 25.986255, mean_eps: 0.400000\n",
            " 5462/6000: episode: 168, duration: 0.564s, episode steps:  44, steps per second:  78, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 5.811496, mae: 13.452907, mean_q: 26.232980, mean_eps: 0.400000\n",
            " 5510/6000: episode: 169, duration: 0.625s, episode steps:  48, steps per second:  77, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.160106, mae: 13.586834, mean_q: 26.269891, mean_eps: 0.400000\n",
            " 5670/6000: episode: 170, duration: 1.960s, episode steps: 160, steps per second:  82, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 6.062875, mae: 13.767610, mean_q: 26.822605, mean_eps: 0.400000\n",
            " 5720/6000: episode: 171, duration: 0.610s, episode steps:  50, steps per second:  82, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.487328, mae: 14.018967, mean_q: 27.285713, mean_eps: 0.400000\n",
            " 5759/6000: episode: 172, duration: 0.615s, episode steps:  39, steps per second:  63, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 7.983421, mae: 14.091172, mean_q: 27.212983, mean_eps: 0.400000\n",
            " 5821/6000: episode: 173, duration: 0.991s, episode steps:  62, steps per second:  63, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 8.107360, mae: 14.158396, mean_q: 27.347564, mean_eps: 0.400000\n",
            " 5852/6000: episode: 174, duration: 0.394s, episode steps:  31, steps per second:  79, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 5.115116, mae: 14.206811, mean_q: 27.674663, mean_eps: 0.400000\n",
            " 5912/6000: episode: 175, duration: 0.769s, episode steps:  60, steps per second:  78, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 6.444091, mae: 14.249352, mean_q: 27.783500, mean_eps: 0.400000\n",
            " 5955/6000: episode: 176, duration: 0.537s, episode steps:  43, steps per second:  80, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 4.598283, mae: 14.245070, mean_q: 27.955593, mean_eps: 0.400000\n",
            "done, took 88.552 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEGCAYAAABhMDI9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9d5gkV3X3/znVcfLM7szO5iDtSmiF8iIkJLIQWGBEcIAXsLBlgo1tbGzAYL8O/ACDeQ0vYGwsI0DkICMQLyIjFFFYrbSSdhV2tTlN2MnT06nq/v6outVVnWZ6dnp6Zud+nmefma7urr7dvXNPnfM9QZRSGAwGg8GgsRq9AIPBYDAsLIxhMBgMBkMIYxgMBoPBEMIYBoPBYDCEMIbBYDAYDCGijV7AqdLd3a02btzY6GUYDAbDouKhhx4aVEr1lLtv0RuGjRs3sn379kYvw2AwGBYVInKw0n0mlGQwGAyGEMYwGAwGgyGEMQwGg8FgCGEMg8FgMBhCGMNgMBgMhhB1NQwi8kUR6ReRx4uO/7mIPCkiu0TkXwPHPyAie0XkKRF5eT3XZjAYDIby1Dtd9cvAvwNf0QdE5MXAtcAFSqmMiKzwjm8F3gCcC6wGfiEiZyml7Dqv0WAwGAwB6uoxKKXuBIaKDv8J8DGlVMZ7TL93/FrgW0qpjFJqP7AXuLSe6zMYDIaFRP9Ymp/v7mv0MhqiMZwFPF9E7heRO0TkOd7xNcDhwOOOeMdKEJG3i8h2Edk+MDBQ5+UaDAbD/PCtBw/zjq9ux3EaOyenEYYhCiwDLgPeC3xHRKSWEyilblBKbVNKbevpKVvRbTAYDIuObN7BUWA3eIBaIwzDEeB7yuUBwAG6gaPAusDj1nrHDAaDYUmgDYK9BD2G7wMvBhCRs4A4MAjcCrxBRBIisgnYAjzQgPUZDAZDQ9AhJKfBHkNds5JE5JvAi4BuETkC/CPwReCLXgprFrhOuYOnd4nId4DdQB54l8lIMhgMSwntKTTaY6irYVBKvbHCXW+u8PiPAB+p34oMBoNh4aJDSY7T2HWYymeDwWBYIOhQ0lIUnw0Gg8FQhqUsPhsMBoOhDLajfxrDYDAYDAZMKMlgMBgMRRTEZ2MYDAaDwUDAYzCGwWAwGAwQEJ9NKMlgMBgMUPAUTCjJYDAYDEChFYbxGAwGg8EALJyWGMYwGAwGwwJB1zGYlhgGg8FgAEwoyWAwGAxFmFCSwWAwGEIYw2AwGAyGEMYwGAwGgyGE3xLDaAwGg8FggCXSEkNEvigi/d4Yz+L7/lpElIh0e7dFRD4jIntF5FERubieazMYDIaFxlJpifFl4BXFB0VkHXA1cChw+LeALd6/twP/Wee1GQwGw4LCWQotMZRSdwJDZe76FPA+IPjurwW+olzuAzpFZFU912cwGAwLiSU7wU1ErgWOKqV2Ft21BjgcuH3EO1buHG8Xke0isn1gYKBOKzUYDIb5xa98Ps1DSSFEpBn4IPAPp3IepdQNSqltSqltPT09c7M4g8FgaDAF8bmx64jO8+udCWwCdooIwFpgh4hcChwF1gUeu9Y7ZjAYDEuCpSI+h1BKPaaUWqGU2qiU2ogbLrpYKXUCuBX4Ay876TJgVCl1fD7XZzAYli7HR6d4YH85SXT+WBLis4h8E/gNcLaIHBGR66s8/DZgH7AX+G/gT+u5NoPBYND8bNcJXv6pO7n+pgcbug7tKeQbbBjqGkpSSr1xmvs3Bn5XwLvquR6D4XTmpnsP8I37D/HTv3pBo5eyqHj86Chv/+pDRC1p9FLMBDeDwTC3HDg5yYGTk41exqLjyRPjALz6gtXkHdXQTdkXn5eSxmAwGOqH46iGpzkuRvrG0gCs7WoCINfAKTlLto7BYDDUB1uphm8oi5G+sTQdTTHakjEAcnbjPsMlWcdgMBjqh+2Ao0AZr6Em+sbS9LYniEVcjSGXb5zH4BiPwWAwzCULpTPnYqNvLENve5JY1N0Ocw2sLjPzGAwGw5yyUIqjFhv9Y2nXMETc7TDbQMPg1zGYUJLBYJgLCsVRDV7IIsJxFP3jGXrbE8Qj2mNooMbgh5IatgTAGAaD4bShUBxlLMNMOTmZJe+okMewEEJJxmMwGAxzQt54DDWjU1VXtCV98Tk7T+Lz527fy8d+/GTo2EIRn+e7iZ7BYKgTC6U4ajHRP+4aht72BCNTOWD+PIZ79g4ykcmHjmmD0OiWGMZjMBhOExZKRstiom8sA8DKjuS8awyZvEM+8FpKKfRXZ1piGAyGOWGhhCEWE31jaUSguzUx7xpDJm+Hvqvg19Zor8+EkgyG0wTbhJJqpm8sw/IW1yj4GsN8GYacExKZQ0bCaAwGg2EusBdIGGIxoauegYLHME/icybvEGzoWslINAITSjIYThNM5XPt9HnFbQDx6HxrDHZIZA7+3mivzxgGg+E0YaFktCwm3HYYRR7DvGkMTsiIL6RQkjEMBsNpwkIpjlos2I7i5GSGnjbXY5hvjSGbd0JG3FkqHoOIfFFE+kXk8cCxT4jIkyLyqIjcIiKdgfs+ICJ7ReQpEXl5PddmMJxuLJRe/ouFTN5GKWiJRwAC6aoN8hhCGsO8LKEi9fYYvgy8oujYz4FnK6XOB54GPgAgIluBNwDnes/5DxGJ1Hl9BsNpg6ljqI1c3v2ctLYwn+Jz3naNQj5gAZylEkpSSt0JDBUd+5lSSpf73Qes9X6/FviWUiqjlNoP7AUuref6DIbTCVPHUBsZ2wYKBiE2j+JzxjM+FT2G0zmUNAP+CPix9/sa4HDgviPeMYPBMANMHUNtaAOgQ0jzqTFow5CvID432rg3zDCIyN8BeeDrs3ju20Vku4hsHxgYmPvFGQyLEF98Nh7DjNAho1jUNQgxa/40hkze9VbCmUiF+5ekYRCRtwKvAt6kCnMIjwLrAg9b6x0rQSl1g1Jqm1JqW09PT13XajAsFhy/7bYxDDNBewbxiCtlWpYQtWR+DEOu4DGoMgOWGu31zbthEJFXAO8DXq2USgXuuhV4g4gkRGQTsAV4YL7XZzAsVozHUBu6vbYOIbm/W/OqMUChR9JCqmOoa0sMEfkm8CKgW0SOAP+Im4WUAH4uIgD3KaXeqZTaJSLfAXbjhpjepZSy67k+g+F0wt9gjMYwI7RnoEVngGhE5mUegw4lgTtYKWJFFlRLjLoaBqXUG8scvrHK4z8CfKR+KzIYTl/05LZGbyqLBe0ZJCIFwxCPWPOkMRReo1yacaOLFGccShKRd4tIu7jcKCI7ROTqei7OYDDMHC1eGsMwM/xQUsBjiEWs0IyEer82FDShxZqV9EdKqTHgaqALeAvwsbqsymAw1IwpcKsNP5QU8Bhi0XkSnwOhJNsubWUyT338KlKLYdAKzTXAV5VSuwLHDAZDg9HaQqPDEIuFrF1efJ6XOoZcdY+h0eJzLYbhIRH5Ga5h+KmItAFm7LjBsEAotN1u8EIWCTqck4guDI1hsYrP1wMXAvuUUikRWQ78YX2WZTAYasX26xiMZZgJZUNJ85auGs5KgrBBb3Rm2YwNg1LKEZGNwJtFRAF3K6VuqdfCDAZDbZi227VR3jDMl8ZQOSspYknDPYZaspL+A3gn8BjwOPAOEflcvRZmMBhqw4SSakOHkuJFWUnzUsdQRmPQBj0WabxhqCWU9BLgHN3CQkRuwi1GMxgMC4C8qXyuiaytN+KAxhC1mMzkKz1lzghlJRV5DLGI1XCvrxbxeS+wPnB7HbBnbpdjMBhmi+mVVBs5v1dSIzSGgMdgh3slxSPWovIY2oAnROQBQOHOStguIrcCKKVeXYf1GQyGGWLabtdGrmwoqXEagxPwGBaTYfiHuq3CYDCcEkopv1eSCSXNjKztYIkr9mrmr46hXFaSZxii0vBQUi1ZSXeIyAZgi1LqFyLSBESVUuP1W57BYJgJQVtgQkkzI2s7IX0B5q+OIWh8irPJFoLHUEtW0tuAm4H/8g6tBb5fj0UZDIbaWEhVs4uFXF6F9AXwNIb8PGgMZSuf3dvxiEWjv8JaxOd3AVcAYwBKqT3AinosymAw1IazgIa8LBayth3SF2A+eyWVqWPQ4nN0EXkMQEYpldU3RCSKK0IbDIYGs5A6cy4WcnlVEkqaN40hVPm88MTnWgzDHSLyQaBJRF4GfBf4YX2WZTAYaqHSUHlDZXK248971sxnr6SoJ3rbTmHMJ7iZUY0Wn2sxDH8LDOBWPr8DuE0p9Xd1WZXBYKgJxxiGmsnaTnmNYT7qGHIOzXF31rSuYwh6DI1OIKglXfXPlVKfBv5bHxCRd3vHDAZDAwnqCo2+2lwsZPOlWUk6jGM7KpTGOtdk8jYtiShj6XypxhCxGp5AUIvHcF2ZY2+t9gQR+aKI9IvI44Fjy0Tk5yKyx/vZ5R0XEfmMiOwVkUdF5OIa1mYwLGmMx1A7OdspKz7r+4JMZvL0jaXn7LUzeYcm7TGUaYnR6ASCaQ2DiLxRRH4IbBKRWwP/fg0MTfP0LwOvKDr2t8AvlVJbgF96twF+C9ji/Xs78J8zfhcGwxLHXkC9/BcLObs0XVXfLjYM/377Xn7/v34zZ6+dyTu0xN2ATUkdwyLJSroX+DfgSe+n/vce4OXVnqiUupNS43EtcJP3+03AawLHv6Jc7gM6RWTVTN6EwbDUMVlJBX79VD+v+dw9oc39xGiaqz55B0eGU/6xSqEkoERnGJrIMjSZZa7I5OyCxlDiMcjCDyUppQ4qpX4NXAXcpZS6AziOW+A2myBcr1LquPf7CaDX+30NcDjwuCPesRJE5O0isl1Etg8MDMxiCQbD6YWzgIa8NJpdx8Z45PAIR4an/GPPDEywt9/9p8naDrHiUFIFjyFnOxUF4fv2neQ7Dx4ue18lMnmHloT2GMItMeKLIZQU4E4gKSJrgJ8Bb8ENFc0ar4V3zZ+AUuoGpdQ2pdS2np6eU1mCwXBaEBKfl7jHoDf1Q0MF70DXDQRnLeRsh3gkfG2r5z8Xz2TI2o6fPVTMf93xDP/fj3bjTSTgkcMj5KdJec3mnRKPIdgSo9FD+GoxDKKUSgGvA/5DKfW7wLmzeM0+HSLyfvZ7x4/itvLWrPWOGQyGaQiGjxqd6tho9AYeMgxeC4pgiCibLyM+V/AY8rYiV2G3frpvgvF0npOTWfb2j/Oaz93DL57oL/tYfz1lNAa/JUZ0cXkMIiKXA28CfuQdi8ziNW+lkOF0HfCDwPE/8LKTLgNGAyEng8FQhVCvpCUeStKb+uGQx+Aey9p26HEz1RhytoNSpd7YZCbP0RE3ZHVgcJJdx8YAGElV1iMcR5G1HZoTRXUMC6iJXi11DO8GPgDcopTaJSJnALdXe4KIfBN4EdAtIkeAfwQ+BnxHRK4HDgK/5z38NuAa3IFAKeAPa1ibwbCkMeJzAb2pHzo5XSipXEuM8umqOe8zzTkOCatwPRzULPYNTvrGKB1oq12MbrlR6jEUeiWBa0CsOtZSVKOWttt34uoM+vY+4C/0bRH5rFLqz4ue88YKp3tpmfMr3EZ9BoOhRoJewpIPJTnlNAbPYwgYhmzZOgbLvy+IHuqTtxWJwK65J2AY9g9Osm/AvT2VqywS6LCW7zEUZyXpVhlKYc0qv+fUqSWUNB1XzOG5DAZDDZi22wWCoSQtCOvNONjVNJsvbYnh1zEUic/a2BQL0Hv6xolHLDYub2b/wKRvKKaqeAzaeyl4DO65HaWwBN9LaKTnN5eGwWAwNIhQgdvStgt+KGk8k2cklQMKm3FQO3A1huKsJHdLLPa6snYhlBRkT/8EZ/S0sHlFK0/3jXPQC19lqhoG9xzlKp8jlvitOIxhMBgMp4RjPAafYKqoDieVCyWVF5+9dNUqoaQgT/eNs6W3jU3dLewbnPQ385l4DMlYBEsK57SVwhIhIoVQUqOYS8PQmGCYwXCaoJTiloePzKrtczhdtcFJ8A0mZ7shGShjGLysJMdRbkuMSumqFUJJwe8mlc1zZHiKs1a0sqm7NfT4qWxlw5D2wlqJqEXUskLzGCKW+KGkRhr4mg2DiDRXuMt0WTUYToHdx8f4q2/v5O49gzU/N9wraS5XtfjI2Q5ru9xtyjcMuXBWkg4Jlcx8jlZKVw0LxFDISNrS28rGbvf1LIE1nU3TeAwFwxCxJFD5DBERdHRrUYSSROR5IrIbt2cSInKBiPyHvl8p9eW5X57BsHTQKY7VNpVKmDqGAnlH0d4Upbs17qeP6tCQbxjsQvuJINVaYrjnLhx/xstA2ryijTM8j2Hj8hY6mmK+V1COrG8YIkQtCVU+W5YQ8dawWEJJn8JtmncSQCm1E3hBPRZlMCxF9GZV3I5hJpg6hgJaO1i3rDngMehQkicie59xaSipgsZgl1ZOHxtx23Cv7Wqitz1BczzC5hWtJGNW1ToGrTEkYhaRiITqGCJWQWNoZESwplCSUqq4U1TtlzYGg6EsWoSczcxh7SW4oQljGGKWxfKWOKNTOisp7DHoz7gklFSlJUbwJ8Dx0Sm6mmMkYxFEhH9+9bm844Vn0hSPzDiUFPQYfPHZW1IjPYZaKp8Pi8jzACUiMdxK6CfqsyyDYemRKyNwzhS/z84CaKfQaPKeqJyMRXwR2K98LgopVUpXLRaf9fOC6arHR9Ks6mjyb//uNrfVW1Ms4qfJliMTCCVFLMG2g+IzWLK4xOd34lYmr8FtbnchplLZYJgz8kVhjloItlNodAO2RpNz3FYXzYEr94LHoOsZKoSSKorPpemqx0fTrOpIlrx+MjaNx+DdV5yVZDvKFZ8XQB1DLS0xBnEb6BkMhjqQLxPHninhls1L3DDk3cK1psAG7WsMRaGkUvG5vMZQCCUFhv+MpblofWfJ6ydjEdJV0lUrZiVp8dlqfB3DtIZBRD5LlZkJSqm/qHSfwWCYObpR22w0hsKQF1kUvZKUUtxw5z5edcFq1nQ2Tf+EGsg7DlHLIhmPkCoKJeV8r6xgSIPErFKNQSnlf6b6ZzpnMzSZZXWZtTdN5zFUykrSdQzSeI9hJqGk7cBDQBK4GNjj/bsQiNdvaQbD0iJfFP+uBe0xxKPWokhXHZrM8i8/fpIfPzb3nfXztiIaEZpjUbJ5B9tRlcXnolCSZQlRS0KGIejB6XTVE6NuRtLK9tJQUlM8UjVdVWcsJeNWKFnAViyeUJJS6iYAEfkT4EqlVN67/Xngrvouz2BYOvgawyw8Bv3chdDLfyboK+rZhM2mI2u7zfGa4u6mn87ZvmHIFBnf4lASuJ9hcU+lwu/u8WOj7gyGahqDUgqR0oYQ4+k88Yjli89Bj8FaRB6DpgtoD9xu9Y4ZDIY54JSykhbQkJeZoK+apxuBORu0x9AUc5vUTeXs0spnX3wu3bhjEQl5bUHBWf+uPYZVZUJJyZi7rWYqeH4TmRytSfeaPBqoY8g7TshjaKTnV0u66seAh0Xkdty+SC8A/qkeizIYliJ6g5iV+Kx7+UctUpn8nK6rHkxlZ28EpyPvOEQjFk1eW+uprF0xK6lYYwA3HBdcV1Dz0aGk49VCSZ5BSudskrHSIZcT6Tyt3lCHSCgrqRDKcm8vAsOglPqSiPwYeC6uGP1+pdSJuq3MYFhi5E6hwE17DImIxfgi0BjSfl1BHUJJ3pyFkMdQ1AqjUroq6FBSuAtr4Xf3+cdHp+hsjvmts4MEX7c0ZwkmMgXDEA1kJTnKq2NYAB5DrU30LgWej+stPOdUXlhE/kpEdonI4yLyTRFJisgmEblfRPaKyLdFxIjbhiXDKYnPvscg/u9KKX9QzUJDF57Vx2NQRC3xNQbXYwiHkjL5yh5DscYQDiUVxOdgcVsQ7SVU6rA6ns77oaSIJYW227qOwdcYZvJu60MtTfQ+hlvtvNv79xci8tHZvKiIrMEdC7pNKfVsIAK8Afg48Cml1GZgGLh+Nuc3GBYjeSd8NVsLhXRVNzSRztls+/Av+Omuvjld41wxVXeNwfI36MlMvsQbq9REDzyNoWIoSXsM5YvbIGAYKqSsTmTytIU8hnATPUu3xFgk4vM1wMuUUl9USn0ReAXwqlN47SjQJCJRoBk4DrwEuNm7/ybgNadwfoNhUVFo1DabUJL7Uxe4jaVznJzMsvv42Fwucc7Q4vNch5KUUl5WktDsaQy6XxKUE5/Lewwh8TnQBkMbsuOjaVZWMAw6vFQpZXUyU+QxFFc+y+ILJQVDZh2zfVGl1FHg/wCHcA3CKG6txIhOhwWO4LbfKEFE3i4i20Vk+8DAwGyXYTAsKPwmevnZi8+6JYbe2IYmM3O3wDkknatPKElfZUcDGsOIZxjiUatMr6TSLTARi4QnvQW+D+2NDU1mWV3JMATE53JMZPK0lPEYbKeo8nmReAz/gpuV9GURuQl3I//IbF5URLqAa4FNwGqgBdcDmRFKqRuUUtuUUtt6enpmswSDYcExF+mqbhM9AoYhO3cLnEPqpTHkfcNQSFcdTrmfQXvSLXhTSgWykkrTVZtiVigMFGycl7OV/5l2tybKrkGnq1bTGNrKZCU5yvUYrMXQEkOjlPqmiPyaguh8KllJVwH7lVIDACLyPeAKoFNEop7XsBa3WZ/BsCQoeAyz1xjcOgbHvzIenFighiFXn3TVYA8kHdIZ9TqdtiVjDE5kydmqYtttcK/4TwYMai4fDiX5LS1i5a+rfY8hX2oYsnn3+eWykmxHkYgGxOc6ZGzNlFrE5yuAMaXUrbiFbu8TkQ2zfN1DwGUi0ixuaeBLcQXt24Hf8R5zHfCDWZ7fYFh05E9FYwhkJdmOWvAeQ7pOlc/auLpZSV4oyTcM7mactR0/PFROfG6KR0JX+6Eq6MBnG4+UpqpC9aykSa/GxNcYIsF5DCyYJnq1hJL+E0iJyAXAe4BngK/M5kWVUvfjisw7gMe8ddwAvB94j4jsBZYDN87m/AbDYiQ3J1lJERy18ENJ9dIYtHENawzuZ6Cv0rN5h6xtE7UKYZsgxW2zc0Xis28YygjXEBSfSw3DhDYM5bKSvDTbhTCPoZbK57xSSonItcDnlFI3isis00mVUv8I/GPR4X24tRIGw5LDr2OYZdttETe2nncKm9dwKuuPjFxITNXJMGQD2kHEEuJRi+EijyFnO+RsVTaMBG4oKLipB0NJtqPI2oV5CuWolq46ns6H1lJcx2DJ4vMYxkXkA8CbgR+JiAXE6rMsg2HpcSpN9IJDXhyn0CxOqYL4upDwxedZZGBVI9hMENxNPqgxgOcx5J3KV/yxcCgp2MY8Zxc6tVZ6fjKqm/eVfo8Fj8FdS3EdQ8Ri0WUl/T6QAa73ROe1wCfqsiqDYQniz2OYjfish7yIhNJVYWGGk9K6nmCOJ97rmoNowDAUh5IyeVecr+gxxAvdUSFsqIPeWCXDEI1YxCNWWY+hRGMonuBmLbImep4x+GTg9iFmqTEYDIZSTkV8dpxCqmNQfAY4OZGF3jlb5pxQr3RVLRTHvM21OR7hyIjbIrs9WdAYcnm3CK4cyZin09gOiWikqAV3wGOoYFjAzVgqJz6Pl9UYAhPcFktLDBG52/s5LiJjxT/rv0SDYWlwai0x3BCE3lSCMfIF6THk6hNKKu6amgwUq7UGs5Jsp2RIj8ZPNy3qACsSFp8raQz6HGXF53TYMBTPY4gEWmIsaPFZKXWl97Ot/ssxGJYup9JEz3YcLE98hrBhOLkAq5/rlZWkr+715xDsfqo1hpzt+MN8yqGfM5Wz6SDmr7EpFiEfTFetZhjiFQxDxtU7/HkMoQluKjzBbTGEkgBE5GLgSty223crpR6uy6oMhiVIwWOofUOwVXhecDC+fXIBFrn5WUlzrTEUeQxNsaBhCKSr5qtnJYXW6H0fzfGIb1RgGsNQYe7zRDqPCDR7rxGsY3C8eQyRxTTBTUT+Abex3XKgG/iyiPx9vRZmMCw1TqmJng4leX/RKS++HYvIggwlTdUtlFSUlVTGY8jmq4eSigvU9PeRjEVC+k11jSHiV3cHGc/kaY1H/fqJ4l5JwZYYi0J8Bt4EXKCUSoPfhvsR4MP1WJjBsNTQqZZ5R/nzf2eKH58u8hh625ML0jCk6yU++1lJXiipjMeQ8QxDYgahJCh4IU0xV4ieUSgpZlXUGHQYCdysJNtx52YEM8vc110EHgNwDAi2E0xgehkZDHNGLhA6qHWKm45P67GQeuNd2Z5cmBqDHrU555XPOiupNJTkVz7bDumcXbWOAUpbgzfFI2666gxDSZUqn/U6gNAYT9e4L74JbqPALq+76peAx4EREfmMiHymPsszGJYOwaE1tV5Jaw9DC5fpnEMsInS3JhamxpDVV+OVN7/79p3kwg/9LDRPYTr8rKRoWHyORyw/iyiXdxidytHRXL4+t6kolJS33c8y6lUpzyRdNVlUJKeZyBR7DJ534KhS8XkhZyUFuMX7p/n13C7FYFja5Ivy5avxrQcOEY1Y/M4la4GA+OxtKqmcTTxisaw1zoMHFpZhUErNqCXGE8fHGEnlGJzI0NE0syYL+nxRK6wxJKKWf4WftV3D0FnhnP5I0MAao5ZF1JsFraumRSqH+iqKz1U8Bj2PIbqYspKUUjeJSBOwXin1VB3XZDAsScJ9/6t7DN984BCJWKRgGIqmf01l3VDJ8pY4w6lszZpFPckE6gAyeafi2rSnkKkwCa0chZYYYY0hEbP8K/xs3mEklaOzgsdQ3OvI7askxCJCOucahkr6hH+OeKR8S4x0PjQSNOgx+EWKC6CJXi1ZSb+NKzb/xLt9oYjcWq+FGQxLjaDHMF0tQyprh2LYxdO/dAx9eUscRxWmmC0E9Lr9uoIKKavaMNSiQxQXuDX7HkPE9xiGJrPkHUVnU7zsOYo1hpzteghRy3IL3OzK+kTwHJU0hpZ4BY9BhVtiLOjK5wD/hNv5dARAKfUIcEYd1mQwLEnygaKr6TbDVNYOxbCDTfTc+/PEoxbLvCljC2nEp74S1y0qKukMBY+h/CS0cmgBX2cl6av/YCipf9z9LCpqDPHSdNWoZRHzag4yucoN+DTJWPleSSVZSd73nXccv45BO0+LpbtqTik1WnSsgTbNYDi9yDnK35SmCyVN5ezQxuOosMcwlXONjB4hqds9LwT0htvWVKhELsfYLDwGv8CtKCspHrX8YwPaMFTQGJLR4nRVRcZGoygAACAASURBVCwqfovsrD29YWjyah6C781xFBPZwlhPKOMxiCDiGodFEUrCzUj6X0BERLaIyGeBe+u0LoNhyZG3HT/0MV3hVyqbLwklBYe8uKGkiH++VIX5w41Ax97bA72LyjEbjcEXnyOFJnrgFpxZlqsTDIynASqKz5YlJKKFK/6s7RDT4rPXXbVaRhKUn8mQytkoRfmsJLsgPuvji8Vj+HPgXNzW29/ATV/9y3osymA4nZjK2v5MgGrk7YLHUO0q2XEU6ZwTDiV5YyH1FagWn1u8q1M9B2AhUAglaY+heiipNo0hXPmcDGQlgZtiqkNJnc3lNQbweh0FUmpjEYuY9hiqzHLQ6GK6sYC2U2igVzBI+vvS71EnD1giIY+hbyxd9fXmmhkbBqVUSin1d0qp53j//l5XQQN4HsSMEZFOEblZRJ4UkSdE5HIRWSYiPxeRPd7PrlrOaTAsRP7hB4/z5hvvn/ZxeUf5V7jVxGe9sQZnBrgZLYXiqFQ2TyJi+amRkwvIMBTEZ60xTOMx5Gfu7ZQb1AMBwxC1/FBSpawk/bxQumpEiEa0+Dy9YVjR5mYeaSMEgSE9ZTwG/X1rRyQSaJXx8KFhnvvRX7K3f6Lqa84ltXgM03FFjY//NPATpdSzgAuAJ4C/BX6plNoC/NK7bTAsWhxH8asn+zkxgyu+vOPQHC+Mn6yEDgvpmQEQGPLih5Lczas54W6MkwsqlOR5DNNoDLMNJYkUNtzmYo8hWhiMU602oinQ6yjneB5DRMg5boHbdKGk3nbXMPSNFr53bRjCGkMhhRYKhj0iheZ6R4bdeRLz6TXMpWGYMSLSAbwAuBFAKZVVSo0A1+I26sP7+ZpGrM9gmCuePDHOycls2SrYIEopcnbBY6hmGILn0jMD/CEvgdBEPLowPQZ9Ja43yGwZPSWTt30toqZQkhOezFbwGNyfhTkNlq8DlCNYuZzzDIGfrjqDUFJvu5sNFtzM/VBSOY+hOJRkid8SQycO6AuCXz3ZxwP7h6q+/qnSEMMAbAIGgC+JyMMi8gURaQF6lVLHvcecoMLcKRF5u4hsF5HtAwMD87Rkg6F27tk7CLihHVVFTNRhg5kYhlSusMnrTVY30dMbDbjx9KZYBBFILSTDkA2Hksq912AbjGKPYW//eMWMnbyt/OltEE5XhUJ/o0o1DJrgPIVCKEn8eQzVhvQAdDXHiUWEvlAoyZvFUCYrqRBKKu26qp+Xyrrf4b/+5Ck++6s9VV//VJlLw1BLWWUUuBj4T6XURcAkRWEj5f4Vlf32lVI3KKW2KaW29fT0zHa9BkPdudszDMGwTzl02KAppjN1KhuRYIaRn1JZzjB4bRta4lEmMgs3lJQvU+AWFG2Dn9vhoRQv+9Sd3P5Uf9lz523Hn/cMgZYYsYL4DNX1BSjSGLxQUrSGdFXLEla0JUMew3jR9DZw5zFAIJRUxWPQBnUym697x9yaDYOItItIuWlun67hNEeAI0oprcjdjGso+kRklfc6q4Dy377BsAjI5G0e2D/kX11WCyfpq+ZCuurMQklTvt7ghpIsCRsGgJZEZEGFkgrpqno+QqkRDHsMhfd7ZHgKpQq1CMVk7fAAnmDls/vTva99mt5LxaGkmBafvXRVfb5qrGhP0D9WRnwu4zFkijyGiBQ8huJQUipjM7xQDIOIPEdEHgMeBR4XkZ0icom+Xyn15ZmeSyl1AjgsImd7h14K7AZuBa7zjl0H/GCm5zQYFhoPHxphKmfz/C3dgPuHnbcdPvbjJ0uu+PJ2OJRUzbso5zHoOoZoYMC93iBb4lEmswvHMEzlagwlBe4fnMiEzlGM7oSqSUYrhZKm8RgCoaS8p1vELCFnKzJ5e1rxGaC3LRlKOtAaQ0uissYQqmPw3vZY2v0s9HtOZW2GUgvEMOAKxX+qlNqolNoAvAv40im89p8DXxeRR4ELgY8CHwNeJiJ7gKu82wbDomT/4CQAl2xYBrh/2E/3TfD5O57hjqfDzrDuFzSzrKTCJp8OGAbLCnsMCd9jiC4oj2EqZxOLSNUq70oaw8npDIOjQsbRsoQ3Xrqe529xQ86xGYeSrKImepYfotI1ItOxsiMcSprI5EOtOaA0K6kgPlNGfM5jO25n2uI6lrmmlrbbtlLqLn1DKXW3iMz6f5vXa2lbmbteOttzGgwLCR0CWd7iCp1TgcZ3xXUKJR7DLEJJwV5JUBxKWlgaQzIW8TfFcgVuuiAwaknIexr0ZktU2hSztlMyy/lfXnee/7vvMVQpboNKdQy6RmRmhmFFe4LxdJ5UNk9zPMpEJu97SZriOoZomVDSRCCUFDSIQ6ksa+JN065jNkz77kTkYhG5GLhDRP5LRF4kIi8Ukf/AzGQwGCqip5Tpq9NU1vbjzJUMQ9K/iq5NfC6uY4CC0LrQQknaMMS9YTrlPQZ3vd2tiZDH4IeSKhiGvNe+ohL6M5luvkMyHvE/55zX3FCfN++oGRmGXl3k5ukMxbMYIJiV5L6WDiVZgZYY415W0lTWDnmL9dQZZuIx/FvR7X/wfgoVsoYMBkMhBKKvTqdytr/ZZIoMgx9Kis2gjiFXahgcryVGeY9hgYWSsjZNsYh/ZV8plNQSj9CciIQqn32PoaLGEA4lFVPwGKbPSsrmHa8Rngp5DFB9epvGL3IbS7Oxu6WksyqUqWPQlc+BlhhB8TkV8PzqmZk0rWFQSr0YQESSwOuBjYHnGcNgMFQgnXdj6foqcSqb9z2GYsOgPYZEzMKS6qGkShpDJFDxC0WhpAVU+TyVm5lh6GiKEY9Yoc9iOvG5XCgpyIzrGAIzGXLeOaNlPttq6CI3LUCPl/MYKqSrBltiBA1D0PMbrqMAXYvG8H3cWQw7AK2oGMNgMFQgk3PTGrVuMJWz/UKzEo8hMJIy5o2QrEQqa/sbhw6pFAb1FB4XCiUtII8hnXNIxqzCplhOY5jK0d4UIxGLhD6r6UNJKpSVVMyM6xgC35lvGAIf7nQFbgAr2otCSek8qzvDmkClAjerjMYwlcuHwogN9RgCrFVKvaJuKzEYTjMyeZtkzPI3GfeKr4L47BRGUsYjVtV01amsTVdznMGJTFhjqFjHECWVtRfMeM8prTHoITVl3uuY5zEoVfislFLTp6s6ji9ql0N/JtNqDLptdtb2jU2tHkN7MkoyZvmZSa7GEK5/iHhr1Sm5WiOKRlyNIZ2z/f8LqawdMgz11BhqSVe9V0TOm/5hBoMB3CvjRDQSmghWCCWFNza9OUYjbjpj0GN4z3ce4SePn/Bvp7I2rQl3VKVvGJQXBw9sisFQErjzABYCY1M5kjMMJSVilv9ZueNM3cdWzkqaRmOYofjcFJinkPeb6AU+2xloDCLCyvak3xZjIlOqMRR7DFaRxxAcsDSVtUOtTepZy1CLYbgSeEhEnhKRR0XkMa8GwWAwlCGTt0nErMImE/jDrugxWOKGkrxq4MlMnu/tOMp7b97J8VG3y+ZUzqYpHnXnCut0VcerfA78RQfrGPS56sEvdvf5a5uOHz16nCdPjHPl5u6AYSgfSvI1Bs9waG8Bqhe4Vdu0axGfAca94jI3lFSbxwBuOMn3GNL50CwGKNN2O6AxOEr5r29J2OOMRy2GJ+s3x7sWw/BbwBbgauC3gVd5Pw0GQxm0x6BbNqdytt+zqJL4HI1YxKJSshmOp/N88HuPoZSrKzTHI6Fc++JB8lC4qm2t47AepRTv/NpDfOU3B6d97MmJDP/7B49z/toO/vCKjb4WUE5oD3kMufBn0RyPVNUYqnkM567u4MJ1nSUicDHayxvz0mZjFbyx6ej1DEMm74aEiusYKmkMTTG39kR7DN2tCS+U5N5e29m0MDQGpdT037zBYPDRGgN4BVOBP+xK6arRiOsxaMOgB7286Owebn9qgB2Hhkll87Qkou4GmSvMY7CkqI7B27x0NXWqDkVuOtQyMoMJdTfde4DRqRzffNtlvpAbtaSkiV427zCVs+loinFyMhswku5GuK6ruWJdRq6oiV4xrzx/Fa88f9W0a9Uag25HEbXCWUkz6ZUE0NuWoG8sHZjeVj1dVYeSVnUkuePpAd+Y97Yn2Tcw4WsMa7qaKvaLmgsa1XbbYDjtcbOSPMPgXeUWCtyKNQYdSrKIRyy/iZ7+4/+dS9YCcHhoipRXBxBs9Fa27XaRxlAPj0FXVOuQRzVOjKXpaU1w9spCD043A0vxg0eO8v6b3ci0bofR0eyGkoo9hnXLmkLzroPknOmH6MyEJt8weB5DdHahpN72JOmcw3FvYE9pgVu4waI27Gu6mugfz3DS8wp62xOkcjaTmTwiruGop8dgDIPBUCdcj8HdYJrjUf8P272vOJQU9hi0IKsNw3lrOgB3c53KeaGkQKO3aqGkFu0x1KH6WZ9zLD39uUdSuZLYfiwiZPMOdzw1wA92HgVgdMrd8IrF58Fx9/iazqZQdk6QvK1CV/azpRBK8jQGT/vRzNT4rPBqGZ4ZcMdylhS4ecbmgQNDiMCW3lbAfY8AT58Y986TRCm3dqE5FmF5a4LhVLbqjI9TwRgGg6FOpIMeQyzCVLaQh14cV88F01Wjli/IDoxniFjC2q5mWhNR+sbSrsfgic/BdFVXfC6frgr18Rj0OWfiMYx4ukEQbQTH0jnSOYd0zmbYC0t1NcdDBW4nJzN0NMVob4qF5l0HmS6UNFMK4rPWGGovcANY6dUyPOPNa26r0BJjJJVj24Yuv1p6TZdrGJ48MQbAijbXwAyOZ2lORFnWHCdnq7p8p2AMg8FQN4IeQ1Pc3cQrVz4HC9wkoDGkWd4SJ2KJ399fi8/BUJLbK4myGkNhvOfcawza0AUH61RitKzHYJG3lS/yDqeyfn5+V3Pc8xgKoaTu1jhN8QhKlX6G4GY4xauIzzNFX9n3j7shIDeUVFuBGxTaYuyt5DEEjM0rzytoH2s7mwF44rjnMXh9lwYnMjTHI3R5jRnrlZlkDIPBUCeCHkOz15StksdQyEryxOeAxtDjXS3q/v5ut85CKEkphaPcYqlISCD1XlvXMdQhlDTpewwzCCVNZUtaUcSi4nsM4G50WsjubI4Rj0TIOwrbUQyOZ+luTYTSf4spnuA2W1oTUTYsb2bHwWF3nZaEKqpnnq7qhZL63RbsLcXis2fIReC3AoZhZUcSETg6MkVzPOIbFNcwRFnW4hrYetUyGMNgMNSJkMcQi1QtcPOzkrT4rDWGiYwfRuhtT3B4KIWj3KwZPTNAjz8uabsdcV9bawz1CDv4HsNMQkkVPIas7fiC80gq6/cA6mqJ+yM5s3nH8xgSoeKzYnJO9XTVWrh4fRcHTqb8dYbSVWdofJrjUdqSUfafdA1DcSjJnaFBKIwEruHR3VnbklG/ueLJiSwt8QhdzdpjMIbBYFhUpIuyksbTed8TmM5jCIrPvsfQnvTTV4N1DLqnTqVQUsQSkjGromBbjf7xNIeHUhXv1x5DOudUbfyXztlk8k7JSM2Y5WkMnmEYTuUYTuWIRYSWeKFtRjbvMJzK0tUSC/UxKiY3TYFbLVy0vtP/vaS76gw9BnC/N/3ZFIeSAF5z4Rre8YIzS45rnaEtGfP7bY1n8q7G4IWSThrDYDAsHpRSRVlJkVDlbqUmejHLIuaJz7ajGJzI+oZhReCKsjkeIemlwOpJX5YVFp+DoY/WRHRWHsOH/98TvOsbOyreHzQ21QToYHgoSCwqpHOOX9E7nMoyksrS2RxHRHyPIZO3GUvn6WiKVQwl2Y5CKar2SqqFi9d3+b/HZyk+Q6HLqiUFUTvIJ3//Qq7a2ltyXGcmtSaivjEEtzV7QWM4DQ2DiERE5GER+X/e7U0icr+I7BWRb4tI9d64BsMCJWe7cX/tMSQDXULjEatUfHYKHoPOxBlOZbEdRU9rIZSk0VlJmbzjGxW/AZu3gQU3r9nOZBicyDBYpZAqWGhWTWcY8VJQSzSGiBXKxx+dcjWGLs+A6EKy4VQO21G0Jyt7DLlAyu9ccPbKNr9AMVrcK6lGjwHcDV5k5msreAxRv0gRXM2oLRHlJ3/5fH7/0nUzPl8tNNpjeDfwROD2x4FPKaU2A8PA9Q1ZlcFwimgNIegxaLpaYmVCSYVNLe61xNA1DNpTWBn0GGIR/+pTX7X7LZvLGIbmeHRWWUkTmTzjVQxK0NhUMwyjlTyGiBXypIYnXY1BDzfS70FnB1XzGLRhmKtQUixicf7aTu/32gf1aLRhaEtW789UjPYY2gOhJHA1IxHhWSvbaa/xnDOlYYZBRNYCrwS+4N0W4CXAzd5DbgJe05jVGQynhu4CmihqSwFuGmaJ+ByofNYagzYMQY1Bo7OSoLAh+0NevJ/Bzas1EZmVxzCRyTOZyVcspAoam2oC9IiuZi6pYxBOThQ8huGU6zF0NmmPwX0P+rNob6rsMQR1mrlCh5OKxeeZpquC2xYDSquepyPoMYRCSYmZteM4FRrpMfxf4H2AvnRaDowopfT/3iPAmnJPFJG3i8h2Edk+MDBQ/5UaDDWiN/6Ed3WbDMSWl7e6xUl6dCPoAjX3aj/mtcTQQrMOJWkDAa6Yrc+pN3ztMUQsNxwVDFu0JGY393kincdRlbuZpkKhpMqGoZrHEJw9obOSuoo8Bu1VtCereAx+v6m529Ze8qwVNMUirGhP+JpN8Wc7HX4oqYzwXI21ncFQUlBjqO08s6EhhkFEXgX0K6Uems3zlVI3KKW2KaW29fT0zPHqDIZTp9RjCISSvE0vuCHmnEL+ve4fVOwxJGMRf2Nt9jQGqGAYiq5oZzvFTT9nokKYaDJr++9RF6mVw9cYmks1Bk13a9wTn3N0tlTyGKJVNIZC6/K54tJNy9j1zy9nRVvS/35q0RegEAqcjccQtYRlLQmSgaZ9LfPgMdTf9JTnCuDVInINkATagU8DnSIS9byGtcDRBq3PYDglqmkMOtUwk3f8+/O28je0uFf53D+epiUeCRVF9bYlGUnl/HRVKNQnBA1DpGhzbElEatYYbEf52UITmTwryjwmlcmzqiPJgZOp6qGkVI6o5aagBgmGu9Yva+bYSJqs7fjGM+F7DK5haU/G/CvmYo9B6zTVZj7PBq3ZlBP1Z4JOGqjVMDTHo3z7HZezpbcVyxI/PTkYlqwXDfEYlFIfUEqtVUptBN4A/Eop9SbgduB3vIddB/ygEeszGE6VYo8hGErSm15QZwhW7OqN5zfPnAyFj6BQSdsU0Bh0iEhrC5ZIiTja1RJnaDIbCl9NRzD0VCnVdTJr++0aqjXSG5lyi9uKQzBBPWDD8hZOeENtirOStMfQ0RQjGfc6klbwGOZSYwit1SrVbmaC/nxqNQwAl2zo8gVmfXExHx5Do7OSink/8B4R2YurOdzY4PUYDLNiJh5DMDMp5xSG2J/Z04ol8OSJcS4K5NJDIV7dHNAY9PCf4JVt8VXtqvYkWdupqYVCMPRUyTCksnnaklHaEtFpNYZy4zT11X3EEj8LByjJStKGoS0ZJR6xsKRyVtJcewyayCw9hnjU4rmblnHe2o5Ten19IVCuFmKuaVQoyUcp9Wvg197v+4BLG7keg2EuyFTQGEQKmTnBWoa8XRhi/1vnrWLPR64B3KKoIBuXN7tGIRopozHg/SxjGLxN98Romu7WsBdSiaCuUEljSGVsmrvdtg/TaQzF+gIUNvH2ZDQkTBeHkgYmMrTEI75X1RyPlngMWgivJWOoFkTE735bK99+x+Wn/PoFj+E0DSUZDKc7xR6D/tkSj/pFU9p4QOlISq0TFIde/ujKTdz6Z1e6Med42DDodFXLKg13rOpwPY1jI6Wzmb/94CH++Ye7So5PzMBjmMjkaYlHaG+KTVv5XM5j0J1Q25tivjGA0lDScCobaqeRDLQc1zx2ZBSAc1a1V1zHqaJ7WTWCJk9baI4vvVCSwXBaUKmOoSUR8a84w1lJakYhkOZ4lM0r3GEuFcVnKb2qXekZBh3DD3LLw0f54c7jJceDxqBSRlMqa/uN4qqJz6NThdqEIAWPIUZXS+H+4lCSUoSKuZriVkko6eHDI/S2J3wjWA+is/QY5gLdSM94DAbDIqXYY2gK/FHrq+BMLiw+F2cSTYdvGNJh8blcKKm7xc3D1yMmg+zpm2AiU7qpB8NH5aqflVJMZvO0JCK0J2PTVj53NJcaBh0aam+KhkJNnc3hdFX9GI3uVhtkx6FhLl7fVVONQa2U02/mi+Z51BiMYTAY6kCxx6DDPi3xqH8s5DHMYiRlUzyCSKHDZjBdtTjObllCb3uS40WhpJMT7lzhdK7Qc0kznceQzjkoVWgtXckw5GyH8Uy+pE8SFEJJHYFQUlsi6nsSwU04GIpqKtIYBsYzHB6aCjW+qwfRiFU3DWM6mozGYDAsbiplJQVDSSGNwXFqzqaJRy3OWtHGI4dHgIJhiEUs3ysJsqojWeIx7PFGTkJpryNtGETKi886nbU14WoMlUJJuqV2cdWzXit4oSTv/s5ASCnqzSvQj9E0xcKhpIcPuQN1gq2y60HMKk0Fni/0/yGjMRgMi5Rij0HPDHY9BvcPO+gxFIvPM+Wi9Z1+d1KdrvrBa87hz1+yueSxKzuaSjSGPX3j/u/F4rH2ErpbE2VDSSkvTTboMZTrqTRSzTBEdSgpRnsyhiWERGgJ6CVB8bmpSHzecWiEWER49ppTSwmdjmjE8luBzzfN8WhZb7AeGMNgMNSBTN4makmob0+TV8XsewyBArec7RCbxRyBYOhEawxXbO7mgnWlV86rPY8huHlX8xjGM3niUYtlzfGyoSTtMWiNwXZU2WFAehZDuawkHT5rT0axLKGjKVbyOG1I2wO9horTVXccGmbr6o5QIWE9uOa8VbzorHI14PXn+Vu6ee1Fa+qqoWgaXsdgMJyOBKe3adZ2NbN+WXNBYwjWMThqVleCF28oGIDpxOuVHe4ksaHJLMu9Woan+8axBBxVahgmM3naElFaEpGy6aq6bsD1GNzNfDydL4mB93leyvKW0vqJYm9gU3cLZ3S3VH0MeOmqnhEamszyyOER3nLZhqrvfy742996Vt1foxIvPaeXl55TOtCnHix5j+FffvwE7/7Ww41ehuE0Izi9TfM/f3I5775qS8BjCBuG2XQFPaO71b+Stqa5ktRpnEGdYU/fhJ/3XxxKmvA2+dZkjImMzYnRNM/96C/YfWzMvd8LJbUkIn7GkNYZ/urbj/C3//MoAI8cHiEesThrZWvJmrTGoL2Er/3xc/ngK88JPSZRLpQUt3yP4ev3HSSbd3jDc+oztGYpsuQNw+1P9vPA/qFGL8OwyLn3mUFuuPMZ/3Y5j6E5HvWE4XIFbs6suoJalnChF06azmNY1eFWP2vDoDOSdDiqnPjcmnDbXUykc+w6NkrfWIYHD7h/L6lMqccwNpVjaDLLrTuP8cOdx8jZDjsODvPsNe1lBfGg+KzPVfw43zAkizSGrE0mb/OV+w7ywrN62NLbVvX9G2bOkjYMOdth/+AkgxOZmpqLGZY2T/eNc3goFTr2jfsP8YmfPuWnfAY7pxZTrsBttuIzwMVeJs50Dof2GE6MuimrWl/Q4agSjyGTpzVZCCUd9VJd9w9OAvidV1viUbascPs7/eKJfn6264TfmfXBA0M8enS0pOeTJuZXPleOase1xhCsY/A0hh88coyB8QzXX7mp+ps31MSSNgwHT06SsxU5W/mZEwbDdLzzaw/xd99/PHTs6MgUOVtx8KS7aaZzdsVCKJ3uGCxwC85jqJUXntVDPGKFJryVY3lrgqhVKHK7Z+8gInDppuVAZY+hNRFjMmNzdDhsGHyNIRFhdWcTr3j2Sr5x/0H+Z8cRVrYnEYEv3LWfbN6pWF+wztNc1nU1V1x3JY8B4O9veZytq9p5/pbuqu/dUBtL2jDs6StkZAxUGXhuMGgmMnn2DUzy2JGRUHaP3jT1/6lqHoNOwcwUeQyzHTBz0foudn/o5aytsrmCG2pa2ZHkkcMjpHM2X7//EFed08uaziYSUatEYJ7M2J5hcD2GI0WGQc930O2kr79yE2PpPA8eGOa1F6/hvDUd/OrJfiAskge5eH0XT3zoFf4wm3JoAxvMVmrzdJXLzlzOV6+/dF4ydZYSS9owPG0Mg6FGnjjuCq/DqZxfE5DJ2/4YTv1/Kp2zq2YZJaJWicZwKiMpZ/rctz5vI/c+c5J3fu0hhiazfgimLRkrmacw7ovP7ib8tFfzcGQ4RSZvk8rmsaRwRX/x+i4u9NJkX3neKq7c7F7Fr2xP+vpGOaxpDGI58fnaC1dzw1su4ctvfY6fYWWYO5a0YdjTP+679f3jpT1kDIufvf3jflWsZmA8w08eL20aNxN2HR0N/O4aieMjgSyffnfzrOYxgJubX9pEr/5XvX94xSYuXt/Jr58a4Nlr2nnupmWAWyNQrsCtLemGkgD2DU7SEo/gKDg8lGIyY9MSj/pX6yLC/37VOfzxlZs4d3W7bxgqeQszJRG1EHFbZWjakjGuPnfltEbFMDuWtmHom/BL6I3HcHrykR89wV99+xH/tuMo3vX1HbzzazsYnYWutPv4GG3JKCKwy0vb1KJsWzJaCCXNxmOYRYFbrUQs4RO/ewFrOpv4y5ee5W/qrYFeR6NTOfK2w1TODSXpiWG2o7jsDFeP2DcwSSqbp7lomtglG5bx96/aiohw8YYuNixv5mVbTy33PhGN0JqIGiMwjyxZw5C3HfYNTnDh+k6aYhFjGE5TDp5McWgo5ReTfeU3B3jAS7fsL9OCejp2HRvjwnWdbFrewu7jrveg9YXnb+lm3+AEeduZgcdg+R6DUop0zpm3rp1n9rRy9/tfzFWBDbstGWUik+f46BTbPvxzfvDIMcBt2NYWqDi+wvMC9g9OerMYKmcTJWMR7njvi3ntRWtPab2dzTFWtJlw0XzSEMMgIutE5HYR2S0iu0Tk3d7xZSLycxHZ4/2sW6vEAydT5GzFWSva6GlLMDCxMA3DV+87yL/9vVfVAAAAGThJREFU7KlGL2NRYjuKI8NTOAoODaXoH0/z8Z88xeoqswmqkc07PN03ztbV7Zyzut33GI6MTCECL9jSQ85WHDiZmlZjiEctPyvpyPAUUzmbM3tKC8DqRbFY25ZwB+3s6ZsgZytuefiodzwa2vzPWdXO8pY4+wcnGZrMlngM9eA9LzuLL771OXV/HUOBRnkMeeCvlVJbgcuAd4nIVuBvgV8qpbYAv/Ru14W9Xiz4rN42VrQl6B+rbhgyeZsv3bM/1MZgPrhlxxG+t+PovL7m6cKJsbR/VX5gcJIH9w8zlbN5v9fWoG+a71yTztl8+Z79bD8wRM5WbF3Vzrmr2zkyPMVoKsfR4SlWtCU4d7XbwG1v//iMPAZd+awNzNbV9Zs8Nh26CZ4Oi/1m30nADTG1BjyGtV1NbOpu4ee7+7j3mZPz0jdoeWuCDctbpn+gYc5oSK8kpdRx4Lj3+7iIPAGsAa4FXuQ97CbcWdDvr8caBsYzxCMWZ65ooactEWomVo5fPdHPP/9wN6s63Hzt+eLQ0BQjqSyOo0yMtUYOnSwUoe0fnGRkKkvUEn8z65uhx3DLw0f5px/u9hMVzl3d4adO7j4+xrGRKdZ0NnHmCnfzerpvYkYeg77I2H1sFEvgWSsbV7nb5g3a0WEx2yv4bElE/XRUS9x+S5u6W9h+cJgze1r4szJdXA2Ln4ZrDCKyEbgIuB/o9YwGwAmgbh2j3nL5RnZ/6OU0x6NuKGkajWG3l6aof84HqWyewYkMeUcxlMrO2+ueLujqZEvcjJpdx8bYvKKVjuYY7cnojDWGu/cOsqwlTm9Hgq7mGJu6W/z2zr/Zd5KjI1Os6WqmOR5lw/JmHjwwNKOsJN1dddexMc7saa17Z9BqtHoaw+HhVMigtQYMQ297kljE4lmr2rEEPvG7FzR0zYb60VDDICKtwP8Af6mUCu24yq0eKtunQkTeLiLbRWT7wMDArF9f536vaEswOpUjnSttGazR7v7uY6MVHzPXHB4qTNvqG0szMJ7hlZ+5yy8wOl3YNzDBNZ++a85Thg8NpYhYwnlrOtg/OMGuY2N+uKa3PTmjUJLjKO7dO8iLzu7hp3/5An787hcQsYTu1gQvPruHb9x/kOOjrscA8LqL1nLXnkHsabqlxgPi8+7jY5zbwDASFFpaP903wQVrO/330xqoY9DH3nzZem7/mxfVfVqaoXE0zDCISAzXKHxdKfU973CfiKzy7l8F9Jd7rlLqBqXUNqXUtp6enlNeS4+X8TBYRYDe5RkEbSBOhbzt8Nlf7pl2IzwU6MfTP5bhkcMj7Do2xv1e/Pd04ddPDbD7+Bg7Do7M6XkPDaVY3Zlk84o2Hj0yysB4xtcBetuT9JX5/J86Mc77b36U9353J7/Y3cfu42MMp3Jcubmb5niUlYFB89dfeQaDE1lytmJNV2HT1NlF02oMObcF9vHRdEP1BShUEj/TP8Gariau2OympbYm3aZ2sYj47zERjZiY/2lOo7KSBLgReEIp9cnAXbcC13m/Xwf8YD7Wow1DpXDS4ESGvrEMK9vdQSd6YtZsuWvvIP/286f5/sPVReWgYegbS/u3DxU1cJstT54Ym3cxvRza2M61J3RoKMX6Zc2c0dPiD5DZ6rWYXtFePuHgmw8c4rsPHeZnu/v402/s4KZ7DwCFNM0gV2xe7usCa72r6eWtCV530RqAqpO+dLqqvuDQBqtR6O6oWdthTWcTv7dtHVdsXk6PV1X82xesPuV6BMPioVEewxXAW4CXiMgj3r9rgI8BLxORPcBV3u2609PqXgX2VzAMuv/86y9ZE7o9W2571JVRgr2aynF4KOU3C+sby/gx87kwDCdG07zyM3fzjfsPnvK5ThWt2+wfrP551MphzzBsCgx+CYaS+sfTJV119w9Ocu7qDn7xnhfSHI/w3YeOsGVFa9kGdSLCO194JpbA5hWFVNPrr9xELCKsaKve/yeTc/z/S9pgNYpgrcKaria2bVzG1//4Mt/7+eTvXcirzl/dqOUZ5pmGGAal1N1KKVFKna+UutD7d5tS6qRS6qVKqS1KqauUUvMyKGFFe3WPQW9cv3PJOu/27HWGnO3ws919ANNmQh0aSrGxu4XlLXH6xgseQ3HL59nw0MFhbEf5g+QbRSZv+3OHDwzOjScEbrO7k5NZ1i1rZqMX9ljb1eRnE/W2JcjZpaL+/sFJNna7mWr//OpzgfLeguY1F63hvg++lHXLCg3stvS2cd8HXsrVVa6wdUuMR4+OsrojSVdLvOJj54PWQLsJrSUYli5mtCewrCWOSGWPYdexMdZ0uvnbqzuSp6Qz3LN3kNGpHBuWN7O3fwKlVMXOkIeGUmz2ip76RktDSf/v0WNk8w6vu7j2ylLdP6iWLKsv3LWPravaeV6VjbJW9vRNkHcUnc0x9s0glHTTvQdY09kUqtothzaeG5a1sLHb3bSDAq/WCvrG0nR74ZJM3ubIcIrXeKGgV1+wGkcpLj+j+vst5xlM19gtHrWYzOS546kBrjlv/tKfK9EWaGmttQTD0qXh6aoLgVjEYsOy5oohol3HRv0QxNZAxetsuO2x47Qlolx3+UavBUF5AdpxlBsKWd5Mb3uC46NpDg+liEcshlM5xtI5Pvmzp/nET2deFT2ayvnvcYdnGJ4ZmCybjXXv3sHQ8amszcd+/CT/dee+Wt7utOj1vHzrSgYnMv5oyHKkczYfue0J/v77j/sDcSqhjef6ZW4a6XWXb+D3thVGP+o2z0Gd4fBQCkfhzxwWEV570dqQ4DxX6AK3iUyea85bNefnr5X2pPEYDAWMYfB43uZu7tt3knzRhvPA/iH2DUz6qXnnr+3kmYGJqhlMlZjI5PnxYyd42dZe/+pVtzIOkrcdBiYyZPIO65Y1s7I9yd7+CTJ5h20b3XU8dmSUfYOTHB9Nc3x0quQcxdiO4rovPcC1n7ubw0MpHj82xoblzdiO4qkT4TXct+8k/+sL9/NlT3gFePTICHlH8fCh4VBc3nFUaC7BTAg+f9exUVriEV50tptddqCK17D9wDDZvMOJsTS3PVa5O+qOQ8N86Ie7aY5HfG/hn699dmiQutYMgkVu+wbc197UXf+Mm+CMgWqhqvlCewzLW+KmNsFgDIPmys3dTGTy7DxS0A+msjbvvXkn65c1c93zNgDwsq29KAU/3XWi5tf47vbDjGfy/MHzNnKWN592b0BnUEq54Zp//Cl/892dgHvFu6I96ee8603kVq/JGcDDh6bXCW68ex+PHB4hZys+8L3HyOYd3vTc9UBpCu4X7toPwI8eLWy+D3taxFg6Hwr5/M13d/LG/75vRu9/JJXlbV/ZzpUf/5U/FGbXsTHOWdXOmZ54Wy0z6e69g0QtYcPyZm68e39Zg7Tz8Ai//1+/QQS++bbLQiGSIDrbJljLcMCbvrZxHgyDnmt89dZef+5xI0nGLCKWmDCSATCGwefyM5Yj4moAmk/89CkOnkzx8defT7PXSOxZK9s4o7sldMX6m2dO8s6vPsTjgV79mbzN+27e6T/OdhRfvGc/2za4w0y6WuJ0t8Z9j0EpxV986xE+/KMnOLOnlbv2uOtYv8wNJWl0j/vbHj+OJe6V546Dw/SPp3nvd3f6vW6C7O2f4P/87Gmu3trL1Vt7udt7j6++YA1tyWhITN8/OMkvn+xjdUeSx46O+m0ldhwcpjnubmY6DHXoZIrvP3KU+/YNhdpPaL794CG+cJcbejoynOKVn7mb25/s59hompu3HyaTt3nCK+5av6wZkeqG4Z69g1y8vou3Pf8MHj0y6vfzCX7mf/PdnSxvSfDDP7uSC9ZVngMQj1q+qB9878tb4qFJYfVCewzXnN/4MBK4YbO2ZNSEkQyAMQw+XS1xnr26w980HzwwxJfu3c8fXL6By89c7j9ORLjmvFX85pmTDIxn+Owv9/CmL9zHT3ad4HX/cS/ffOAQ4F7Rf2f7Ef706zv4m+/u5EM/3MXhoanQ0PLNK1r9zKRvPXiYH+48xl9etYXb/uJKbnjLJfzhFRtdw+CJmyJud8uOJrevzbNWtnP+mg52HBrmv+/cx3cfOsJ7v7sTpRT37zvpZx697+adNMUifPi1z/Zff01nEys7kmxd5WomT/eNc8Odz/BPt+4iZln8+5suBlwDpJRix6ERrt7aS3sy6gvXX7p3vy+c/6gotDOSyvJPt+7mwz96gjufHuB9Nz/KSCrL//zJ87hofSdfuvcAn/r5HiazNlefu5JkLMKazibfMCil+MXuPkZTruYwPJnl8WOjXLG5m9dfvJY1nU38/fcfJ52z2X1sjBvufIa//s5O9vRP8C+vP29GWT697UmOBwzpvoHJeQkjAbzwrG7efNl639AvBN75wjN5w6XrG70MwwLAZCUFuGJzNzfevY/dx8Z4382Psqazife/4lklj7vmvFX8++17ufbf7+bYaJpXX7Ca9778bD54y2N88JbHOLOnlRvv3s/ZvW08f0s3X7xnP46CLStaufrcQgbKWb1tfG/HUR45PMJHfvQEzztzOX/xki2ICFefu9J/rI6Hr+5oIh61WL+smceOjnLxhk6a41G+fM8B9vRNsKojyb3PnOQPvvgAd+1xB71fcWY3Ow6N8H9//0JWtCXpaU1w+RnL2dLrhm62rm7na/cd5FWfvdsvdnvzZeu5eH0XF6zt4LbHjvPK81YxOJHhkg1dDKdy7Dg4wlg6x3cePMxvn7+K/YOT3PbYcf7kRWcyOpWjoynGNx44xFTOZmV7krd/dTvpnMNHX3seF6zr5I+vPIN3fWMHn7/jGX5v21o/PLapu8U3DN9/5Ch/9e2dvOjsHr701ufwm30nUQqu3LKcpniEj7/+fN584/289UsP8NDBYXK2G1Z66/M28uKzZ9bx89lr2vnO9iN87MdP8tdXn8X+wUleeNapV9LPhM0r2vjwa86bl9eaKe984ZmNXoJhgWAMQ4AXbOnm83c8wzWfuQuAb/zxc2lJlH5E56xq44yeFo4MT/HR157HGy9dh4jw+Tdfwis+fSdv+8p2Rqdy/Ovrz+f3nrOOv3n52diOIhmLEAl0SH3WynYmMgd5zefuodnb7Mp1UNWhpHXLXDffNwzru2iOR7jhToes7fCV6y/l3372NHftGeSNl64jk3f43o6jXHVOL9de6BYniQjfeNtz/Sv9C9d18qV7DvD8Lcv51985n/ZkzA8ZvfL8VXz0tif54C2PAe7Q+ZOTWT79yz286+s7mMzaXH/lGdz7zCD/8uMned/NO/nO9iP87iVuv6ArNi/nPS87m9/9/L1cubmbN17qZgW9/Nxe1nY1kbcVf/fKrf773Lyila/dd5Cb7j3AJ3/+NF3NMX791ACf+eVeN5srGeX8tW546Mot3bzx0vV884FDXHXOCj762vNoTUb9kN9M+NC1zyYasfj8Hc/wwP6T9I9n2NRjWj0YDMYwBLj8zOX89x9sYzKTZ/3y5opNwkSEm/7wUhylQj1jWhJRPv768/lf/30/3a1xXu1txpWyPF538Ro6mmLkbIdn///t3XuQlXUdx/H3hwUxbnIRZeWyLEoagshFkhRrxAyZBDUm8DJaNl3VICdLpZIcJjUrSyu11CQviZUONGqR1JiUF1YEARUXgRm5ozRctFTg2x+/34HnWc5hBXb3eU7n+5o5s8/+zmU/+z1n93uey/k9PTulPiSV1K1DW6paiT7x+sLthsTGADC8pgtD+nThlxcNpX7DNobVdMXMOH9EHwZUd0p9ViK5fPYJR9Gj06Gc1LfrXk3p4pF9Wbp2K7MWruVDbao4rkdHNr/9Hmbw3IrNTD9nIIN6HUbndm244YlXebhuNR87uht/WLAaM7jhvEEMq+nCn644lZpu7Xf/3NZVrZj55ZFUSant+V/7xDEsXbOV62YvpW3rVjz29VFc+8hibnnyNTq3a8PPLxia2lE7bdwAzh5cHfcP7f+U5Ie2qeIH5w7io7VdufaR0PxqfQ4g59D+HmqYN8OHD7e6urqsY6Q8PP8Nundq+4E3aXwQ98xbybCaLgzu3ZnlG7fx+OL1XHH6MUjitrn1fPzY7rvfTTcls3A2r/d37mLiSX14d8dOfvpkPWMHVjOo1575fe56egW9urRjzMAezKt/k+dXbWbK6P77fQ6JHTt3cfe8lfTu2o6xg6pZ/e93uOvplXzxtH7NumP09U3bmTn/DSaP7l90LdG5/zeSXjCz4UWv88bgnHOVZ1+NwY9Kcs45l+KNwTnnXIo3BueccyneGJxzzqV4Y3DOOZfijcE551yKNwbnnHMp3hicc86llP0H3CRtAg70jPaHA282eqv8KKe85ZQVyitvOWWF8spbTlnh4PLWmFnRWSPLvjEcDEl1pT75l0fllLecskJ55S2nrFBeecspKzRfXt+U5JxzLsUbg3POuZRKbwy/yjrAfiqnvOWUFcorbzllhfLKW05ZoZnyVvQ+Buecc3ur9DUG55xzDXhjcM45l1KxjUHSGEnLJC2XdHXWeZIk9Zb0d0kvS1oqaXIcnyZpjaSF8TI266wFklZJWhxz1cWxrpL+Kqk+fi1+rtSWzXlson4LJW2VNCVPtZV0j6SNkpYkxorWUsGt8XX8kqShOch6s6RXY55HJXWO430l/SdR4ztaMus+8pZ87iVdE2u7TNKncpB1ZiLnKkkL43jT1tbMKu4CVAGvA/2AQ4BFwICscyXyVQND43JH4DVgADAN+GbW+UpkXgUc3mDsh8DVcflq4KascxZ5HawHavJUW+A0YCiwpLFaAmOBJwABJwPP5SDrmUDruHxTImvf5O1yVNuiz338m1sEtAVq4/+MqiyzNrj+x8D3mqO2lbrGMAJYbmYrzOw94CFgfMaZdjOzdWa2IC5vA14Bemab6oCMB2bE5RnAORlmKWY08LqZHegn55uFmf0D2NxguFQtxwO/teBZoLOk6pZJWjyrmc0xsx3x22eBXi2VpzElalvKeOAhM3vXzFYCywn/O1rEvrJKEvBZ4HfN8bMrtTH0BN5IfL+anP7jldQXGAI8F4cuj6vo9+Rh00yCAXMkvSDpS3HsSDNbF5fXA0dmE62kSaT/sPJaWyhdy7y/li8lrNEU1Ep6UdJTkkZlFaqIYs99nms7CthgZvWJsSarbaU2hrIgqQPwR2CKmW0FbgeOBk4E1hFWJfPiVDMbCpwFXCbptOSVFtZ3c3NstKRDgHHA7+NQnmubkrdaliJpKrADeCAOrQP6mNkQ4ErgQUmdssqXUDbPfcL5pN/UNGltK7UxrAF6J77vFcdyQ1IbQlN4wMweATCzDWa208x2Ab+mBVdrG2Nma+LXjcCjhGwbCps14teN2SXcy1nAAjPbAPmubVSqlrl8LUv6HPBp4MLYyIibZN6Kyy8Qttl/OLOQ0T6e+7zWtjVwHjCzMNbUta3UxjAf6C+pNr5znATMzjjTbnH74d3AK2b2k8R4ctvxucCShvfNgqT2kjoWlgk7H5cQanpJvNklwKxsEhaVeseV19omlKrlbODieHTSycCWxCanTEgaA3wLGGdm7yTGu0uqisv9gP7AimxS7rGP5342MElSW0m1hLzPt3S+Is4AXjWz1YWBJq9tS+1hz9uFcDTHa4TOOjXrPA2ynUrYVPASsDBexgL3AYvj+GygOuusMW8/wtEbi4ClhXoC3YC5QD3wJNA166wxV3vgLeCwxFhuaktoWOuA9wnbtb9QqpaEo5F+EV/Hi4HhOci6nLBtvvDavSPe9jPx9bEQWACcnZPalnzugamxtsuAs7LOGsfvBb7S4LZNWlufEsM551xKpW5Kcs45V4I3BueccyneGJxzzqV4Y3DOOZfijcE551yKNwbnDoCk6yWd0QSPs70p8jjXlPxwVecyJGm7mXXIOodzSb7G4Fwk6SJJz8f57O+UVCVpu6RbFM6LMVdS93jbeyVNiMs3Kpw74yVJP4pjfSX9LY7NldQnjtdKekbh3BXTG/z8qyTNj/f5fhxrL+kxSYskLZE0sWWr4iqRNwbnAEkfASYCp5jZicBO4ELCp6TrzOx44Cngugb360aYRuF4MzsBKPyzvw2YEcceAG6N4z8DbjezQYRPtRYe50zCNAYjCJO5DYsTEY4B1prZYDMbCPy5yX955xrwxuBcMBoYBsyPZ8UaTZjqYxd7Jiu7nzBdSdIW4L/A3ZLOAwpzA40EHozL9yXudwp75mi6L/E4Z8bLi4QpDY4jNIrFwCcl3SRplJltOcjf07lGtc46gHM5IcI7/GtSg9J3G9wutVPOzHZIGkFoJBOAy4HTG/lZxXbsCbjBzO7c64pwus6xwHRJc83s+kYe37mD4msMzgVzgQmSjoDd51iuIfyNTIi3uQCYl7xTPGfGYWb2OPANYHC86l+EWXshbJJ6Oi7/s8F4wV+AS+PjIamnpCMkHQW8Y2b3AzcTTvXoXLPyNQbnADN7WdJ3CGeha0WY0fIy4G1gRLxuI2E/RFJHYJakQwnv+q+M41cAv5F0FbAJ+Hwcn0w4icq3SUxDbmZz4n6OZ8Ks62wHLgKOAW6WtCtm+mrT/ubO7c0PV3VuH/xwUleJfFOSc865FF9jcM45l+JrDM4551K8MTjnnEvxxuCccy7FG4NzzrkUbwzOOedS/geE7VSPDEjBjgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 66.000, steps: 66\n",
            "Episode 2: reward: 49.000, steps: 49\n",
            "Episode 3: reward: 52.000, steps: 52\n",
            "Episode 4: reward: 42.000, steps: 42\n",
            "Episode 5: reward: 46.000, steps: 46\n",
            "Episode 6: reward: 54.000, steps: 54\n",
            "Episode 7: reward: 69.000, steps: 69\n",
            "Episode 8: reward: 46.000, steps: 46\n",
            "Episode 9: reward: 48.000, steps: 48\n",
            "Episode 10: reward: 76.000, steps: 76\n",
            "Episode 11: reward: 50.000, steps: 50\n",
            "Episode 12: reward: 61.000, steps: 61\n",
            "Episode 13: reward: 56.000, steps: 56\n",
            "Episode 14: reward: 65.000, steps: 65\n",
            "Episode 15: reward: 46.000, steps: 46\n",
            "Episode 16: reward: 42.000, steps: 42\n",
            "Episode 17: reward: 98.000, steps: 98\n",
            "Episode 18: reward: 47.000, steps: 47\n",
            "Episode 19: reward: 63.000, steps: 63\n",
            "Episode 20: reward: 90.000, steps: 90\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb171e57c10>"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    }
  ]
}