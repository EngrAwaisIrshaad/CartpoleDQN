{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPoleDQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Megacity1/CartpoleDQN/blob/main/Week%203%20Deep%20RL%202/CartPoleDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Deep Q Network (DQN) for CartPole Using Boltzmann Q Policy\n",
        "This exercise implements a DQN for CartPole using a Boltzmann Q policy for selecting the actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGsC7cJ5jNcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda73cbf-3020-40f4-e45e-7b87053448f1"
      },
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.24.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (13.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.44.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.10.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMIHLgQ3Z-lF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5a8608-456f-4127-ce8a-f551e5f4e4f3"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0AMLzq08ap0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b380e1-6c3d-4bce-8233-a8ac01deaf26"
      },
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "!pip install Adam\n",
        "#from keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Adam in /usr/local/lib/python3.7/dist-packages (0.0.0.dev0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll6bNdUm54WS"
      },
      "source": [
        "Implementation of DQN for CartPole, applying policy BoltzmannQPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCrPKNy40PC"
      },
      "source": [
        "##Implement DQN with BoltzmannGumbelQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efM9jkXr5A3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "74320ade-a650-4862-b96f-cb5993919a2b"
      },
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import BoltzmannGumbelQPolicy\n",
        "from rl.policy import LinearAnnealedPolicy\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n",
        "# setup experience replay buffer\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
        "                               attr='eps',            \n",
        "                               value_max=2.,\n",
        "                               value_min=.2, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=200)\n",
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "# add extra layers here\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=15,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy) \n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=4000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_33\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_31 (Flatten)        (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_63 (Dense)            (None, 16)                80        \n",
            "                                                                 \n",
            " dense_64 (Dense)            (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 4000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   44/4000: episode: 1, duration: 8.779s, episode steps:  44, steps per second:   5, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.598857, mae: 0.616910, mean_q: 0.200632, mean_eps: 1.734500\n",
            "   60/4000: episode: 2, duration: 0.307s, episode steps:  16, steps per second:  52, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.506022, mae: 0.613859, mean_q: 0.300292, mean_eps: 1.536500\n",
            "   70/4000: episode: 3, duration: 0.166s, episode steps:  10, steps per second:  60, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.451315, mae: 0.617122, mean_q: 0.359671, mean_eps: 1.419500\n",
            "  108/4000: episode: 4, duration: 0.692s, episode steps:  38, steps per second:  55, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.402991, mae: 0.664470, mean_q: 0.513012, mean_eps: 1.203500\n",
            "  119/4000: episode: 5, duration: 0.217s, episode steps:  11, steps per second:  51, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.400166, mae: 0.740531, mean_q: 0.759812, mean_eps: 0.983000\n",
            "  140/4000: episode: 6, duration: 0.331s, episode steps:  21, steps per second:  63, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.348472, mae: 0.781210, mean_q: 0.900202, mean_eps: 0.839000\n",
            "  155/4000: episode: 7, duration: 0.170s, episode steps:  15, steps per second:  88, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.274979, mae: 0.791142, mean_q: 1.028533, mean_eps: 0.677000\n",
            "  168/4000: episode: 8, duration: 0.159s, episode steps:  13, steps per second:  82, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.251742, mae: 0.838209, mean_q: 1.214289, mean_eps: 0.551000\n",
            "  180/4000: episode: 9, duration: 0.161s, episode steps:  12, steps per second:  75, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.240018, mae: 0.914349, mean_q: 1.468057, mean_eps: 0.438500\n",
            "  195/4000: episode: 10, duration: 0.197s, episode steps:  15, steps per second:  76, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 0.219526, mae: 0.917790, mean_q: 1.512798, mean_eps: 0.317000\n",
            "  208/4000: episode: 11, duration: 0.161s, episode steps:  13, steps per second:  81, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.846 [0.000, 1.000],  loss: 0.200652, mae: 0.967381, mean_q: 1.732105, mean_eps: 0.210385\n",
            "  217/4000: episode: 12, duration: 0.139s, episode steps:   9, steps per second:  65, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.338314, mae: 1.079554, mean_q: 1.977270, mean_eps: 0.200000\n",
            "  227/4000: episode: 13, duration: 0.160s, episode steps:  10, steps per second:  63, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.249561, mae: 1.071948, mean_q: 1.938410, mean_eps: 0.200000\n",
            "  237/4000: episode: 14, duration: 0.176s, episode steps:  10, steps per second:  57, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.340658, mae: 1.188016, mean_q: 2.167185, mean_eps: 0.200000\n",
            "  246/4000: episode: 15, duration: 0.166s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.331391, mae: 1.185935, mean_q: 2.208658, mean_eps: 0.200000\n",
            "  258/4000: episode: 16, duration: 0.226s, episode steps:  12, steps per second:  53, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 0.430659, mae: 1.283197, mean_q: 2.325316, mean_eps: 0.200000\n",
            "  268/4000: episode: 17, duration: 0.170s, episode steps:  10, steps per second:  59, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.447093, mae: 1.324897, mean_q: 2.398497, mean_eps: 0.200000\n",
            "  277/4000: episode: 18, duration: 0.183s, episode steps:   9, steps per second:  49, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.507545, mae: 1.374197, mean_q: 2.443399, mean_eps: 0.200000\n",
            "  290/4000: episode: 19, duration: 0.247s, episode steps:  13, steps per second:  53, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.636514, mae: 1.448690, mean_q: 2.560604, mean_eps: 0.200000\n",
            "  299/4000: episode: 20, duration: 0.186s, episode steps:   9, steps per second:  48, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.436567, mae: 1.411348, mean_q: 2.492271, mean_eps: 0.200000\n",
            "  308/4000: episode: 21, duration: 0.180s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.503424, mae: 1.507328, mean_q: 2.737021, mean_eps: 0.200000\n",
            "  322/4000: episode: 22, duration: 0.282s, episode steps:  14, steps per second:  50, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 0.569326, mae: 1.598269, mean_q: 2.923313, mean_eps: 0.200000\n",
            "  332/4000: episode: 23, duration: 0.194s, episode steps:  10, steps per second:  52, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.554687, mae: 1.595257, mean_q: 2.916799, mean_eps: 0.200000\n",
            "  342/4000: episode: 24, duration: 0.225s, episode steps:  10, steps per second:  44, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.535772, mae: 1.664366, mean_q: 3.032665, mean_eps: 0.200000\n",
            "  352/4000: episode: 25, duration: 0.218s, episode steps:  10, steps per second:  46, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.682657, mae: 1.753898, mean_q: 3.136197, mean_eps: 0.200000\n",
            "  361/4000: episode: 26, duration: 0.218s, episode steps:   9, steps per second:  41, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.432468, mae: 1.651499, mean_q: 3.017522, mean_eps: 0.200000\n",
            "  373/4000: episode: 27, duration: 0.241s, episode steps:  12, steps per second:  50, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 0.563818, mae: 1.780689, mean_q: 3.256813, mean_eps: 0.200000\n",
            "  382/4000: episode: 28, duration: 0.168s, episode steps:   9, steps per second:  53, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.687505, mae: 1.892502, mean_q: 3.451137, mean_eps: 0.200000\n",
            "  391/4000: episode: 29, duration: 0.166s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.967474, mae: 1.981839, mean_q: 3.560122, mean_eps: 0.200000\n",
            "  401/4000: episode: 30, duration: 0.178s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.625692, mae: 1.898243, mean_q: 3.481066, mean_eps: 0.200000\n",
            "  410/4000: episode: 31, duration: 0.187s, episode steps:   9, steps per second:  48, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.771996, mae: 1.913663, mean_q: 3.474828, mean_eps: 0.200000\n",
            "  419/4000: episode: 32, duration: 0.137s, episode steps:   9, steps per second:  66, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.826969, mae: 2.037388, mean_q: 3.646646, mean_eps: 0.200000\n",
            "  430/4000: episode: 33, duration: 0.194s, episode steps:  11, steps per second:  57, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.589984, mae: 1.960536, mean_q: 3.709742, mean_eps: 0.200000\n",
            "  440/4000: episode: 34, duration: 0.177s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.859801, mae: 2.043140, mean_q: 3.848510, mean_eps: 0.200000\n",
            "  450/4000: episode: 35, duration: 0.193s, episode steps:  10, steps per second:  52, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.776985, mae: 2.095480, mean_q: 3.886363, mean_eps: 0.200000\n",
            "  460/4000: episode: 36, duration: 0.151s, episode steps:  10, steps per second:  66, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.653917, mae: 2.121508, mean_q: 3.986558, mean_eps: 0.200000\n",
            "  472/4000: episode: 37, duration: 0.237s, episode steps:  12, steps per second:  51, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.980455, mae: 2.225183, mean_q: 4.145283, mean_eps: 0.200000\n",
            "  485/4000: episode: 38, duration: 0.256s, episode steps:  13, steps per second:  51, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.846 [0.000, 1.000],  loss: 0.876779, mae: 2.190441, mean_q: 4.105256, mean_eps: 0.200000\n",
            "  494/4000: episode: 39, duration: 0.137s, episode steps:   9, steps per second:  66, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 1.196561, mae: 2.342865, mean_q: 4.220307, mean_eps: 0.200000\n",
            "  505/4000: episode: 40, duration: 0.217s, episode steps:  11, steps per second:  51, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.870311, mae: 2.278755, mean_q: 4.184900, mean_eps: 0.200000\n",
            "  516/4000: episode: 41, duration: 0.213s, episode steps:  11, steps per second:  52, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.966241, mae: 2.404505, mean_q: 4.337285, mean_eps: 0.200000\n",
            "  527/4000: episode: 42, duration: 0.274s, episode steps:  11, steps per second:  40, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.810703, mae: 2.355020, mean_q: 4.385284, mean_eps: 0.200000\n",
            "  538/4000: episode: 43, duration: 0.177s, episode steps:  11, steps per second:  62, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 1.191694, mae: 2.478938, mean_q: 4.540020, mean_eps: 0.200000\n",
            "  548/4000: episode: 44, duration: 0.132s, episode steps:  10, steps per second:  76, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.734338, mae: 2.421757, mean_q: 4.502838, mean_eps: 0.200000\n",
            "  559/4000: episode: 45, duration: 0.133s, episode steps:  11, steps per second:  83, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.985988, mae: 2.503106, mean_q: 4.646802, mean_eps: 0.200000\n",
            "  570/4000: episode: 46, duration: 0.149s, episode steps:  11, steps per second:  74, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 1.000019, mae: 2.546591, mean_q: 4.737668, mean_eps: 0.200000\n",
            "  579/4000: episode: 47, duration: 0.109s, episode steps:   9, steps per second:  83, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.887249, mae: 2.559954, mean_q: 4.741427, mean_eps: 0.200000\n",
            "  588/4000: episode: 48, duration: 0.126s, episode steps:   9, steps per second:  71, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.848312, mae: 2.560657, mean_q: 4.781374, mean_eps: 0.200000\n",
            "  599/4000: episode: 49, duration: 0.129s, episode steps:  11, steps per second:  85, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 1.133012, mae: 2.660284, mean_q: 4.856731, mean_eps: 0.200000\n",
            "  610/4000: episode: 50, duration: 0.138s, episode steps:  11, steps per second:  80, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 1.018575, mae: 2.682300, mean_q: 4.824761, mean_eps: 0.200000\n",
            "  619/4000: episode: 51, duration: 0.119s, episode steps:   9, steps per second:  75, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.790676, mae: 2.646927, mean_q: 4.933052, mean_eps: 0.200000\n",
            "  632/4000: episode: 52, duration: 0.163s, episode steps:  13, steps per second:  80, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.846 [0.000, 1.000],  loss: 0.997607, mae: 2.733893, mean_q: 5.026156, mean_eps: 0.200000\n",
            "  642/4000: episode: 53, duration: 0.130s, episode steps:  10, steps per second:  77, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.842227, mae: 2.758850, mean_q: 5.082844, mean_eps: 0.200000\n",
            "  652/4000: episode: 54, duration: 0.133s, episode steps:  10, steps per second:  75, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.029072, mae: 2.803331, mean_q: 5.134372, mean_eps: 0.200000\n",
            "  665/4000: episode: 55, duration: 0.178s, episode steps:  13, steps per second:  73, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.885599, mae: 2.818138, mean_q: 5.100231, mean_eps: 0.200000\n",
            "  674/4000: episode: 56, duration: 0.130s, episode steps:   9, steps per second:  69, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 1.029944, mae: 2.847483, mean_q: 5.119231, mean_eps: 0.200000\n",
            "  692/4000: episode: 57, duration: 0.251s, episode steps:  18, steps per second:  72, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.801883, mae: 2.833762, mean_q: 5.155764, mean_eps: 0.200000\n",
            "  705/4000: episode: 58, duration: 0.158s, episode steps:  13, steps per second:  82, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.752602, mae: 2.889930, mean_q: 5.327864, mean_eps: 0.200000\n",
            "  714/4000: episode: 59, duration: 0.116s, episode steps:   9, steps per second:  78, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.819813, mae: 2.943130, mean_q: 5.399490, mean_eps: 0.200000\n",
            "  723/4000: episode: 60, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.772124, mae: 2.965805, mean_q: 5.430414, mean_eps: 0.200000\n",
            "  733/4000: episode: 61, duration: 0.137s, episode steps:  10, steps per second:  73, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.824428, mae: 2.989048, mean_q: 5.429227, mean_eps: 0.200000\n",
            "  743/4000: episode: 62, duration: 0.134s, episode steps:  10, steps per second:  74, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.532598, mae: 2.952793, mean_q: 5.543435, mean_eps: 0.200000\n",
            "  754/4000: episode: 63, duration: 0.139s, episode steps:  11, steps per second:  79, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.825217, mae: 3.065892, mean_q: 5.604860, mean_eps: 0.200000\n",
            "  766/4000: episode: 64, duration: 0.147s, episode steps:  12, steps per second:  81, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.701675, mae: 3.060301, mean_q: 5.672793, mean_eps: 0.200000\n",
            "  777/4000: episode: 65, duration: 0.334s, episode steps:  11, steps per second:  33, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.901125, mae: 3.123514, mean_q: 5.632284, mean_eps: 0.200000\n",
            "  789/4000: episode: 66, duration: 0.290s, episode steps:  12, steps per second:  41, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.621537, mae: 3.094772, mean_q: 5.681490, mean_eps: 0.200000\n",
            "  798/4000: episode: 67, duration: 0.212s, episode steps:   9, steps per second:  42, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.786706, mae: 3.154951, mean_q: 5.674365, mean_eps: 0.200000\n",
            "  809/4000: episode: 68, duration: 0.238s, episode steps:  11, steps per second:  46, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.872309, mae: 3.193082, mean_q: 5.711658, mean_eps: 0.200000\n",
            "  823/4000: episode: 69, duration: 0.202s, episode steps:  14, steps per second:  69, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.706392, mae: 3.192605, mean_q: 5.770623, mean_eps: 0.200000\n",
            "  832/4000: episode: 70, duration: 0.131s, episode steps:   9, steps per second:  69, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.794477, mae: 3.234998, mean_q: 5.867947, mean_eps: 0.200000\n",
            "  843/4000: episode: 71, duration: 0.160s, episode steps:  11, steps per second:  69, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.821466, mae: 3.292081, mean_q: 5.964297, mean_eps: 0.200000\n",
            "  870/4000: episode: 72, duration: 0.371s, episode steps:  27, steps per second:  73, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.593 [0.000, 1.000],  loss: 0.787185, mae: 3.331538, mean_q: 5.992899, mean_eps: 0.200000\n",
            "  887/4000: episode: 73, duration: 0.234s, episode steps:  17, steps per second:  73, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.829716, mae: 3.356792, mean_q: 6.094388, mean_eps: 0.200000\n",
            "  902/4000: episode: 74, duration: 0.282s, episode steps:  15, steps per second:  53, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.597336, mae: 3.364920, mean_q: 6.196566, mean_eps: 0.200000\n",
            "  911/4000: episode: 75, duration: 0.156s, episode steps:   9, steps per second:  58, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.565074, mae: 3.385219, mean_q: 6.265640, mean_eps: 0.200000\n",
            "  932/4000: episode: 76, duration: 0.379s, episode steps:  21, steps per second:  55, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.733911, mae: 3.455568, mean_q: 6.324381, mean_eps: 0.200000\n",
            "  943/4000: episode: 77, duration: 0.171s, episode steps:  11, steps per second:  64, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 1.040540, mae: 3.556436, mean_q: 6.352627, mean_eps: 0.200000\n",
            "  976/4000: episode: 78, duration: 0.403s, episode steps:  33, steps per second:  82, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 0.860353, mae: 3.567709, mean_q: 6.431856, mean_eps: 0.200000\n",
            " 1022/4000: episode: 79, duration: 0.555s, episode steps:  46, steps per second:  83, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.766681, mae: 3.614308, mean_q: 6.600209, mean_eps: 0.200000\n",
            " 1056/4000: episode: 80, duration: 0.405s, episode steps:  34, steps per second:  84, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.879373, mae: 3.743644, mean_q: 6.832322, mean_eps: 0.200000\n",
            " 1101/4000: episode: 81, duration: 0.538s, episode steps:  45, steps per second:  84, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 0.689612, mae: 3.835933, mean_q: 7.120667, mean_eps: 0.200000\n",
            " 1111/4000: episode: 82, duration: 0.178s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.818111, mae: 3.934007, mean_q: 7.349282, mean_eps: 0.200000\n",
            " 1132/4000: episode: 83, duration: 0.372s, episode steps:  21, steps per second:  56, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.672612, mae: 3.919559, mean_q: 7.308434, mean_eps: 0.200000\n",
            " 1156/4000: episode: 84, duration: 0.403s, episode steps:  24, steps per second:  59, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.101320, mae: 4.024197, mean_q: 7.349245, mean_eps: 0.200000\n",
            " 1167/4000: episode: 85, duration: 0.180s, episode steps:  11, steps per second:  61, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.926393, mae: 4.056322, mean_q: 7.444246, mean_eps: 0.200000\n",
            " 1175/4000: episode: 86, duration: 0.141s, episode steps:   8, steps per second:  57, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.125 [0.000, 1.000],  loss: 0.986982, mae: 4.119121, mean_q: 7.624796, mean_eps: 0.200000\n",
            " 1185/4000: episode: 87, duration: 0.175s, episode steps:  10, steps per second:  57, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 1.337383, mae: 4.221123, mean_q: 7.736977, mean_eps: 0.200000\n",
            " 1196/4000: episode: 88, duration: 0.192s, episode steps:  11, steps per second:  57, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.978664, mae: 4.185954, mean_q: 7.733466, mean_eps: 0.200000\n",
            " 1205/4000: episode: 89, duration: 0.198s, episode steps:   9, steps per second:  46, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 1.028363, mae: 4.248744, mean_q: 7.875426, mean_eps: 0.200000\n",
            " 1214/4000: episode: 90, duration: 0.200s, episode steps:   9, steps per second:  45, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.927114, mae: 4.254918, mean_q: 7.943855, mean_eps: 0.200000\n",
            " 1227/4000: episode: 91, duration: 0.176s, episode steps:  13, steps per second:  74, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.154 [0.000, 1.000],  loss: 1.051599, mae: 4.307367, mean_q: 8.020107, mean_eps: 0.200000\n",
            " 1237/4000: episode: 92, duration: 0.141s, episode steps:  10, steps per second:  71, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.982933, mae: 4.344877, mean_q: 8.091029, mean_eps: 0.200000\n",
            " 1250/4000: episode: 93, duration: 0.156s, episode steps:  13, steps per second:  83, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.154 [0.000, 1.000],  loss: 1.138260, mae: 4.425136, mean_q: 8.254618, mean_eps: 0.200000\n",
            " 1263/4000: episode: 94, duration: 0.163s, episode steps:  13, steps per second:  80, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 1.555864, mae: 4.544305, mean_q: 8.459794, mean_eps: 0.200000\n",
            " 1274/4000: episode: 95, duration: 0.147s, episode steps:  11, steps per second:  75, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 1.545002, mae: 4.586926, mean_q: 8.462686, mean_eps: 0.200000\n",
            " 1286/4000: episode: 96, duration: 0.155s, episode steps:  12, steps per second:  78, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.083 [0.000, 1.000],  loss: 1.358845, mae: 4.616969, mean_q: 8.596092, mean_eps: 0.200000\n",
            " 1297/4000: episode: 97, duration: 0.151s, episode steps:  11, steps per second:  73, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 1.090281, mae: 4.624346, mean_q: 8.735567, mean_eps: 0.200000\n",
            " 1308/4000: episode: 98, duration: 0.215s, episode steps:  11, steps per second:  51, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 1.331010, mae: 4.763527, mean_q: 9.029000, mean_eps: 0.200000\n",
            " 1318/4000: episode: 99, duration: 0.170s, episode steps:  10, steps per second:  59, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.348092, mae: 4.869792, mean_q: 9.051704, mean_eps: 0.200000\n",
            " 1332/4000: episode: 100, duration: 0.264s, episode steps:  14, steps per second:  53, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.143 [0.000, 1.000],  loss: 2.420724, mae: 4.990053, mean_q: 9.060464, mean_eps: 0.200000\n",
            " 1341/4000: episode: 101, duration: 0.177s, episode steps:   9, steps per second:  51, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 1.677252, mae: 4.900049, mean_q: 8.955050, mean_eps: 0.200000\n",
            " 1351/4000: episode: 102, duration: 0.199s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 2.597947, mae: 5.048701, mean_q: 9.050289, mean_eps: 0.200000\n",
            " 1360/4000: episode: 103, duration: 0.141s, episode steps:   9, steps per second:  64, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.909804, mae: 4.983836, mean_q: 9.071987, mean_eps: 0.200000\n",
            " 1370/4000: episode: 104, duration: 0.136s, episode steps:  10, steps per second:  73, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 1.865742, mae: 4.984857, mean_q: 9.090539, mean_eps: 0.200000\n",
            " 1381/4000: episode: 105, duration: 0.148s, episode steps:  11, steps per second:  74, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 2.231614, mae: 5.111471, mean_q: 9.312796, mean_eps: 0.200000\n",
            " 1389/4000: episode: 106, duration: 0.115s, episode steps:   8, steps per second:  69, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.855055, mae: 5.144467, mean_q: 9.440051, mean_eps: 0.200000\n",
            " 1403/4000: episode: 107, duration: 0.183s, episode steps:  14, steps per second:  76, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.214 [0.000, 1.000],  loss: 1.595935, mae: 5.129145, mean_q: 9.484007, mean_eps: 0.200000\n",
            " 1413/4000: episode: 108, duration: 0.125s, episode steps:  10, steps per second:  80, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.269581, mae: 5.213608, mean_q: 9.550417, mean_eps: 0.200000\n",
            " 1421/4000: episode: 109, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.836832, mae: 5.138880, mean_q: 9.514043, mean_eps: 0.200000\n",
            " 1430/4000: episode: 110, duration: 0.123s, episode steps:   9, steps per second:  73, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.010315, mae: 5.269891, mean_q: 9.749461, mean_eps: 0.200000\n",
            " 1441/4000: episode: 111, duration: 0.136s, episode steps:  11, steps per second:  81, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 2.039629, mae: 5.266441, mean_q: 9.614218, mean_eps: 0.200000\n",
            " 1451/4000: episode: 112, duration: 0.125s, episode steps:  10, steps per second:  80, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 2.475782, mae: 5.316311, mean_q: 9.611363, mean_eps: 0.200000\n",
            " 1462/4000: episode: 113, duration: 0.141s, episode steps:  11, steps per second:  78, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 2.506441, mae: 5.371687, mean_q: 9.710843, mean_eps: 0.200000\n",
            " 1472/4000: episode: 114, duration: 0.132s, episode steps:  10, steps per second:  76, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 2.169956, mae: 5.348105, mean_q: 9.706176, mean_eps: 0.200000\n",
            " 1482/4000: episode: 115, duration: 0.136s, episode steps:  10, steps per second:  73, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 1.918494, mae: 5.416047, mean_q: 9.969274, mean_eps: 0.200000\n",
            " 1492/4000: episode: 116, duration: 0.127s, episode steps:  10, steps per second:  79, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 2.627377, mae: 5.478553, mean_q: 9.991900, mean_eps: 0.200000\n",
            " 1501/4000: episode: 117, duration: 0.112s, episode steps:   9, steps per second:  81, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 4.152007, mae: 5.714617, mean_q: 10.138890, mean_eps: 0.200000\n",
            " 1511/4000: episode: 118, duration: 0.136s, episode steps:  10, steps per second:  73, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 3.355585, mae: 5.670374, mean_q: 10.081288, mean_eps: 0.200000\n",
            " 1521/4000: episode: 119, duration: 0.123s, episode steps:  10, steps per second:  82, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 3.144167, mae: 5.582272, mean_q: 9.988121, mean_eps: 0.200000\n",
            " 1530/4000: episode: 120, duration: 0.144s, episode steps:   9, steps per second:  63, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 2.213123, mae: 5.548987, mean_q: 10.100791, mean_eps: 0.200000\n",
            " 1539/4000: episode: 121, duration: 0.128s, episode steps:   9, steps per second:  70, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 2.878464, mae: 5.635328, mean_q: 10.161752, mean_eps: 0.200000\n",
            " 1550/4000: episode: 122, duration: 0.138s, episode steps:  11, steps per second:  80, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 3.008391, mae: 5.740627, mean_q: 10.343907, mean_eps: 0.200000\n",
            " 1559/4000: episode: 123, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 2.850490, mae: 5.743248, mean_q: 10.458101, mean_eps: 0.200000\n",
            " 1569/4000: episode: 124, duration: 0.127s, episode steps:  10, steps per second:  79, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 3.191128, mae: 5.727003, mean_q: 10.413291, mean_eps: 0.200000\n",
            " 1581/4000: episode: 125, duration: 0.174s, episode steps:  12, steps per second:  69, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 3.187697, mae: 5.853415, mean_q: 10.585946, mean_eps: 0.200000\n",
            " 1591/4000: episode: 126, duration: 0.183s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 2.748740, mae: 5.799988, mean_q: 10.580033, mean_eps: 0.200000\n",
            " 1601/4000: episode: 127, duration: 0.178s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 3.221108, mae: 5.868073, mean_q: 10.574733, mean_eps: 0.200000\n",
            " 1611/4000: episode: 128, duration: 0.165s, episode steps:  10, steps per second:  60, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 2.820752, mae: 5.815537, mean_q: 10.553555, mean_eps: 0.200000\n",
            " 1621/4000: episode: 129, duration: 0.182s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 2.830649, mae: 5.859423, mean_q: 10.613879, mean_eps: 0.200000\n",
            " 1630/4000: episode: 130, duration: 0.190s, episode steps:   9, steps per second:  47, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 2.985707, mae: 5.890265, mean_q: 10.736775, mean_eps: 0.200000\n",
            " 1640/4000: episode: 131, duration: 0.143s, episode steps:  10, steps per second:  70, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 3.245456, mae: 5.960416, mean_q: 10.789262, mean_eps: 0.200000\n",
            " 1648/4000: episode: 132, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.756214, mae: 5.972304, mean_q: 10.857800, mean_eps: 0.200000\n",
            " 1658/4000: episode: 133, duration: 0.129s, episode steps:  10, steps per second:  78, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 2.272650, mae: 5.913414, mean_q: 10.832133, mean_eps: 0.200000\n",
            " 1673/4000: episode: 134, duration: 0.192s, episode steps:  15, steps per second:  78, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 2.855967, mae: 5.974633, mean_q: 10.936022, mean_eps: 0.200000\n",
            " 1681/4000: episode: 135, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.208032, mae: 5.938475, mean_q: 11.011657, mean_eps: 0.200000\n",
            " 1694/4000: episode: 136, duration: 0.166s, episode steps:  13, steps per second:  78, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 2.310442, mae: 5.977603, mean_q: 11.127027, mean_eps: 0.200000\n",
            " 1705/4000: episode: 137, duration: 0.148s, episode steps:  11, steps per second:  74, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 4.677696, mae: 6.231143, mean_q: 11.061796, mean_eps: 0.200000\n",
            " 1716/4000: episode: 138, duration: 0.161s, episode steps:  11, steps per second:  68, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 3.941779, mae: 6.123574, mean_q: 10.825611, mean_eps: 0.200000\n",
            " 1728/4000: episode: 139, duration: 0.215s, episode steps:  12, steps per second:  56, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 2.907833, mae: 6.089980, mean_q: 10.898802, mean_eps: 0.200000\n",
            " 1737/4000: episode: 140, duration: 0.177s, episode steps:   9, steps per second:  51, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 3.874644, mae: 6.218801, mean_q: 11.021297, mean_eps: 0.200000\n",
            " 1746/4000: episode: 141, duration: 0.212s, episode steps:   9, steps per second:  42, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 4.609675, mae: 6.273657, mean_q: 11.014861, mean_eps: 0.200000\n",
            " 1756/4000: episode: 142, duration: 0.195s, episode steps:  10, steps per second:  51, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 2.996235, mae: 6.142678, mean_q: 11.041569, mean_eps: 0.200000\n",
            " 1769/4000: episode: 143, duration: 0.251s, episode steps:  13, steps per second:  52, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 3.329119, mae: 6.161735, mean_q: 10.992020, mean_eps: 0.200000\n",
            " 1781/4000: episode: 144, duration: 0.238s, episode steps:  12, steps per second:  51, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 3.792488, mae: 6.252963, mean_q: 11.176534, mean_eps: 0.200000\n",
            " 1790/4000: episode: 145, duration: 0.162s, episode steps:   9, steps per second:  56, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 2.692198, mae: 6.140417, mean_q: 11.180868, mean_eps: 0.200000\n",
            " 1802/4000: episode: 146, duration: 0.206s, episode steps:  12, steps per second:  58, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 3.922883, mae: 6.264666, mean_q: 11.201525, mean_eps: 0.200000\n",
            " 1813/4000: episode: 147, duration: 0.194s, episode steps:  11, steps per second:  57, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 3.098191, mae: 6.223474, mean_q: 11.212445, mean_eps: 0.200000\n",
            " 1823/4000: episode: 148, duration: 0.175s, episode steps:  10, steps per second:  57, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 2.878559, mae: 6.184194, mean_q: 11.251014, mean_eps: 0.200000\n",
            " 1836/4000: episode: 149, duration: 0.227s, episode steps:  13, steps per second:  57, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 4.167899, mae: 6.392400, mean_q: 11.389999, mean_eps: 0.200000\n",
            " 1847/4000: episode: 150, duration: 0.187s, episode steps:  11, steps per second:  59, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 2.874569, mae: 6.272785, mean_q: 11.417935, mean_eps: 0.200000\n",
            " 1858/4000: episode: 151, duration: 0.205s, episode steps:  11, steps per second:  54, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 2.948162, mae: 6.281019, mean_q: 11.434245, mean_eps: 0.200000\n",
            " 1868/4000: episode: 152, duration: 0.200s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 3.236112, mae: 6.309996, mean_q: 11.476395, mean_eps: 0.200000\n",
            " 1879/4000: episode: 153, duration: 0.179s, episode steps:  11, steps per second:  61, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 4.331401, mae: 6.443836, mean_q: 11.527423, mean_eps: 0.200000\n",
            " 1889/4000: episode: 154, duration: 0.128s, episode steps:  10, steps per second:  78, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 4.468380, mae: 6.456897, mean_q: 11.434722, mean_eps: 0.200000\n",
            " 1908/4000: episode: 155, duration: 0.227s, episode steps:  19, steps per second:  84, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 3.688526, mae: 6.403663, mean_q: 11.364853, mean_eps: 0.200000\n",
            " 1920/4000: episode: 156, duration: 0.139s, episode steps:  12, steps per second:  86, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 5.018241, mae: 6.502025, mean_q: 11.339020, mean_eps: 0.200000\n",
            " 1930/4000: episode: 157, duration: 0.148s, episode steps:  10, steps per second:  67, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 4.518615, mae: 6.459450, mean_q: 11.247232, mean_eps: 0.200000\n",
            " 1942/4000: episode: 158, duration: 0.168s, episode steps:  12, steps per second:  71, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 2.103689, mae: 6.232960, mean_q: 11.260441, mean_eps: 0.200000\n",
            " 1952/4000: episode: 159, duration: 0.134s, episode steps:  10, steps per second:  74, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 2.992563, mae: 6.350155, mean_q: 11.532988, mean_eps: 0.200000\n",
            " 1967/4000: episode: 160, duration: 0.184s, episode steps:  15, steps per second:  82, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 3.507854, mae: 6.413648, mean_q: 11.600993, mean_eps: 0.200000\n",
            " 1979/4000: episode: 161, duration: 0.155s, episode steps:  12, steps per second:  77, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 4.503139, mae: 6.495336, mean_q: 11.451030, mean_eps: 0.200000\n",
            " 1994/4000: episode: 162, duration: 0.194s, episode steps:  15, steps per second:  77, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 3.807012, mae: 6.438050, mean_q: 11.412464, mean_eps: 0.200000\n",
            " 2007/4000: episode: 163, duration: 0.165s, episode steps:  13, steps per second:  79, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 3.338644, mae: 6.380048, mean_q: 11.377266, mean_eps: 0.200000\n",
            " 2019/4000: episode: 164, duration: 0.170s, episode steps:  12, steps per second:  71, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.637982, mae: 6.477192, mean_q: 11.578738, mean_eps: 0.200000\n",
            " 2033/4000: episode: 165, duration: 0.175s, episode steps:  14, steps per second:  80, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.755553, mae: 6.331370, mean_q: 11.471859, mean_eps: 0.200000\n",
            " 2048/4000: episode: 166, duration: 0.291s, episode steps:  15, steps per second:  52, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 3.032733, mae: 6.408586, mean_q: 11.672629, mean_eps: 0.200000\n",
            " 2065/4000: episode: 167, duration: 0.309s, episode steps:  17, steps per second:  55, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 3.869412, mae: 6.447385, mean_q: 11.495850, mean_eps: 0.200000\n",
            " 2093/4000: episode: 168, duration: 0.484s, episode steps:  28, steps per second:  58, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 2.778915, mae: 6.373131, mean_q: 11.554418, mean_eps: 0.200000\n",
            " 2118/4000: episode: 169, duration: 0.409s, episode steps:  25, steps per second:  61, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 3.230239, mae: 6.423668, mean_q: 11.636168, mean_eps: 0.200000\n",
            " 2140/4000: episode: 170, duration: 0.365s, episode steps:  22, steps per second:  60, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.888116, mae: 6.493870, mean_q: 11.534492, mean_eps: 0.200000\n",
            " 2166/4000: episode: 171, duration: 0.472s, episode steps:  26, steps per second:  55, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.135299, mae: 6.429490, mean_q: 11.568814, mean_eps: 0.200000\n",
            " 2185/4000: episode: 172, duration: 0.307s, episode steps:  19, steps per second:  62, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2.771597, mae: 6.411407, mean_q: 11.698077, mean_eps: 0.200000\n",
            " 2254/4000: episode: 173, duration: 1.078s, episode steps:  69, steps per second:  64, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 3.552226, mae: 6.498198, mean_q: 11.654901, mean_eps: 0.200000\n",
            " 2278/4000: episode: 174, duration: 0.402s, episode steps:  24, steps per second:  60, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.451722, mae: 6.477960, mean_q: 11.551496, mean_eps: 0.200000\n",
            " 2301/4000: episode: 175, duration: 0.301s, episode steps:  23, steps per second:  76, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 2.350636, mae: 6.407950, mean_q: 11.699201, mean_eps: 0.200000\n",
            " 2330/4000: episode: 176, duration: 0.337s, episode steps:  29, steps per second:  86, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 3.605586, mae: 6.530579, mean_q: 11.829649, mean_eps: 0.200000\n",
            " 2376/4000: episode: 177, duration: 0.559s, episode steps:  46, steps per second:  82, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.940128, mae: 6.517730, mean_q: 11.815037, mean_eps: 0.200000\n",
            " 2461/4000: episode: 178, duration: 1.125s, episode steps:  85, steps per second:  76, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 3.025096, mae: 6.507311, mean_q: 11.833423, mean_eps: 0.200000\n",
            " 2483/4000: episode: 179, duration: 0.392s, episode steps:  22, steps per second:  56, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.776095, mae: 6.546158, mean_q: 12.027101, mean_eps: 0.200000\n",
            " 2512/4000: episode: 180, duration: 0.515s, episode steps:  29, steps per second:  56, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 2.523121, mae: 6.567264, mean_q: 12.137833, mean_eps: 0.200000\n",
            " 2538/4000: episode: 181, duration: 0.419s, episode steps:  26, steps per second:  62, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.443507, mae: 6.603305, mean_q: 11.935188, mean_eps: 0.200000\n",
            " 2570/4000: episode: 182, duration: 0.593s, episode steps:  32, steps per second:  54, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.779505, mae: 6.606414, mean_q: 12.104201, mean_eps: 0.200000\n",
            " 2628/4000: episode: 183, duration: 0.988s, episode steps:  58, steps per second:  59, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 2.796931, mae: 6.656297, mean_q: 12.240582, mean_eps: 0.200000\n",
            " 2667/4000: episode: 184, duration: 0.626s, episode steps:  39, steps per second:  62, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 3.233401, mae: 6.721301, mean_q: 12.251986, mean_eps: 0.200000\n",
            " 2723/4000: episode: 185, duration: 0.956s, episode steps:  56, steps per second:  59, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.738175, mae: 6.681070, mean_q: 12.259691, mean_eps: 0.200000\n",
            " 2770/4000: episode: 186, duration: 0.730s, episode steps:  47, steps per second:  64, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 2.836246, mae: 6.753185, mean_q: 12.430753, mean_eps: 0.200000\n",
            " 2847/4000: episode: 187, duration: 1.096s, episode steps:  77, steps per second:  70, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 2.574576, mae: 6.781589, mean_q: 12.575741, mean_eps: 0.200000\n",
            " 2894/4000: episode: 188, duration: 0.727s, episode steps:  47, steps per second:  65, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 2.173022, mae: 6.822624, mean_q: 12.767547, mean_eps: 0.200000\n",
            " 2948/4000: episode: 189, duration: 0.843s, episode steps:  54, steps per second:  64, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 2.519114, mae: 6.919381, mean_q: 12.872160, mean_eps: 0.200000\n",
            " 3020/4000: episode: 190, duration: 1.096s, episode steps:  72, steps per second:  66, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 2.680387, mae: 7.010095, mean_q: 13.025288, mean_eps: 0.200000\n",
            " 3050/4000: episode: 191, duration: 0.348s, episode steps:  30, steps per second:  86, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 2.690504, mae: 7.030988, mean_q: 13.066053, mean_eps: 0.200000\n",
            " 3107/4000: episode: 192, duration: 0.732s, episode steps:  57, steps per second:  78, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 2.626191, mae: 7.112538, mean_q: 13.232706, mean_eps: 0.200000\n",
            " 3146/4000: episode: 193, duration: 0.604s, episode steps:  39, steps per second:  65, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 2.454655, mae: 7.148053, mean_q: 13.405782, mean_eps: 0.200000\n",
            " 3197/4000: episode: 194, duration: 0.656s, episode steps:  51, steps per second:  78, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 2.305066, mae: 7.196260, mean_q: 13.510550, mean_eps: 0.200000\n",
            " 3250/4000: episode: 195, duration: 0.617s, episode steps:  53, steps per second:  86, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 2.318364, mae: 7.277890, mean_q: 13.669209, mean_eps: 0.200000\n",
            " 3283/4000: episode: 196, duration: 0.375s, episode steps:  33, steps per second:  88, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 2.683682, mae: 7.447124, mean_q: 13.978401, mean_eps: 0.200000\n",
            " 3330/4000: episode: 197, duration: 0.551s, episode steps:  47, steps per second:  85, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 2.427314, mae: 7.463972, mean_q: 13.973477, mean_eps: 0.200000\n",
            " 3363/4000: episode: 198, duration: 0.381s, episode steps:  33, steps per second:  87, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 2.003609, mae: 7.485347, mean_q: 14.168409, mean_eps: 0.200000\n",
            " 3400/4000: episode: 199, duration: 0.441s, episode steps:  37, steps per second:  84, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 3.254697, mae: 7.634891, mean_q: 14.181516, mean_eps: 0.200000\n",
            " 3437/4000: episode: 200, duration: 0.442s, episode steps:  37, steps per second:  84, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 2.514777, mae: 7.631323, mean_q: 14.331255, mean_eps: 0.200000\n",
            " 3491/4000: episode: 201, duration: 0.628s, episode steps:  54, steps per second:  86, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.975886, mae: 7.715346, mean_q: 14.444643, mean_eps: 0.200000\n",
            " 3532/4000: episode: 202, duration: 0.478s, episode steps:  41, steps per second:  86, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 3.133748, mae: 7.793402, mean_q: 14.580737, mean_eps: 0.200000\n",
            " 3576/4000: episode: 203, duration: 0.523s, episode steps:  44, steps per second:  84, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 3.023173, mae: 7.887201, mean_q: 14.797414, mean_eps: 0.200000\n",
            " 3614/4000: episode: 204, duration: 0.418s, episode steps:  38, steps per second:  91, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.845516, mae: 7.884459, mean_q: 14.797112, mean_eps: 0.200000\n",
            " 3748/4000: episode: 205, duration: 1.470s, episode steps: 134, steps per second:  91, episode reward: 134.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 3.016764, mae: 7.999053, mean_q: 15.024226, mean_eps: 0.200000\n",
            " 3778/4000: episode: 206, duration: 0.358s, episode steps:  30, steps per second:  84, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 2.641574, mae: 8.182103, mean_q: 15.478601, mean_eps: 0.200000\n",
            " 3828/4000: episode: 207, duration: 0.738s, episode steps:  50, steps per second:  68, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 3.486221, mae: 8.256557, mean_q: 15.464907, mean_eps: 0.200000\n",
            " 3870/4000: episode: 208, duration: 0.687s, episode steps:  42, steps per second:  61, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 2.879864, mae: 8.290290, mean_q: 15.677353, mean_eps: 0.200000\n",
            " 3909/4000: episode: 209, duration: 0.651s, episode steps:  39, steps per second:  60, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 3.166097, mae: 8.441880, mean_q: 15.909541, mean_eps: 0.200000\n",
            " 3937/4000: episode: 210, duration: 0.340s, episode steps:  28, steps per second:  82, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 2.731440, mae: 8.355042, mean_q: 15.839477, mean_eps: 0.200000\n",
            " 3972/4000: episode: 211, duration: 0.434s, episode steps:  35, steps per second:  81, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 3.515545, mae: 8.481320, mean_q: 16.037656, mean_eps: 0.200000\n",
            " 4000/4000: episode: 212, duration: 0.332s, episode steps:  28, steps per second:  84, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.729735, mae: 8.659801, mean_q: 16.341548, mean_eps: 0.200000\n",
            "done, took 67.775 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 44.000, steps: 44\n",
            "Episode 2: reward: 28.000, steps: 28\n",
            "Episode 3: reward: 37.000, steps: 37\n",
            "Episode 4: reward: 26.000, steps: 26\n",
            "Episode 5: reward: 38.000, steps: 38\n",
            "Episode 6: reward: 38.000, steps: 38\n",
            "Episode 7: reward: 28.000, steps: 28\n",
            "Episode 8: reward: 24.000, steps: 24\n",
            "Episode 9: reward: 30.000, steps: 30\n",
            "Episode 10: reward: 38.000, steps: 38\n",
            "Episode 11: reward: 30.000, steps: 30\n",
            "Episode 12: reward: 36.000, steps: 36\n",
            "Episode 13: reward: 42.000, steps: 42\n",
            "Episode 14: reward: 23.000, steps: 23\n",
            "Episode 15: reward: 32.000, steps: 32\n",
            "Episode 16: reward: 26.000, steps: 26\n",
            "Episode 17: reward: 27.000, steps: 27\n",
            "Episode 18: reward: 30.000, steps: 30\n",
            "Episode 19: reward: 44.000, steps: 44\n",
            "Episode 20: reward: 35.000, steps: 35\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb1725cadd0>"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    }
  ]
}