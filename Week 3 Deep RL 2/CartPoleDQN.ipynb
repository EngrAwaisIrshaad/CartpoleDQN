{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPoleDQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Megacity1/CartpoleDQN/blob/main/Week%203%20Deep%20RL%202/CartPoleDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Deep Q Network (DQN) for CartPole Using Boltzmann Q Policy\n",
        "This exercise implements a DQN for CartPole using a Boltzmann Q policy for selecting the actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGsC7cJ5jNcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda73cbf-3020-40f4-e45e-7b87053448f1"
      },
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.24.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (13.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.44.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.10.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMIHLgQ3Z-lF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5a8608-456f-4127-ce8a-f551e5f4e4f3"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0AMLzq08ap0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b380e1-6c3d-4bce-8233-a8ac01deaf26"
      },
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "!pip install Adam\n",
        "#from keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Adam in /usr/local/lib/python3.7/dist-packages (0.0.0.dev0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll6bNdUm54WS"
      },
      "source": [
        "Implementation of DQN for CartPole, applying policy BoltzmannQPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCrPKNy40PC"
      },
      "source": [
        "##Implement DQN with BoltzmannGumbelQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efM9jkXr5A3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a5df65d9-e4c0-4cdf-b10c-fc73de8ff2ca"
      },
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import BoltzmannGumbelQPolicy\n",
        "from rl.policy import LinearAnnealedPolicy\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n",
        "# setup experience replay buffer\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
        "                               attr='eps',            \n",
        "                               value_max=10.,\n",
        "                               value_min=.10, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=200)\n",
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "# add extra layers here\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=20,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy) \n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=8000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_40\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_38 (Flatten)        (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_77 (Dense)            (None, 16)                80        \n",
            "                                                                 \n",
            " dense_78 (Dense)            (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 8000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   19/8000: episode: 1, duration: 2.357s, episode steps:  19, steps per second:   8, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   56/8000: episode: 2, duration: 9.489s, episode steps:  37, steps per second:   4, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 0.431406, mae: 0.533741, mean_q: 0.160572, mean_eps: 8.119000\n",
            "   71/8000: episode: 3, duration: 0.306s, episode steps:  15, steps per second:  49, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.340057, mae: 0.555431, mean_q: 0.331383, mean_eps: 6.881500\n",
            "   94/8000: episode: 4, duration: 0.362s, episode steps:  23, steps per second:  64, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 0.279172, mae: 0.579686, mean_q: 0.468595, mean_eps: 5.941000\n",
            "  108/8000: episode: 5, duration: 0.267s, episode steps:  14, steps per second:  52, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.245186, mae: 0.608629, mean_q: 0.589078, mean_eps: 5.025250\n",
            "  138/8000: episode: 6, duration: 0.429s, episode steps:  30, steps per second:  70, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.433 [0.000, 1.000],  loss: 0.234495, mae: 0.643371, mean_q: 0.696550, mean_eps: 3.936250\n",
            "  157/8000: episode: 7, duration: 0.398s, episode steps:  19, steps per second:  48, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.222419, mae: 0.688000, mean_q: 0.824300, mean_eps: 2.723500\n",
            "  168/8000: episode: 8, duration: 0.226s, episode steps:  11, steps per second:  49, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.213563, mae: 0.731514, mean_q: 0.916152, mean_eps: 1.981000\n",
            "  189/8000: episode: 9, duration: 0.383s, episode steps:  21, steps per second:  55, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.189808, mae: 0.774949, mean_q: 1.040753, mean_eps: 1.189000\n",
            "  197/8000: episode: 10, duration: 0.206s, episode steps:   8, steps per second:  39, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.174948, mae: 0.818249, mean_q: 1.161715, mean_eps: 0.471250\n",
            "  208/8000: episode: 11, duration: 0.262s, episode steps:  11, steps per second:  42, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.188091, mae: 0.862249, mean_q: 1.263211, mean_eps: 0.127000\n",
            "  218/8000: episode: 12, duration: 0.214s, episode steps:  10, steps per second:  47, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.174680, mae: 0.874177, mean_q: 1.292705, mean_eps: 0.100000\n",
            "  233/8000: episode: 13, duration: 0.269s, episode steps:  15, steps per second:  56, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.867 [0.000, 1.000],  loss: 0.192919, mae: 0.944537, mean_q: 1.447034, mean_eps: 0.100000\n",
            "  243/8000: episode: 14, duration: 0.189s, episode steps:  10, steps per second:  53, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.212422, mae: 1.007284, mean_q: 1.590234, mean_eps: 0.100000\n",
            "  253/8000: episode: 15, duration: 0.200s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.206295, mae: 1.013791, mean_q: 1.595380, mean_eps: 0.100000\n",
            "  282/8000: episode: 16, duration: 0.555s, episode steps:  29, steps per second:  52, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.655 [0.000, 1.000],  loss: 0.231381, mae: 1.108106, mean_q: 1.770471, mean_eps: 0.100000\n",
            "  291/8000: episode: 17, duration: 0.173s, episode steps:   9, steps per second:  52, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.313189, mae: 1.194379, mean_q: 1.906750, mean_eps: 0.100000\n",
            "  301/8000: episode: 18, duration: 0.229s, episode steps:  10, steps per second:  44, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.242243, mae: 1.173091, mean_q: 1.878459, mean_eps: 0.100000\n",
            "  315/8000: episode: 19, duration: 0.256s, episode steps:  14, steps per second:  55, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.857 [0.000, 1.000],  loss: 0.193084, mae: 1.192155, mean_q: 1.983634, mean_eps: 0.100000\n",
            "  325/8000: episode: 20, duration: 0.197s, episode steps:  10, steps per second:  51, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.270207, mae: 1.296755, mean_q: 2.199250, mean_eps: 0.100000\n",
            "  334/8000: episode: 21, duration: 0.204s, episode steps:   9, steps per second:  44, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.264939, mae: 1.307911, mean_q: 2.203632, mean_eps: 0.100000\n",
            "  346/8000: episode: 22, duration: 0.262s, episode steps:  12, steps per second:  46, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 0.190952, mae: 1.334616, mean_q: 2.336694, mean_eps: 0.100000\n",
            "  356/8000: episode: 23, duration: 0.180s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.200729, mae: 1.365223, mean_q: 2.437953, mean_eps: 0.100000\n",
            "  366/8000: episode: 24, duration: 0.196s, episode steps:  10, steps per second:  51, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.320281, mae: 1.458744, mean_q: 2.623779, mean_eps: 0.100000\n",
            "  376/8000: episode: 25, duration: 0.206s, episode steps:  10, steps per second:  49, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.426649, mae: 1.534351, mean_q: 2.635137, mean_eps: 0.100000\n",
            "  388/8000: episode: 26, duration: 0.281s, episode steps:  12, steps per second:  43, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.245527, mae: 1.483915, mean_q: 2.587052, mean_eps: 0.100000\n",
            "  396/8000: episode: 27, duration: 0.208s, episode steps:   8, steps per second:  39, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.468040, mae: 1.644868, mean_q: 2.807952, mean_eps: 0.100000\n",
            "  407/8000: episode: 28, duration: 0.260s, episode steps:  11, steps per second:  42, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.369374, mae: 1.623858, mean_q: 2.815102, mean_eps: 0.100000\n",
            "  417/8000: episode: 29, duration: 0.192s, episode steps:  10, steps per second:  52, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.467041, mae: 1.707790, mean_q: 2.957571, mean_eps: 0.100000\n",
            "  428/8000: episode: 30, duration: 0.204s, episode steps:  11, steps per second:  54, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.435804, mae: 1.731719, mean_q: 3.010885, mean_eps: 0.100000\n",
            "  438/8000: episode: 31, duration: 0.185s, episode steps:  10, steps per second:  54, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.438820, mae: 1.742487, mean_q: 3.015208, mean_eps: 0.100000\n",
            "  450/8000: episode: 32, duration: 0.231s, episode steps:  12, steps per second:  52, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 0.420380, mae: 1.813233, mean_q: 3.135057, mean_eps: 0.100000\n",
            "  460/8000: episode: 33, duration: 0.178s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.429614, mae: 1.841571, mean_q: 3.298154, mean_eps: 0.100000\n",
            "  469/8000: episode: 34, duration: 0.173s, episode steps:   9, steps per second:  52, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.373334, mae: 1.833072, mean_q: 3.334463, mean_eps: 0.100000\n",
            "  479/8000: episode: 35, duration: 0.185s, episode steps:  10, steps per second:  54, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.509920, mae: 1.920900, mean_q: 3.458858, mean_eps: 0.100000\n",
            "  490/8000: episode: 36, duration: 0.219s, episode steps:  11, steps per second:  50, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.430758, mae: 1.927047, mean_q: 3.476927, mean_eps: 0.100000\n",
            "  500/8000: episode: 37, duration: 0.165s, episode steps:  10, steps per second:  60, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.551145, mae: 2.010074, mean_q: 3.553299, mean_eps: 0.100000\n",
            "  511/8000: episode: 38, duration: 0.192s, episode steps:  11, steps per second:  57, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.534033, mae: 2.035109, mean_q: 3.550007, mean_eps: 0.100000\n",
            "  520/8000: episode: 39, duration: 0.140s, episode steps:   9, steps per second:  65, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.421739, mae: 2.013058, mean_q: 3.522175, mean_eps: 0.100000\n",
            "  531/8000: episode: 40, duration: 0.166s, episode steps:  11, steps per second:  66, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.507682, mae: 2.074715, mean_q: 3.635455, mean_eps: 0.100000\n",
            "  543/8000: episode: 41, duration: 0.171s, episode steps:  12, steps per second:  70, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 0.493054, mae: 2.106788, mean_q: 3.747873, mean_eps: 0.100000\n",
            "  553/8000: episode: 42, duration: 0.157s, episode steps:  10, steps per second:  64, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.456439, mae: 2.130827, mean_q: 3.857417, mean_eps: 0.100000\n",
            "  563/8000: episode: 43, duration: 0.184s, episode steps:  10, steps per second:  54, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.498031, mae: 2.178276, mean_q: 3.998264, mean_eps: 0.100000\n",
            "  572/8000: episode: 44, duration: 0.218s, episode steps:   9, steps per second:  41, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.643213, mae: 2.247701, mean_q: 3.970988, mean_eps: 0.100000\n",
            "  581/8000: episode: 45, duration: 0.210s, episode steps:   9, steps per second:  43, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.409847, mae: 2.199022, mean_q: 3.952870, mean_eps: 0.100000\n",
            "  591/8000: episode: 46, duration: 0.180s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.405698, mae: 2.211819, mean_q: 4.057426, mean_eps: 0.100000\n",
            "  601/8000: episode: 47, duration: 0.185s, episode steps:  10, steps per second:  54, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.446761, mae: 2.240274, mean_q: 4.137886, mean_eps: 0.100000\n",
            "  612/8000: episode: 48, duration: 0.196s, episode steps:  11, steps per second:  56, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.406149, mae: 2.244485, mean_q: 4.225940, mean_eps: 0.100000\n",
            "  622/8000: episode: 49, duration: 0.263s, episode steps:  10, steps per second:  38, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.520003, mae: 2.289709, mean_q: 4.196664, mean_eps: 0.100000\n",
            "  634/8000: episode: 50, duration: 0.256s, episode steps:  12, steps per second:  47, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.408414, mae: 2.264996, mean_q: 4.235767, mean_eps: 0.100000\n",
            "  643/8000: episode: 51, duration: 0.167s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.397687, mae: 2.306082, mean_q: 4.391852, mean_eps: 0.100000\n",
            "  653/8000: episode: 52, duration: 0.220s, episode steps:  10, steps per second:  45, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.473947, mae: 2.335223, mean_q: 4.427837, mean_eps: 0.100000\n",
            "  662/8000: episode: 53, duration: 0.227s, episode steps:   9, steps per second:  40, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.389691, mae: 2.361479, mean_q: 4.492496, mean_eps: 0.100000\n",
            "  672/8000: episode: 54, duration: 0.244s, episode steps:  10, steps per second:  41, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.314701, mae: 2.363431, mean_q: 4.567314, mean_eps: 0.100000\n",
            "  683/8000: episode: 55, duration: 0.239s, episode steps:  11, steps per second:  46, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.452521, mae: 2.426655, mean_q: 4.543053, mean_eps: 0.100000\n",
            "  693/8000: episode: 56, duration: 0.187s, episode steps:  10, steps per second:  53, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.413378, mae: 2.464939, mean_q: 4.598128, mean_eps: 0.100000\n",
            "  704/8000: episode: 57, duration: 0.214s, episode steps:  11, steps per second:  51, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.412447, mae: 2.506913, mean_q: 4.671226, mean_eps: 0.100000\n",
            "  714/8000: episode: 58, duration: 0.214s, episode steps:  10, steps per second:  47, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.399297, mae: 2.544533, mean_q: 4.753912, mean_eps: 0.100000\n",
            "  723/8000: episode: 59, duration: 0.179s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.268158, mae: 2.535006, mean_q: 4.799127, mean_eps: 0.100000\n",
            "  731/8000: episode: 60, duration: 0.190s, episode steps:   8, steps per second:  42, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.349456, mae: 2.592232, mean_q: 4.939881, mean_eps: 0.100000\n",
            "  739/8000: episode: 61, duration: 0.186s, episode steps:   8, steps per second:  43, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.465427, mae: 2.625123, mean_q: 4.915084, mean_eps: 0.100000\n",
            "  748/8000: episode: 62, duration: 0.195s, episode steps:   9, steps per second:  46, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.361024, mae: 2.624843, mean_q: 4.880758, mean_eps: 0.100000\n",
            "  758/8000: episode: 63, duration: 0.154s, episode steps:  10, steps per second:  65, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.390248, mae: 2.653498, mean_q: 4.892387, mean_eps: 0.100000\n",
            "  769/8000: episode: 64, duration: 0.154s, episode steps:  11, steps per second:  71, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.391306, mae: 2.651682, mean_q: 4.938086, mean_eps: 0.100000\n",
            "  777/8000: episode: 65, duration: 0.112s, episode steps:   8, steps per second:  72, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.875 [0.000, 1.000],  loss: 0.589663, mae: 2.754408, mean_q: 5.070924, mean_eps: 0.100000\n",
            "  785/8000: episode: 66, duration: 0.121s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.875 [0.000, 1.000],  loss: 0.298378, mae: 2.672086, mean_q: 4.981677, mean_eps: 0.100000\n",
            "  796/8000: episode: 67, duration: 0.165s, episode steps:  11, steps per second:  67, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.415060, mae: 2.714383, mean_q: 5.047866, mean_eps: 0.100000\n",
            "  806/8000: episode: 68, duration: 0.155s, episode steps:  10, steps per second:  64, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.482136, mae: 2.769327, mean_q: 5.122134, mean_eps: 0.100000\n",
            "  817/8000: episode: 69, duration: 0.150s, episode steps:  11, steps per second:  73, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.340886, mae: 2.752875, mean_q: 5.077001, mean_eps: 0.100000\n",
            "  827/8000: episode: 70, duration: 0.159s, episode steps:  10, steps per second:  63, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.382167, mae: 2.763710, mean_q: 5.153121, mean_eps: 0.100000\n",
            "  835/8000: episode: 71, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.352277, mae: 2.797135, mean_q: 5.294221, mean_eps: 0.100000\n",
            "  844/8000: episode: 72, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.308406, mae: 2.798017, mean_q: 5.354036, mean_eps: 0.100000\n",
            "  855/8000: episode: 73, duration: 0.162s, episode steps:  11, steps per second:  68, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.298742, mae: 2.835384, mean_q: 5.383975, mean_eps: 0.100000\n",
            "  867/8000: episode: 74, duration: 0.185s, episode steps:  12, steps per second:  65, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.321596, mae: 2.895204, mean_q: 5.394165, mean_eps: 0.100000\n",
            "  878/8000: episode: 75, duration: 0.148s, episode steps:  11, steps per second:  74, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.383047, mae: 2.890705, mean_q: 5.380000, mean_eps: 0.100000\n",
            "  892/8000: episode: 76, duration: 0.206s, episode steps:  14, steps per second:  68, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.411460, mae: 2.942749, mean_q: 5.509811, mean_eps: 0.100000\n",
            "  902/8000: episode: 77, duration: 0.170s, episode steps:  10, steps per second:  59, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.344438, mae: 2.983114, mean_q: 5.733604, mean_eps: 0.100000\n",
            "  912/8000: episode: 78, duration: 0.146s, episode steps:  10, steps per second:  68, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.339970, mae: 2.886025, mean_q: 5.380603, mean_eps: 0.100000\n",
            "  923/8000: episode: 79, duration: 0.176s, episode steps:  11, steps per second:  63, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.340186, mae: 2.919932, mean_q: 5.340756, mean_eps: 0.100000\n",
            "  935/8000: episode: 80, duration: 0.173s, episode steps:  12, steps per second:  69, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.454637, mae: 3.018947, mean_q: 5.574426, mean_eps: 0.100000\n",
            "  946/8000: episode: 81, duration: 0.159s, episode steps:  11, steps per second:  69, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.312486, mae: 3.021207, mean_q: 5.690655, mean_eps: 0.100000\n",
            "  955/8000: episode: 82, duration: 0.135s, episode steps:   9, steps per second:  67, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.370706, mae: 3.042398, mean_q: 5.671136, mean_eps: 0.100000\n",
            "  970/8000: episode: 83, duration: 0.221s, episode steps:  15, steps per second:  68, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.387207, mae: 3.063166, mean_q: 5.673680, mean_eps: 0.100000\n",
            "  987/8000: episode: 84, duration: 0.238s, episode steps:  17, steps per second:  72, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.176 [0.000, 1.000],  loss: 0.521982, mae: 3.144640, mean_q: 5.773232, mean_eps: 0.100000\n",
            "  998/8000: episode: 85, duration: 0.150s, episode steps:  11, steps per second:  73, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.718305, mae: 3.242986, mean_q: 5.959770, mean_eps: 0.100000\n",
            " 1009/8000: episode: 86, duration: 0.166s, episode steps:  11, steps per second:  66, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.435751, mae: 3.155740, mean_q: 5.851030, mean_eps: 0.100000\n",
            " 1023/8000: episode: 87, duration: 0.194s, episode steps:  14, steps per second:  72, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.637120, mae: 3.272642, mean_q: 6.069910, mean_eps: 0.100000\n",
            " 1049/8000: episode: 88, duration: 0.365s, episode steps:  26, steps per second:  71, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.449851, mae: 3.320832, mean_q: 6.192338, mean_eps: 0.100000\n",
            " 1069/8000: episode: 89, duration: 0.290s, episode steps:  20, steps per second:  69, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.363011, mae: 3.355216, mean_q: 6.269947, mean_eps: 0.100000\n",
            " 1130/8000: episode: 90, duration: 0.817s, episode steps:  61, steps per second:  75, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.393 [0.000, 1.000],  loss: 0.552637, mae: 3.495185, mean_q: 6.550530, mean_eps: 0.100000\n",
            " 1138/8000: episode: 91, duration: 0.123s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.723541, mae: 3.741909, mean_q: 6.998055, mean_eps: 0.100000\n",
            " 1151/8000: episode: 92, duration: 0.188s, episode steps:  13, steps per second:  69, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.154 [0.000, 1.000],  loss: 0.528544, mae: 3.742930, mean_q: 7.059721, mean_eps: 0.100000\n",
            " 1160/8000: episode: 93, duration: 0.140s, episode steps:   9, steps per second:  65, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.476383, mae: 3.744604, mean_q: 7.071417, mean_eps: 0.100000\n",
            " 1171/8000: episode: 94, duration: 0.151s, episode steps:  11, steps per second:  73, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.405086, mae: 3.756134, mean_q: 7.153855, mean_eps: 0.100000\n",
            " 1185/8000: episode: 95, duration: 0.199s, episode steps:  14, steps per second:  70, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 1.020893, mae: 3.916043, mean_q: 7.300689, mean_eps: 0.100000\n",
            " 1201/8000: episode: 96, duration: 0.233s, episode steps:  16, steps per second:  69, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.564762, mae: 3.861272, mean_q: 7.214479, mean_eps: 0.100000\n",
            " 1211/8000: episode: 97, duration: 0.139s, episode steps:  10, steps per second:  72, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.886510, mae: 3.933737, mean_q: 7.414337, mean_eps: 0.100000\n",
            " 1224/8000: episode: 98, duration: 0.193s, episode steps:  13, steps per second:  67, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.459255, mae: 3.973018, mean_q: 7.563996, mean_eps: 0.100000\n",
            " 1235/8000: episode: 99, duration: 0.149s, episode steps:  11, steps per second:  74, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.480047, mae: 3.975350, mean_q: 7.515350, mean_eps: 0.100000\n",
            " 1246/8000: episode: 100, duration: 0.168s, episode steps:  11, steps per second:  65, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 1.428588, mae: 4.123144, mean_q: 7.629835, mean_eps: 0.100000\n",
            " 1259/8000: episode: 101, duration: 0.202s, episode steps:  13, steps per second:  64, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.154 [0.000, 1.000],  loss: 1.036538, mae: 4.088544, mean_q: 7.557178, mean_eps: 0.100000\n",
            " 1269/8000: episode: 102, duration: 0.156s, episode steps:  10, steps per second:  64, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.731659, mae: 4.120978, mean_q: 7.707828, mean_eps: 0.100000\n",
            " 1281/8000: episode: 103, duration: 0.179s, episode steps:  12, steps per second:  67, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.767803, mae: 4.083656, mean_q: 7.657862, mean_eps: 0.100000\n",
            " 1291/8000: episode: 104, duration: 0.144s, episode steps:  10, steps per second:  69, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.402938, mae: 4.296544, mean_q: 7.938936, mean_eps: 0.100000\n",
            " 1301/8000: episode: 105, duration: 0.160s, episode steps:  10, steps per second:  63, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.146527, mae: 4.242530, mean_q: 7.826357, mean_eps: 0.100000\n",
            " 1310/8000: episode: 106, duration: 0.119s, episode steps:   9, steps per second:  75, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.796232, mae: 4.157867, mean_q: 7.750428, mean_eps: 0.100000\n",
            " 1319/8000: episode: 107, duration: 0.147s, episode steps:   9, steps per second:  61, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 1.746766, mae: 4.339195, mean_q: 7.918976, mean_eps: 0.100000\n",
            " 1332/8000: episode: 108, duration: 0.171s, episode steps:  13, steps per second:  76, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.955639, mae: 4.246112, mean_q: 7.896148, mean_eps: 0.100000\n",
            " 1348/8000: episode: 109, duration: 0.209s, episode steps:  16, steps per second:  77, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.903121, mae: 4.354817, mean_q: 8.105147, mean_eps: 0.100000\n",
            " 1358/8000: episode: 110, duration: 0.151s, episode steps:  10, steps per second:  66, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 1.222582, mae: 4.449081, mean_q: 8.309281, mean_eps: 0.100000\n",
            " 1371/8000: episode: 111, duration: 0.189s, episode steps:  13, steps per second:  69, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1.078038, mae: 4.474177, mean_q: 8.311203, mean_eps: 0.100000\n",
            " 1386/8000: episode: 112, duration: 0.230s, episode steps:  15, steps per second:  65, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1.269712, mae: 4.518857, mean_q: 8.316173, mean_eps: 0.100000\n",
            " 1411/8000: episode: 113, duration: 0.373s, episode steps:  25, steps per second:  67, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 1.434637, mae: 4.573888, mean_q: 8.440445, mean_eps: 0.100000\n",
            " 1422/8000: episode: 114, duration: 0.151s, episode steps:  11, steps per second:  73, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 1.050945, mae: 4.557494, mean_q: 8.415779, mean_eps: 0.100000\n",
            " 1433/8000: episode: 115, duration: 0.173s, episode steps:  11, steps per second:  64, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 1.004431, mae: 4.575340, mean_q: 8.543655, mean_eps: 0.100000\n",
            " 1447/8000: episode: 116, duration: 0.214s, episode steps:  14, steps per second:  65, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 1.276284, mae: 4.587214, mean_q: 8.555186, mean_eps: 0.100000\n",
            " 1461/8000: episode: 117, duration: 0.329s, episode steps:  14, steps per second:  43, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 1.177234, mae: 4.696278, mean_q: 8.758924, mean_eps: 0.100000\n",
            " 1471/8000: episode: 118, duration: 0.244s, episode steps:  10, steps per second:  41, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.818990, mae: 4.642143, mean_q: 8.754005, mean_eps: 0.100000\n",
            " 1483/8000: episode: 119, duration: 0.259s, episode steps:  12, steps per second:  46, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 2.151730, mae: 4.862521, mean_q: 8.878170, mean_eps: 0.100000\n",
            " 1500/8000: episode: 120, duration: 0.408s, episode steps:  17, steps per second:  42, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1.550946, mae: 4.797639, mean_q: 8.766528, mean_eps: 0.100000\n",
            " 1532/8000: episode: 121, duration: 0.646s, episode steps:  32, steps per second:  50, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.405314, mae: 4.815334, mean_q: 8.866905, mean_eps: 0.100000\n",
            " 1562/8000: episode: 122, duration: 0.566s, episode steps:  30, steps per second:  53, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1.898757, mae: 4.937761, mean_q: 8.994944, mean_eps: 0.100000\n",
            " 1585/8000: episode: 123, duration: 0.504s, episode steps:  23, steps per second:  46, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 1.599454, mae: 4.884063, mean_q: 9.001728, mean_eps: 0.100000\n",
            " 1610/8000: episode: 124, duration: 0.463s, episode steps:  25, steps per second:  54, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 1.735527, mae: 4.964533, mean_q: 9.092736, mean_eps: 0.100000\n",
            " 1695/8000: episode: 125, duration: 1.596s, episode steps:  85, steps per second:  53, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 1.486763, mae: 5.048779, mean_q: 9.358248, mean_eps: 0.100000\n",
            " 1739/8000: episode: 126, duration: 0.639s, episode steps:  44, steps per second:  69, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 1.502019, mae: 5.212076, mean_q: 9.696201, mean_eps: 0.100000\n",
            " 1780/8000: episode: 127, duration: 0.569s, episode steps:  41, steps per second:  72, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 1.853427, mae: 5.279982, mean_q: 9.739180, mean_eps: 0.100000\n",
            " 1833/8000: episode: 128, duration: 0.777s, episode steps:  53, steps per second:  68, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 1.269668, mae: 5.347864, mean_q: 10.036847, mean_eps: 0.100000\n",
            " 1877/8000: episode: 129, duration: 0.840s, episode steps:  44, steps per second:  52, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.719802, mae: 5.527032, mean_q: 10.336808, mean_eps: 0.100000\n",
            " 1900/8000: episode: 130, duration: 0.483s, episode steps:  23, steps per second:  48, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 1.488423, mae: 5.537534, mean_q: 10.416157, mean_eps: 0.100000\n",
            " 1924/8000: episode: 131, duration: 0.485s, episode steps:  24, steps per second:  49, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.489183, mae: 5.613487, mean_q: 10.605235, mean_eps: 0.100000\n",
            " 1953/8000: episode: 132, duration: 0.600s, episode steps:  29, steps per second:  48, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 1.698016, mae: 5.656719, mean_q: 10.637593, mean_eps: 0.100000\n",
            " 1996/8000: episode: 133, duration: 0.842s, episode steps:  43, steps per second:  51, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 1.388513, mae: 5.714750, mean_q: 10.775450, mean_eps: 0.100000\n",
            " 2035/8000: episode: 134, duration: 0.556s, episode steps:  39, steps per second:  70, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1.731773, mae: 5.824925, mean_q: 10.941183, mean_eps: 0.100000\n",
            " 2063/8000: episode: 135, duration: 0.410s, episode steps:  28, steps per second:  68, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 1.588494, mae: 5.864807, mean_q: 11.069002, mean_eps: 0.100000\n",
            " 2118/8000: episode: 136, duration: 0.761s, episode steps:  55, steps per second:  72, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 1.438866, mae: 6.017366, mean_q: 11.439435, mean_eps: 0.100000\n",
            " 2148/8000: episode: 137, duration: 0.520s, episode steps:  30, steps per second:  58, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.763524, mae: 6.141102, mean_q: 11.630102, mean_eps: 0.100000\n",
            " 2202/8000: episode: 138, duration: 1.155s, episode steps:  54, steps per second:  47, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.050535, mae: 6.236490, mean_q: 11.709697, mean_eps: 0.100000\n",
            " 2268/8000: episode: 139, duration: 1.271s, episode steps:  66, steps per second:  52, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 1.905993, mae: 6.368565, mean_q: 12.034069, mean_eps: 0.100000\n",
            " 2297/8000: episode: 140, duration: 0.615s, episode steps:  29, steps per second:  47, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 2.039904, mae: 6.413357, mean_q: 12.133836, mean_eps: 0.100000\n",
            " 2318/8000: episode: 141, duration: 0.449s, episode steps:  21, steps per second:  47, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 2.732846, mae: 6.540568, mean_q: 12.182413, mean_eps: 0.100000\n",
            " 2340/8000: episode: 142, duration: 0.506s, episode steps:  22, steps per second:  44, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.197106, mae: 6.495730, mean_q: 12.213053, mean_eps: 0.100000\n",
            " 2364/8000: episode: 143, duration: 0.551s, episode steps:  24, steps per second:  44, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.342122, mae: 6.566874, mean_q: 12.373508, mean_eps: 0.100000\n",
            " 2390/8000: episode: 144, duration: 0.648s, episode steps:  26, steps per second:  40, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1.885072, mae: 6.585059, mean_q: 12.470793, mean_eps: 0.100000\n",
            " 2432/8000: episode: 145, duration: 1.000s, episode steps:  42, steps per second:  42, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.992469, mae: 6.691213, mean_q: 12.694423, mean_eps: 0.100000\n",
            " 2465/8000: episode: 146, duration: 0.793s, episode steps:  33, steps per second:  42, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 2.782960, mae: 6.765988, mean_q: 12.654923, mean_eps: 0.100000\n",
            " 2525/8000: episode: 147, duration: 1.289s, episode steps:  60, steps per second:  47, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 1.952655, mae: 6.825369, mean_q: 12.963359, mean_eps: 0.100000\n",
            " 2593/8000: episode: 148, duration: 1.314s, episode steps:  68, steps per second:  52, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 2.111448, mae: 6.960516, mean_q: 13.230668, mean_eps: 0.100000\n",
            " 2656/8000: episode: 149, duration: 1.112s, episode steps:  63, steps per second:  57, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 2.094511, mae: 7.094898, mean_q: 13.507975, mean_eps: 0.100000\n",
            " 2708/8000: episode: 150, duration: 0.881s, episode steps:  52, steps per second:  59, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 2.295021, mae: 7.236527, mean_q: 13.744712, mean_eps: 0.100000\n",
            " 2756/8000: episode: 151, duration: 0.739s, episode steps:  48, steps per second:  65, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 2.635879, mae: 7.384615, mean_q: 13.973689, mean_eps: 0.100000\n",
            " 2801/8000: episode: 152, duration: 0.978s, episode steps:  45, steps per second:  46, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 2.661056, mae: 7.441988, mean_q: 14.095313, mean_eps: 0.100000\n",
            " 2855/8000: episode: 153, duration: 1.038s, episode steps:  54, steps per second:  52, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 2.321980, mae: 7.588636, mean_q: 14.464141, mean_eps: 0.100000\n",
            " 2901/8000: episode: 154, duration: 0.620s, episode steps:  46, steps per second:  74, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 3.034906, mae: 7.664801, mean_q: 14.519561, mean_eps: 0.100000\n",
            " 2930/8000: episode: 155, duration: 0.543s, episode steps:  29, steps per second:  53, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 2.274895, mae: 7.628125, mean_q: 14.483575, mean_eps: 0.100000\n",
            " 3036/8000: episode: 156, duration: 1.560s, episode steps: 106, steps per second:  68, episode reward: 106.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 2.909493, mae: 7.805437, mean_q: 14.776205, mean_eps: 0.100000\n",
            " 3070/8000: episode: 157, duration: 0.509s, episode steps:  34, steps per second:  67, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 3.151439, mae: 7.866096, mean_q: 14.853555, mean_eps: 0.100000\n",
            " 3135/8000: episode: 158, duration: 1.004s, episode steps:  65, steps per second:  65, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 2.479575, mae: 8.035965, mean_q: 15.380332, mean_eps: 0.100000\n",
            " 3182/8000: episode: 159, duration: 0.845s, episode steps:  47, steps per second:  56, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.985702, mae: 8.062364, mean_q: 15.489972, mean_eps: 0.100000\n",
            " 3246/8000: episode: 160, duration: 1.100s, episode steps:  64, steps per second:  58, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 2.195223, mae: 8.141426, mean_q: 15.674538, mean_eps: 0.100000\n",
            " 3372/8000: episode: 161, duration: 1.718s, episode steps: 126, steps per second:  73, episode reward: 126.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 2.820208, mae: 8.386752, mean_q: 16.023135, mean_eps: 0.100000\n",
            " 3407/8000: episode: 162, duration: 0.458s, episode steps:  35, steps per second:  76, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 2.560670, mae: 8.485391, mean_q: 16.270587, mean_eps: 0.100000\n",
            " 3444/8000: episode: 163, duration: 0.535s, episode steps:  37, steps per second:  69, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 2.202722, mae: 8.538943, mean_q: 16.400036, mean_eps: 0.100000\n",
            " 3481/8000: episode: 164, duration: 0.507s, episode steps:  37, steps per second:  73, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 3.504174, mae: 8.705767, mean_q: 16.549167, mean_eps: 0.100000\n",
            " 3562/8000: episode: 165, duration: 1.230s, episode steps:  81, steps per second:  66, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 2.418040, mae: 8.727161, mean_q: 16.760923, mean_eps: 0.100000\n",
            " 3596/8000: episode: 166, duration: 0.648s, episode steps:  34, steps per second:  52, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.677365, mae: 8.873535, mean_q: 17.050424, mean_eps: 0.100000\n",
            " 3664/8000: episode: 167, duration: 1.198s, episode steps:  68, steps per second:  57, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 2.875785, mae: 9.011214, mean_q: 17.299222, mean_eps: 0.100000\n",
            " 3720/8000: episode: 168, duration: 0.904s, episode steps:  56, steps per second:  62, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 3.722128, mae: 9.164596, mean_q: 17.505055, mean_eps: 0.100000\n",
            " 3823/8000: episode: 169, duration: 1.395s, episode steps: 103, steps per second:  74, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 2.633820, mae: 9.235586, mean_q: 17.803495, mean_eps: 0.100000\n",
            " 3892/8000: episode: 170, duration: 0.952s, episode steps:  69, steps per second:  72, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 2.830294, mae: 9.386953, mean_q: 18.061422, mean_eps: 0.100000\n",
            " 3984/8000: episode: 171, duration: 1.196s, episode steps:  92, steps per second:  77, episode reward: 92.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 3.745215, mae: 9.532166, mean_q: 18.204717, mean_eps: 0.100000\n",
            " 4065/8000: episode: 172, duration: 1.340s, episode steps:  81, steps per second:  60, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 2.718925, mae: 9.648113, mean_q: 18.606419, mean_eps: 0.100000\n",
            " 4133/8000: episode: 173, duration: 1.164s, episode steps:  68, steps per second:  58, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.517417, mae: 9.796896, mean_q: 18.872293, mean_eps: 0.100000\n",
            " 4234/8000: episode: 174, duration: 1.555s, episode steps: 101, steps per second:  65, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 3.328094, mae: 9.880778, mean_q: 18.995274, mean_eps: 0.100000\n",
            " 4295/8000: episode: 175, duration: 1.041s, episode steps:  61, steps per second:  59, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 3.615093, mae: 10.034888, mean_q: 19.333590, mean_eps: 0.100000\n",
            " 4403/8000: episode: 176, duration: 1.477s, episode steps: 108, steps per second:  73, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 3.496877, mae: 10.183800, mean_q: 19.651808, mean_eps: 0.100000\n",
            " 4527/8000: episode: 177, duration: 1.833s, episode steps: 124, steps per second:  68, episode reward: 124.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 3.340094, mae: 10.411661, mean_q: 20.173143, mean_eps: 0.100000\n",
            " 4608/8000: episode: 178, duration: 1.030s, episode steps:  81, steps per second:  79, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 3.729695, mae: 10.656166, mean_q: 20.610113, mean_eps: 0.100000\n",
            " 4803/8000: episode: 179, duration: 3.037s, episode steps: 195, steps per second:  64, episode reward: 195.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 3.876067, mae: 10.915687, mean_q: 21.126117, mean_eps: 0.100000\n",
            " 4902/8000: episode: 180, duration: 1.286s, episode steps:  99, steps per second:  77, episode reward: 99.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 3.442042, mae: 11.083584, mean_q: 21.525855, mean_eps: 0.100000\n",
            " 5010/8000: episode: 181, duration: 1.414s, episode steps: 108, steps per second:  76, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 4.162872, mae: 11.234610, mean_q: 21.776255, mean_eps: 0.100000\n",
            " 5159/8000: episode: 182, duration: 2.163s, episode steps: 149, steps per second:  69, episode reward: 149.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 4.304875, mae: 11.415375, mean_q: 22.061274, mean_eps: 0.100000\n",
            " 5282/8000: episode: 183, duration: 2.128s, episode steps: 123, steps per second:  58, episode reward: 123.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 4.601277, mae: 11.646337, mean_q: 22.502275, mean_eps: 0.100000\n",
            " 5369/8000: episode: 184, duration: 1.178s, episode steps:  87, steps per second:  74, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 4.239061, mae: 11.799988, mean_q: 22.917891, mean_eps: 0.100000\n",
            " 5537/8000: episode: 185, duration: 2.998s, episode steps: 168, steps per second:  56, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 4.410142, mae: 11.904197, mean_q: 23.086531, mean_eps: 0.100000\n",
            " 5641/8000: episode: 186, duration: 1.462s, episode steps: 104, steps per second:  71, episode reward: 104.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 4.112060, mae: 12.176536, mean_q: 23.717682, mean_eps: 0.100000\n",
            " 5781/8000: episode: 187, duration: 1.861s, episode steps: 140, steps per second:  75, episode reward: 140.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 4.201853, mae: 12.394468, mean_q: 24.151440, mean_eps: 0.100000\n",
            " 5878/8000: episode: 188, duration: 1.262s, episode steps:  97, steps per second:  77, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 4.244037, mae: 12.637445, mean_q: 24.652507, mean_eps: 0.100000\n",
            " 5973/8000: episode: 189, duration: 1.311s, episode steps:  95, steps per second:  72, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 4.660417, mae: 12.744343, mean_q: 24.782830, mean_eps: 0.100000\n",
            " 6051/8000: episode: 190, duration: 1.099s, episode steps:  78, steps per second:  71, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 5.017550, mae: 12.936498, mean_q: 25.152486, mean_eps: 0.100000\n",
            " 6233/8000: episode: 191, duration: 2.653s, episode steps: 182, steps per second:  69, episode reward: 182.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 4.687563, mae: 13.072721, mean_q: 25.566669, mean_eps: 0.100000\n",
            " 6311/8000: episode: 192, duration: 1.037s, episode steps:  78, steps per second:  75, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 4.432000, mae: 13.287511, mean_q: 25.931778, mean_eps: 0.100000\n",
            " 6436/8000: episode: 193, duration: 1.705s, episode steps: 125, steps per second:  73, episode reward: 125.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 4.022167, mae: 13.305477, mean_q: 26.034845, mean_eps: 0.100000\n",
            " 6613/8000: episode: 194, duration: 2.571s, episode steps: 177, steps per second:  69, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 4.102761, mae: 13.629969, mean_q: 26.688056, mean_eps: 0.100000\n",
            " 6697/8000: episode: 195, duration: 1.530s, episode steps:  84, steps per second:  55, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 5.255643, mae: 13.782847, mean_q: 26.900333, mean_eps: 0.100000\n",
            " 6785/8000: episode: 196, duration: 1.432s, episode steps:  88, steps per second:  61, episode reward: 88.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 4.735320, mae: 14.034124, mean_q: 27.465222, mean_eps: 0.100000\n",
            " 6886/8000: episode: 197, duration: 1.736s, episode steps: 101, steps per second:  58, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 5.580361, mae: 14.085773, mean_q: 27.498523, mean_eps: 0.100000\n",
            " 6979/8000: episode: 198, duration: 1.484s, episode steps:  93, steps per second:  63, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 5.345893, mae: 14.269786, mean_q: 27.910890, mean_eps: 0.100000\n",
            " 7087/8000: episode: 199, duration: 1.481s, episode steps: 108, steps per second:  73, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 5.265369, mae: 14.418071, mean_q: 28.243232, mean_eps: 0.100000\n",
            " 7182/8000: episode: 200, duration: 1.453s, episode steps:  95, steps per second:  65, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 5.548943, mae: 14.456072, mean_q: 28.272393, mean_eps: 0.100000\n",
            " 7256/8000: episode: 201, duration: 1.519s, episode steps:  74, steps per second:  49, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 4.508037, mae: 14.642747, mean_q: 28.755202, mean_eps: 0.100000\n",
            " 7354/8000: episode: 202, duration: 1.589s, episode steps:  98, steps per second:  62, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 4.761757, mae: 14.788130, mean_q: 29.021234, mean_eps: 0.100000\n",
            " 7554/8000: episode: 203, duration: 3.812s, episode steps: 200, steps per second:  52, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 4.425030, mae: 15.011560, mean_q: 29.516084, mean_eps: 0.100000\n",
            " 7677/8000: episode: 204, duration: 1.625s, episode steps: 123, steps per second:  76, episode reward: 123.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 5.276758, mae: 15.347263, mean_q: 30.164657, mean_eps: 0.100000\n",
            " 7825/8000: episode: 205, duration: 1.988s, episode steps: 148, steps per second:  74, episode reward: 148.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 5.820899, mae: 15.380877, mean_q: 30.152400, mean_eps: 0.100000\n",
            " 7873/8000: episode: 206, duration: 0.696s, episode steps:  48, steps per second:  69, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 4.628183, mae: 15.407555, mean_q: 30.329672, mean_eps: 0.100000\n",
            "done, took 140.532 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 195.000, steps: 195\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 130.000, steps: 130\n",
            "Episode 5: reward: 128.000, steps: 128\n",
            "Episode 6: reward: 79.000, steps: 79\n",
            "Episode 7: reward: 200.000, steps: 200\n",
            "Episode 8: reward: 147.000, steps: 147\n",
            "Episode 9: reward: 200.000, steps: 200\n",
            "Episode 10: reward: 176.000, steps: 176\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 137.000, steps: 137\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 92.000, steps: 92\n",
            "Episode 15: reward: 181.000, steps: 181\n",
            "Episode 16: reward: 81.000, steps: 81\n",
            "Episode 17: reward: 185.000, steps: 185\n",
            "Episode 18: reward: 122.000, steps: 122\n",
            "Episode 19: reward: 108.000, steps: 108\n",
            "Episode 20: reward: 110.000, steps: 110\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb1718bf450>"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    }
  ]
}