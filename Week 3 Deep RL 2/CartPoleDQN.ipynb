{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPoleDQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Megacity1/CartpoleDQN/blob/main/Week%203%20Deep%20RL%202/CartPoleDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Deep Q Network (DQN) for CartPole Using Boltzmann Q Policy\n",
        "This exercise implements a DQN for CartPole using a Boltzmann Q policy for selecting the actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGsC7cJ5jNcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda73cbf-3020-40f4-e45e-7b87053448f1"
      },
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.24.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (13.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.44.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.10.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMIHLgQ3Z-lF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5a8608-456f-4127-ce8a-f551e5f4e4f3"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0AMLzq08ap0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b380e1-6c3d-4bce-8233-a8ac01deaf26"
      },
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "!pip install Adam\n",
        "#from keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Adam in /usr/local/lib/python3.7/dist-packages (0.0.0.dev0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll6bNdUm54WS"
      },
      "source": [
        "Implementation of DQN for CartPole, applying policy BoltzmannQPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCrPKNy40PC"
      },
      "source": [
        "##Implement DQN with BoltzmannGumbelQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efM9jkXr5A3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "df26de01-c08d-4f8b-b520-0f821af88946"
      },
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import BoltzmannGumbelQPolicy\n",
        "from rl.policy import LinearAnnealedPolicy\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n",
        "# setup experience replay buffer\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
        "                               attr='eps',            \n",
        "                               value_max=5.,\n",
        "                               value_min=.5, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=200)\n",
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "# add extra layers here\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=25,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy) \n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=8000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_48\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_46 (Flatten)        (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_93 (Dense)            (None, 16)                80        \n",
            "                                                                 \n",
            " dense_94 (Dense)            (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 8000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   14/8000: episode: 1, duration: 2.751s, episode steps:  14, steps per second:   5, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   71/8000: episode: 2, duration: 10.940s, episode steps:  57, steps per second:   5, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 0.461205, mae: 0.560181, mean_q: 0.235865, mean_eps: 3.920000\n",
            "   83/8000: episode: 3, duration: 0.380s, episode steps:  12, steps per second:  32, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.361101, mae: 0.583431, mean_q: 0.437215, mean_eps: 3.278750\n",
            "   92/8000: episode: 4, duration: 0.299s, episode steps:   9, steps per second:  30, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.327096, mae: 0.604245, mean_q: 0.515365, mean_eps: 3.042500\n",
            "  118/8000: episode: 5, duration: 0.716s, episode steps:  26, steps per second:  36, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.289777, mae: 0.634907, mean_q: 0.603117, mean_eps: 2.648750\n",
            "  131/8000: episode: 6, duration: 0.270s, episode steps:  13, steps per second:  48, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.249970, mae: 0.662403, mean_q: 0.706320, mean_eps: 2.210000\n",
            "  159/8000: episode: 7, duration: 0.524s, episode steps:  28, steps per second:  53, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 0.217404, mae: 0.703501, mean_q: 0.825177, mean_eps: 1.748750\n",
            "  173/8000: episode: 8, duration: 0.362s, episode steps:  14, steps per second:  39, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.194012, mae: 0.742927, mean_q: 0.943071, mean_eps: 1.276250\n",
            "  191/8000: episode: 9, duration: 0.496s, episode steps:  18, steps per second:  36, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 0.174565, mae: 0.795060, mean_q: 1.075848, mean_eps: 0.916250\n",
            "  205/8000: episode: 10, duration: 0.376s, episode steps:  14, steps per second:  37, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.171119, mae: 0.851336, mean_q: 1.196911, mean_eps: 0.572321\n",
            "  217/8000: episode: 11, duration: 0.212s, episode steps:  12, steps per second:  57, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.151821, mae: 0.862138, mean_q: 1.254667, mean_eps: 0.500000\n",
            "  231/8000: episode: 12, duration: 0.221s, episode steps:  14, steps per second:  63, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.149607, mae: 0.916047, mean_q: 1.383109, mean_eps: 0.500000\n",
            "  246/8000: episode: 13, duration: 0.238s, episode steps:  15, steps per second:  63, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.155339, mae: 0.956537, mean_q: 1.477015, mean_eps: 0.500000\n",
            "  260/8000: episode: 14, duration: 0.236s, episode steps:  14, steps per second:  59, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.158415, mae: 1.014618, mean_q: 1.603091, mean_eps: 0.500000\n",
            "  272/8000: episode: 15, duration: 0.198s, episode steps:  12, steps per second:  61, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.159189, mae: 1.069735, mean_q: 1.737053, mean_eps: 0.500000\n",
            "  284/8000: episode: 16, duration: 0.187s, episode steps:  12, steps per second:  64, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.178150, mae: 1.100552, mean_q: 1.811848, mean_eps: 0.500000\n",
            "  304/8000: episode: 17, duration: 0.319s, episode steps:  20, steps per second:  63, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.172679, mae: 1.163496, mean_q: 1.925831, mean_eps: 0.500000\n",
            "  315/8000: episode: 18, duration: 0.181s, episode steps:  11, steps per second:  61, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.224837, mae: 1.217450, mean_q: 2.029720, mean_eps: 0.500000\n",
            "  327/8000: episode: 19, duration: 0.211s, episode steps:  12, steps per second:  57, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 0.188684, mae: 1.251128, mean_q: 2.133705, mean_eps: 0.500000\n",
            "  339/8000: episode: 20, duration: 0.196s, episode steps:  12, steps per second:  61, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.218063, mae: 1.313477, mean_q: 2.257527, mean_eps: 0.500000\n",
            "  348/8000: episode: 21, duration: 0.182s, episode steps:   9, steps per second:  49, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.226442, mae: 1.354102, mean_q: 2.338739, mean_eps: 0.500000\n",
            "  359/8000: episode: 22, duration: 0.170s, episode steps:  11, steps per second:  65, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.244051, mae: 1.411219, mean_q: 2.453446, mean_eps: 0.500000\n",
            "  369/8000: episode: 23, duration: 0.168s, episode steps:  10, steps per second:  60, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.397649, mae: 1.499897, mean_q: 2.526285, mean_eps: 0.500000\n",
            "  379/8000: episode: 24, duration: 0.152s, episode steps:  10, steps per second:  66, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.335275, mae: 1.509362, mean_q: 2.579707, mean_eps: 0.500000\n",
            "  391/8000: episode: 25, duration: 0.217s, episode steps:  12, steps per second:  55, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.083 [0.000, 1.000],  loss: 0.291679, mae: 1.512247, mean_q: 2.593013, mean_eps: 0.500000\n",
            "  400/8000: episode: 26, duration: 0.147s, episode steps:   9, steps per second:  61, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.395705, mae: 1.620714, mean_q: 2.751176, mean_eps: 0.500000\n",
            "  412/8000: episode: 27, duration: 0.214s, episode steps:  12, steps per second:  56, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.083 [0.000, 1.000],  loss: 0.336023, mae: 1.641903, mean_q: 2.859409, mean_eps: 0.500000\n",
            "  421/8000: episode: 28, duration: 0.153s, episode steps:   9, steps per second:  59, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.270522, mae: 1.645261, mean_q: 2.928287, mean_eps: 0.500000\n",
            "  431/8000: episode: 29, duration: 0.159s, episode steps:  10, steps per second:  63, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.407193, mae: 1.727584, mean_q: 3.027085, mean_eps: 0.500000\n",
            "  439/8000: episode: 30, duration: 0.127s, episode steps:   8, steps per second:  63, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.494588, mae: 1.770428, mean_q: 3.099464, mean_eps: 0.500000\n",
            "  458/8000: episode: 31, duration: 0.342s, episode steps:  19, steps per second:  56, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 0.493136, mae: 1.799882, mean_q: 3.153733, mean_eps: 0.500000\n",
            "  468/8000: episode: 32, duration: 0.162s, episode steps:  10, steps per second:  62, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.372378, mae: 1.811933, mean_q: 3.186419, mean_eps: 0.500000\n",
            "  486/8000: episode: 33, duration: 0.302s, episode steps:  18, steps per second:  60, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 0.439487, mae: 1.874575, mean_q: 3.340620, mean_eps: 0.500000\n",
            "  497/8000: episode: 34, duration: 0.184s, episode steps:  11, steps per second:  60, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.548364, mae: 1.969043, mean_q: 3.500837, mean_eps: 0.500000\n",
            "  510/8000: episode: 35, duration: 0.227s, episode steps:  13, steps per second:  57, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.846 [0.000, 1.000],  loss: 0.444032, mae: 1.990058, mean_q: 3.538214, mean_eps: 0.500000\n",
            "  524/8000: episode: 36, duration: 0.244s, episode steps:  14, steps per second:  57, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.396780, mae: 1.983319, mean_q: 3.566076, mean_eps: 0.500000\n",
            "  533/8000: episode: 37, duration: 0.156s, episode steps:   9, steps per second:  58, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.476532, mae: 2.087468, mean_q: 3.742150, mean_eps: 0.500000\n",
            "  549/8000: episode: 38, duration: 0.261s, episode steps:  16, steps per second:  61, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.473652, mae: 2.121727, mean_q: 3.865894, mean_eps: 0.500000\n",
            "  560/8000: episode: 39, duration: 0.176s, episode steps:  11, steps per second:  62, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.452580, mae: 2.158997, mean_q: 3.954098, mean_eps: 0.500000\n",
            "  571/8000: episode: 40, duration: 0.211s, episode steps:  11, steps per second:  52, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.569970, mae: 2.233444, mean_q: 4.041322, mean_eps: 0.500000\n",
            "  583/8000: episode: 41, duration: 0.203s, episode steps:  12, steps per second:  59, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.484522, mae: 2.237404, mean_q: 4.104009, mean_eps: 0.500000\n",
            "  597/8000: episode: 42, duration: 0.219s, episode steps:  14, steps per second:  64, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 0.802639, mae: 2.359171, mean_q: 4.257805, mean_eps: 0.500000\n",
            "  613/8000: episode: 43, duration: 0.243s, episode steps:  16, steps per second:  66, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.614705, mae: 2.369370, mean_q: 4.279141, mean_eps: 0.500000\n",
            "  625/8000: episode: 44, duration: 0.194s, episode steps:  12, steps per second:  62, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.493305, mae: 2.385470, mean_q: 4.396099, mean_eps: 0.500000\n",
            "  637/8000: episode: 45, duration: 0.212s, episode steps:  12, steps per second:  57, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.847846, mae: 2.499017, mean_q: 4.520627, mean_eps: 0.500000\n",
            "  649/8000: episode: 46, duration: 0.192s, episode steps:  12, steps per second:  62, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.665930, mae: 2.522671, mean_q: 4.550008, mean_eps: 0.500000\n",
            "  659/8000: episode: 47, duration: 0.167s, episode steps:  10, steps per second:  60, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.691719, mae: 2.557916, mean_q: 4.574785, mean_eps: 0.500000\n",
            "  677/8000: episode: 48, duration: 0.299s, episode steps:  18, steps per second:  60, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.778979, mae: 2.602255, mean_q: 4.640380, mean_eps: 0.500000\n",
            "  692/8000: episode: 49, duration: 0.303s, episode steps:  15, steps per second:  49, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.635572, mae: 2.637268, mean_q: 4.740254, mean_eps: 0.500000\n",
            "  705/8000: episode: 50, duration: 0.227s, episode steps:  13, steps per second:  57, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.660574, mae: 2.687178, mean_q: 4.856025, mean_eps: 0.500000\n",
            "  718/8000: episode: 51, duration: 0.217s, episode steps:  13, steps per second:  60, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.917238, mae: 2.790065, mean_q: 4.941607, mean_eps: 0.500000\n",
            "  729/8000: episode: 52, duration: 0.181s, episode steps:  11, steps per second:  61, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.747509, mae: 2.789854, mean_q: 4.955080, mean_eps: 0.500000\n",
            "  740/8000: episode: 53, duration: 0.213s, episode steps:  11, steps per second:  52, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.558000, mae: 2.776686, mean_q: 5.004916, mean_eps: 0.500000\n",
            "  750/8000: episode: 54, duration: 0.193s, episode steps:  10, steps per second:  52, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.614396, mae: 2.816091, mean_q: 5.124948, mean_eps: 0.500000\n",
            "  761/8000: episode: 55, duration: 0.214s, episode steps:  11, steps per second:  51, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.940982, mae: 2.934547, mean_q: 5.193946, mean_eps: 0.500000\n",
            "  773/8000: episode: 56, duration: 0.184s, episode steps:  12, steps per second:  65, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.937120, mae: 2.971857, mean_q: 5.231576, mean_eps: 0.500000\n",
            "  785/8000: episode: 57, duration: 0.191s, episode steps:  12, steps per second:  63, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.705253, mae: 2.930825, mean_q: 5.265472, mean_eps: 0.500000\n",
            "  796/8000: episode: 58, duration: 0.173s, episode steps:  11, steps per second:  64, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.666679, mae: 2.948243, mean_q: 5.357590, mean_eps: 0.500000\n",
            "  806/8000: episode: 59, duration: 0.200s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.627361, mae: 2.984993, mean_q: 5.437832, mean_eps: 0.500000\n",
            "  820/8000: episode: 60, duration: 0.237s, episode steps:  14, steps per second:  59, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.718824, mae: 3.038160, mean_q: 5.522551, mean_eps: 0.500000\n",
            "  829/8000: episode: 61, duration: 0.145s, episode steps:   9, steps per second:  62, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.711577, mae: 3.080505, mean_q: 5.601127, mean_eps: 0.500000\n",
            "  840/8000: episode: 62, duration: 0.177s, episode steps:  11, steps per second:  62, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.887510, mae: 3.126902, mean_q: 5.611633, mean_eps: 0.500000\n",
            "  850/8000: episode: 63, duration: 0.158s, episode steps:  10, steps per second:  63, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.998394, mae: 3.182229, mean_q: 5.647114, mean_eps: 0.500000\n",
            "  860/8000: episode: 64, duration: 0.182s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.766882, mae: 3.152085, mean_q: 5.660049, mean_eps: 0.500000\n",
            "  877/8000: episode: 65, duration: 0.428s, episode steps:  17, steps per second:  40, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.815370, mae: 3.194735, mean_q: 5.771546, mean_eps: 0.500000\n",
            "  894/8000: episode: 66, duration: 0.488s, episode steps:  17, steps per second:  35, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 0.942549, mae: 3.260644, mean_q: 5.873710, mean_eps: 0.500000\n",
            "  906/8000: episode: 67, duration: 0.316s, episode steps:  12, steps per second:  38, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 1.006395, mae: 3.302356, mean_q: 5.890884, mean_eps: 0.500000\n",
            "  916/8000: episode: 68, duration: 0.196s, episode steps:  10, steps per second:  51, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.830518, mae: 3.294065, mean_q: 5.893972, mean_eps: 0.500000\n",
            "  930/8000: episode: 69, duration: 0.314s, episode steps:  14, steps per second:  45, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.797648, mae: 3.330689, mean_q: 5.980574, mean_eps: 0.500000\n",
            "  947/8000: episode: 70, duration: 0.340s, episode steps:  17, steps per second:  50, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 1.115062, mae: 3.422089, mean_q: 6.091557, mean_eps: 0.500000\n",
            "  961/8000: episode: 71, duration: 0.278s, episode steps:  14, steps per second:  50, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.870367, mae: 3.400165, mean_q: 6.072394, mean_eps: 0.500000\n",
            "  973/8000: episode: 72, duration: 0.221s, episode steps:  12, steps per second:  54, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.932972, mae: 3.443921, mean_q: 6.180240, mean_eps: 0.500000\n",
            " 1011/8000: episode: 73, duration: 0.743s, episode steps:  38, steps per second:  51, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 0.864715, mae: 3.490611, mean_q: 6.333545, mean_eps: 0.500000\n",
            " 1025/8000: episode: 74, duration: 0.360s, episode steps:  14, steps per second:  39, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.900311, mae: 3.557988, mean_q: 6.484155, mean_eps: 0.500000\n",
            " 1037/8000: episode: 75, duration: 0.333s, episode steps:  12, steps per second:  36, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.870855, mae: 3.550270, mean_q: 6.505212, mean_eps: 0.500000\n",
            " 1049/8000: episode: 76, duration: 0.324s, episode steps:  12, steps per second:  37, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 1.027827, mae: 3.622772, mean_q: 6.574797, mean_eps: 0.500000\n",
            " 1065/8000: episode: 77, duration: 0.376s, episode steps:  16, steps per second:  43, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.989749, mae: 3.646916, mean_q: 6.570341, mean_eps: 0.500000\n",
            " 1094/8000: episode: 78, duration: 0.663s, episode steps:  29, steps per second:  44, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 0.867618, mae: 3.691686, mean_q: 6.739324, mean_eps: 0.500000\n",
            " 1110/8000: episode: 79, duration: 0.272s, episode steps:  16, steps per second:  59, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.864982, mae: 3.748125, mean_q: 6.901511, mean_eps: 0.500000\n",
            " 1134/8000: episode: 80, duration: 0.374s, episode steps:  24, steps per second:  64, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 1.140706, mae: 3.831224, mean_q: 6.958870, mean_eps: 0.500000\n",
            " 1145/8000: episode: 81, duration: 0.212s, episode steps:  11, steps per second:  52, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 1.097543, mae: 3.853973, mean_q: 6.954351, mean_eps: 0.500000\n",
            " 1159/8000: episode: 82, duration: 0.230s, episode steps:  14, steps per second:  61, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.929819, mae: 3.827989, mean_q: 6.926465, mean_eps: 0.500000\n",
            " 1172/8000: episode: 83, duration: 0.224s, episode steps:  13, steps per second:  58, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.800165, mae: 3.854856, mean_q: 7.133108, mean_eps: 0.500000\n",
            " 1185/8000: episode: 84, duration: 0.223s, episode steps:  13, steps per second:  58, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 1.308611, mae: 3.970713, mean_q: 7.256458, mean_eps: 0.500000\n",
            " 1206/8000: episode: 85, duration: 0.411s, episode steps:  21, steps per second:  51, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 1.163945, mae: 3.983757, mean_q: 7.229009, mean_eps: 0.500000\n",
            " 1224/8000: episode: 86, duration: 0.353s, episode steps:  18, steps per second:  51, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.038129, mae: 4.011890, mean_q: 7.292150, mean_eps: 0.500000\n",
            " 1235/8000: episode: 87, duration: 0.236s, episode steps:  11, steps per second:  47, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.883245, mae: 3.995703, mean_q: 7.347870, mean_eps: 0.500000\n",
            " 1249/8000: episode: 88, duration: 0.278s, episode steps:  14, steps per second:  50, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 1.075804, mae: 4.039884, mean_q: 7.447258, mean_eps: 0.500000\n",
            " 1267/8000: episode: 89, duration: 0.308s, episode steps:  18, steps per second:  59, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.195795, mae: 4.102289, mean_q: 7.480164, mean_eps: 0.500000\n",
            " 1287/8000: episode: 90, duration: 0.381s, episode steps:  20, steps per second:  52, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.953515, mae: 4.132252, mean_q: 7.630668, mean_eps: 0.500000\n",
            " 1305/8000: episode: 91, duration: 0.354s, episode steps:  18, steps per second:  51, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.867017, mae: 4.172136, mean_q: 7.747730, mean_eps: 0.500000\n",
            " 1324/8000: episode: 92, duration: 0.363s, episode steps:  19, steps per second:  52, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 0.913386, mae: 4.218837, mean_q: 7.879364, mean_eps: 0.500000\n",
            " 1344/8000: episode: 93, duration: 0.376s, episode steps:  20, steps per second:  53, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.639683, mae: 4.348153, mean_q: 7.883661, mean_eps: 0.500000\n",
            " 1361/8000: episode: 94, duration: 0.338s, episode steps:  17, steps per second:  50, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1.401699, mae: 4.315988, mean_q: 7.724239, mean_eps: 0.500000\n",
            " 1376/8000: episode: 95, duration: 0.404s, episode steps:  15, steps per second:  37, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1.023819, mae: 4.289380, mean_q: 7.869128, mean_eps: 0.500000\n",
            " 1403/8000: episode: 96, duration: 0.797s, episode steps:  27, steps per second:  34, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 1.304877, mae: 4.395522, mean_q: 8.082602, mean_eps: 0.500000\n",
            " 1421/8000: episode: 97, duration: 0.524s, episode steps:  18, steps per second:  34, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 1.403207, mae: 4.421539, mean_q: 8.077627, mean_eps: 0.500000\n",
            " 1441/8000: episode: 98, duration: 0.495s, episode steps:  20, steps per second:  40, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.909759, mae: 4.412391, mean_q: 8.170854, mean_eps: 0.500000\n",
            " 1469/8000: episode: 99, duration: 0.510s, episode steps:  28, steps per second:  55, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 1.497664, mae: 4.591447, mean_q: 8.379869, mean_eps: 0.500000\n",
            " 1505/8000: episode: 100, duration: 0.783s, episode steps:  36, steps per second:  46, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1.123396, mae: 4.591083, mean_q: 8.458692, mean_eps: 0.500000\n",
            " 1557/8000: episode: 101, duration: 0.999s, episode steps:  52, steps per second:  52, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.141858, mae: 4.707746, mean_q: 8.806185, mean_eps: 0.500000\n",
            " 1582/8000: episode: 102, duration: 0.468s, episode steps:  25, steps per second:  53, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 1.563297, mae: 4.867901, mean_q: 8.998723, mean_eps: 0.500000\n",
            " 1604/8000: episode: 103, duration: 0.449s, episode steps:  22, steps per second:  49, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 1.740892, mae: 4.934073, mean_q: 9.068815, mean_eps: 0.500000\n",
            " 1633/8000: episode: 104, duration: 0.567s, episode steps:  29, steps per second:  51, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 1.206576, mae: 4.887852, mean_q: 9.084438, mean_eps: 0.500000\n",
            " 1668/8000: episode: 105, duration: 0.620s, episode steps:  35, steps per second:  56, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 1.411022, mae: 5.004499, mean_q: 9.334095, mean_eps: 0.500000\n",
            " 1688/8000: episode: 106, duration: 0.359s, episode steps:  20, steps per second:  56, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.455975, mae: 5.076335, mean_q: 9.521167, mean_eps: 0.500000\n",
            " 1706/8000: episode: 107, duration: 0.330s, episode steps:  18, steps per second:  55, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.872392, mae: 5.167432, mean_q: 9.554415, mean_eps: 0.500000\n",
            " 1729/8000: episode: 108, duration: 0.423s, episode steps:  23, steps per second:  54, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.443379, mae: 5.133966, mean_q: 9.607386, mean_eps: 0.500000\n",
            " 1794/8000: episode: 109, duration: 1.145s, episode steps:  65, steps per second:  57, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1.705318, mae: 5.292466, mean_q: 9.865251, mean_eps: 0.500000\n",
            " 1820/8000: episode: 110, duration: 0.523s, episode steps:  26, steps per second:  50, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.795758, mae: 5.421698, mean_q: 10.064808, mean_eps: 0.500000\n",
            " 1855/8000: episode: 111, duration: 0.636s, episode steps:  35, steps per second:  55, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 1.882592, mae: 5.510179, mean_q: 10.265921, mean_eps: 0.500000\n",
            " 1877/8000: episode: 112, duration: 0.407s, episode steps:  22, steps per second:  54, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.756097, mae: 5.581849, mean_q: 10.427710, mean_eps: 0.500000\n",
            " 1904/8000: episode: 113, duration: 0.429s, episode steps:  27, steps per second:  63, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 1.799195, mae: 5.576963, mean_q: 10.422834, mean_eps: 0.500000\n",
            " 1926/8000: episode: 114, duration: 0.364s, episode steps:  22, steps per second:  60, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  loss: 1.788768, mae: 5.628629, mean_q: 10.434085, mean_eps: 0.500000\n",
            " 1954/8000: episode: 115, duration: 0.809s, episode steps:  28, steps per second:  35, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.898136, mae: 5.703167, mean_q: 10.655198, mean_eps: 0.500000\n",
            " 1992/8000: episode: 116, duration: 1.003s, episode steps:  38, steps per second:  38, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.821422, mae: 5.755110, mean_q: 10.794655, mean_eps: 0.500000\n",
            " 2015/8000: episode: 117, duration: 0.653s, episode steps:  23, steps per second:  35, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 2.215292, mae: 5.884245, mean_q: 11.025644, mean_eps: 0.500000\n",
            " 2059/8000: episode: 118, duration: 1.095s, episode steps:  44, steps per second:  40, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 1.989981, mae: 5.892510, mean_q: 11.058264, mean_eps: 0.500000\n",
            " 2122/8000: episode: 119, duration: 1.435s, episode steps:  63, steps per second:  44, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 2.041326, mae: 6.014348, mean_q: 11.264845, mean_eps: 0.500000\n",
            " 2166/8000: episode: 120, duration: 0.982s, episode steps:  44, steps per second:  45, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 1.937833, mae: 6.129677, mean_q: 11.523683, mean_eps: 0.500000\n",
            " 2198/8000: episode: 121, duration: 0.537s, episode steps:  32, steps per second:  60, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 2.559067, mae: 6.262904, mean_q: 11.686212, mean_eps: 0.500000\n",
            " 2218/8000: episode: 122, duration: 0.316s, episode steps:  20, steps per second:  63, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 2.227764, mae: 6.213149, mean_q: 11.604959, mean_eps: 0.500000\n",
            " 2243/8000: episode: 123, duration: 0.381s, episode steps:  25, steps per second:  66, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 1.857873, mae: 6.257679, mean_q: 11.834525, mean_eps: 0.500000\n",
            " 2274/8000: episode: 124, duration: 0.497s, episode steps:  31, steps per second:  62, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 2.365140, mae: 6.379615, mean_q: 11.909439, mean_eps: 0.500000\n",
            " 2292/8000: episode: 125, duration: 0.312s, episode steps:  18, steps per second:  58, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 2.564212, mae: 6.424503, mean_q: 11.970866, mean_eps: 0.500000\n",
            " 2308/8000: episode: 126, duration: 0.330s, episode steps:  16, steps per second:  49, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 2.194080, mae: 6.364116, mean_q: 11.929704, mean_eps: 0.500000\n",
            " 2327/8000: episode: 127, duration: 0.440s, episode steps:  19, steps per second:  43, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 2.119271, mae: 6.463436, mean_q: 12.153844, mean_eps: 0.500000\n",
            " 2369/8000: episode: 128, duration: 0.982s, episode steps:  42, steps per second:  43, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 2.423690, mae: 6.470693, mean_q: 12.075421, mean_eps: 0.500000\n",
            " 2405/8000: episode: 129, duration: 0.852s, episode steps:  36, steps per second:  42, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.965058, mae: 6.539084, mean_q: 12.346787, mean_eps: 0.500000\n",
            " 2428/8000: episode: 130, duration: 0.521s, episode steps:  23, steps per second:  44, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.859794, mae: 6.574834, mean_q: 12.459747, mean_eps: 0.500000\n",
            " 2466/8000: episode: 131, duration: 0.671s, episode steps:  38, steps per second:  57, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.157471, mae: 6.680381, mean_q: 12.635321, mean_eps: 0.500000\n",
            " 2500/8000: episode: 132, duration: 0.541s, episode steps:  34, steps per second:  63, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.975675, mae: 6.785900, mean_q: 12.887392, mean_eps: 0.500000\n",
            " 2534/8000: episode: 133, duration: 0.562s, episode steps:  34, steps per second:  60, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.481775, mae: 6.844375, mean_q: 12.903158, mean_eps: 0.500000\n",
            " 2562/8000: episode: 134, duration: 0.423s, episode steps:  28, steps per second:  66, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.642782, mae: 6.846900, mean_q: 12.873070, mean_eps: 0.500000\n",
            " 2598/8000: episode: 135, duration: 0.583s, episode steps:  36, steps per second:  62, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.550422, mae: 6.989489, mean_q: 13.196028, mean_eps: 0.500000\n",
            " 2614/8000: episode: 136, duration: 0.244s, episode steps:  16, steps per second:  65, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 2.585584, mae: 7.052497, mean_q: 13.274654, mean_eps: 0.500000\n",
            " 2654/8000: episode: 137, duration: 0.620s, episode steps:  40, steps per second:  64, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 2.351169, mae: 7.073295, mean_q: 13.404905, mean_eps: 0.500000\n",
            " 2704/8000: episode: 138, duration: 0.950s, episode steps:  50, steps per second:  53, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 2.931188, mae: 7.147985, mean_q: 13.407868, mean_eps: 0.500000\n",
            " 2771/8000: episode: 139, duration: 1.271s, episode steps:  67, steps per second:  53, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 2.134463, mae: 7.229847, mean_q: 13.752170, mean_eps: 0.500000\n",
            " 2800/8000: episode: 140, duration: 0.468s, episode steps:  29, steps per second:  62, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 2.782816, mae: 7.386462, mean_q: 14.010651, mean_eps: 0.500000\n",
            " 2832/8000: episode: 141, duration: 0.526s, episode steps:  32, steps per second:  61, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 2.510696, mae: 7.481447, mean_q: 14.274962, mean_eps: 0.500000\n",
            " 2874/8000: episode: 142, duration: 0.630s, episode steps:  42, steps per second:  67, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.555734, mae: 7.463420, mean_q: 14.233769, mean_eps: 0.500000\n",
            " 2916/8000: episode: 143, duration: 0.627s, episode steps:  42, steps per second:  67, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.360760, mae: 7.585833, mean_q: 14.304493, mean_eps: 0.500000\n",
            " 2967/8000: episode: 144, duration: 0.772s, episode steps:  51, steps per second:  66, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.927084, mae: 7.650993, mean_q: 14.484877, mean_eps: 0.500000\n",
            " 2992/8000: episode: 145, duration: 0.428s, episode steps:  25, steps per second:  58, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 3.171719, mae: 7.772242, mean_q: 14.717141, mean_eps: 0.500000\n",
            " 3034/8000: episode: 146, duration: 1.120s, episode steps:  42, steps per second:  37, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 2.898112, mae: 7.838331, mean_q: 14.942044, mean_eps: 0.500000\n",
            " 3073/8000: episode: 147, duration: 0.995s, episode steps:  39, steps per second:  39, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 2.823326, mae: 7.846058, mean_q: 14.915781, mean_eps: 0.500000\n",
            " 3104/8000: episode: 148, duration: 0.752s, episode steps:  31, steps per second:  41, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 2.843954, mae: 7.965219, mean_q: 15.177784, mean_eps: 0.500000\n",
            " 3180/8000: episode: 149, duration: 1.320s, episode steps:  76, steps per second:  58, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 2.877878, mae: 8.090414, mean_q: 15.472623, mean_eps: 0.500000\n",
            " 3232/8000: episode: 150, duration: 0.830s, episode steps:  52, steps per second:  63, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 3.444686, mae: 8.211071, mean_q: 15.599007, mean_eps: 0.500000\n",
            " 3272/8000: episode: 151, duration: 0.605s, episode steps:  40, steps per second:  66, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.900950, mae: 8.245852, mean_q: 15.788821, mean_eps: 0.500000\n",
            " 3397/8000: episode: 152, duration: 2.651s, episode steps: 125, steps per second:  47, episode reward: 125.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 3.605239, mae: 8.417219, mean_q: 15.983993, mean_eps: 0.500000\n",
            " 3440/8000: episode: 153, duration: 1.030s, episode steps:  43, steps per second:  42, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 3.146401, mae: 8.539429, mean_q: 16.284944, mean_eps: 0.500000\n",
            " 3464/8000: episode: 154, duration: 0.660s, episode steps:  24, steps per second:  36, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 2.933516, mae: 8.635134, mean_q: 16.508353, mean_eps: 0.500000\n",
            " 3516/8000: episode: 155, duration: 0.952s, episode steps:  52, steps per second:  55, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 2.743816, mae: 8.661683, mean_q: 16.602402, mean_eps: 0.500000\n",
            " 3615/8000: episode: 156, duration: 1.944s, episode steps:  99, steps per second:  51, episode reward: 99.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 2.582161, mae: 8.801486, mean_q: 16.976695, mean_eps: 0.500000\n",
            " 3626/8000: episode: 157, duration: 0.178s, episode steps:  11, steps per second:  62, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 3.164425, mae: 8.946908, mean_q: 17.251347, mean_eps: 0.500000\n",
            " 3666/8000: episode: 158, duration: 0.643s, episode steps:  40, steps per second:  62, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.743455, mae: 9.022301, mean_q: 17.242797, mean_eps: 0.500000\n",
            " 3729/8000: episode: 159, duration: 1.017s, episode steps:  63, steps per second:  62, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 3.633148, mae: 9.069696, mean_q: 17.400945, mean_eps: 0.500000\n",
            " 3778/8000: episode: 160, duration: 0.794s, episode steps:  49, steps per second:  62, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 3.357334, mae: 9.270802, mean_q: 17.825828, mean_eps: 0.500000\n",
            " 3798/8000: episode: 161, duration: 0.340s, episode steps:  20, steps per second:  59, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 3.568702, mae: 9.255292, mean_q: 17.801601, mean_eps: 0.500000\n",
            " 3820/8000: episode: 162, duration: 0.352s, episode steps:  22, steps per second:  63, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.885370, mae: 9.320594, mean_q: 18.012300, mean_eps: 0.500000\n",
            " 3850/8000: episode: 163, duration: 0.481s, episode steps:  30, steps per second:  62, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.656035, mae: 9.365770, mean_q: 18.091348, mean_eps: 0.500000\n",
            " 3919/8000: episode: 164, duration: 1.124s, episode steps:  69, steps per second:  61, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 3.718391, mae: 9.414684, mean_q: 18.037975, mean_eps: 0.500000\n",
            " 3979/8000: episode: 165, duration: 1.063s, episode steps:  60, steps per second:  56, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 3.439195, mae: 9.578591, mean_q: 18.443980, mean_eps: 0.500000\n",
            " 4022/8000: episode: 166, duration: 0.900s, episode steps:  43, steps per second:  48, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 3.290615, mae: 9.667739, mean_q: 18.634406, mean_eps: 0.500000\n",
            " 4108/8000: episode: 167, duration: 1.755s, episode steps:  86, steps per second:  49, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 3.648408, mae: 9.800380, mean_q: 18.907492, mean_eps: 0.500000\n",
            " 4159/8000: episode: 168, duration: 1.177s, episode steps:  51, steps per second:  43, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 4.009798, mae: 9.882725, mean_q: 19.023264, mean_eps: 0.500000\n",
            " 4177/8000: episode: 169, duration: 0.298s, episode steps:  18, steps per second:  60, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.785680, mae: 9.930394, mean_q: 19.157574, mean_eps: 0.500000\n",
            " 4199/8000: episode: 170, duration: 0.363s, episode steps:  22, steps per second:  61, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  loss: 3.475943, mae: 9.992366, mean_q: 19.312670, mean_eps: 0.500000\n",
            " 4231/8000: episode: 171, duration: 0.554s, episode steps:  32, steps per second:  58, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 4.453962, mae: 10.099374, mean_q: 19.448669, mean_eps: 0.500000\n",
            " 4274/8000: episode: 172, duration: 0.693s, episode steps:  43, steps per second:  62, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 3.707330, mae: 10.108036, mean_q: 19.511259, mean_eps: 0.500000\n",
            " 4311/8000: episode: 173, duration: 0.976s, episode steps:  37, steps per second:  38, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 4.563927, mae: 10.184839, mean_q: 19.582112, mean_eps: 0.500000\n",
            " 4345/8000: episode: 174, duration: 0.822s, episode steps:  34, steps per second:  41, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 3.860948, mae: 10.142890, mean_q: 19.560657, mean_eps: 0.500000\n",
            " 4377/8000: episode: 175, duration: 0.799s, episode steps:  32, steps per second:  40, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 4.172110, mae: 10.386957, mean_q: 20.088897, mean_eps: 0.500000\n",
            " 4396/8000: episode: 176, duration: 0.446s, episode steps:  19, steps per second:  43, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 3.035549, mae: 10.321156, mean_q: 20.101106, mean_eps: 0.500000\n",
            " 4469/8000: episode: 177, duration: 1.301s, episode steps:  73, steps per second:  56, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 4.533177, mae: 10.422500, mean_q: 20.056222, mean_eps: 0.500000\n",
            " 4553/8000: episode: 178, duration: 1.752s, episode steps:  84, steps per second:  48, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 3.677952, mae: 10.541444, mean_q: 20.474465, mean_eps: 0.500000\n",
            " 4594/8000: episode: 179, duration: 0.856s, episode steps:  41, steps per second:  48, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 5.911695, mae: 10.775286, mean_q: 20.655984, mean_eps: 0.500000\n",
            " 4661/8000: episode: 180, duration: 1.448s, episode steps:  67, steps per second:  46, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 4.377758, mae: 10.752237, mean_q: 20.784478, mean_eps: 0.500000\n",
            " 4754/8000: episode: 181, duration: 1.666s, episode steps:  93, steps per second:  56, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 4.296966, mae: 10.902851, mean_q: 21.096128, mean_eps: 0.500000\n",
            " 4834/8000: episode: 182, duration: 2.195s, episode steps:  80, steps per second:  36, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 4.468756, mae: 11.062293, mean_q: 21.375544, mean_eps: 0.500000\n",
            " 4883/8000: episode: 183, duration: 1.341s, episode steps:  49, steps per second:  37, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 4.851040, mae: 11.078230, mean_q: 21.336211, mean_eps: 0.500000\n",
            " 4907/8000: episode: 184, duration: 0.687s, episode steps:  24, steps per second:  35, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 4.855122, mae: 11.080877, mean_q: 21.345433, mean_eps: 0.500000\n",
            " 5057/8000: episode: 185, duration: 3.287s, episode steps: 150, steps per second:  46, episode reward: 150.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 4.123143, mae: 11.280730, mean_q: 21.879082, mean_eps: 0.500000\n",
            " 5109/8000: episode: 186, duration: 0.871s, episode steps:  52, steps per second:  60, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 3.860025, mae: 11.526148, mean_q: 22.442360, mean_eps: 0.500000\n",
            " 5168/8000: episode: 187, duration: 0.902s, episode steps:  59, steps per second:  65, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.559 [0.000, 1.000],  loss: 4.700175, mae: 11.553332, mean_q: 22.339986, mean_eps: 0.500000\n",
            " 5237/8000: episode: 188, duration: 1.155s, episode steps:  69, steps per second:  60, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 4.200796, mae: 11.710534, mean_q: 22.771518, mean_eps: 0.500000\n",
            " 5258/8000: episode: 189, duration: 0.329s, episode steps:  21, steps per second:  64, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.381 [0.000, 1.000],  loss: 4.540206, mae: 11.788533, mean_q: 22.922960, mean_eps: 0.500000\n",
            " 5326/8000: episode: 190, duration: 1.316s, episode steps:  68, steps per second:  52, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  loss: 5.375845, mae: 11.752641, mean_q: 22.743618, mean_eps: 0.500000\n",
            " 5387/8000: episode: 191, duration: 1.057s, episode steps:  61, steps per second:  58, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 5.470864, mae: 11.953719, mean_q: 23.127929, mean_eps: 0.500000\n",
            " 5472/8000: episode: 192, duration: 1.436s, episode steps:  85, steps per second:  59, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 4.983809, mae: 12.031066, mean_q: 23.353494, mean_eps: 0.500000\n",
            " 5531/8000: episode: 193, duration: 1.063s, episode steps:  59, steps per second:  56, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 5.094847, mae: 12.057929, mean_q: 23.384970, mean_eps: 0.500000\n",
            " 5595/8000: episode: 194, duration: 1.159s, episode steps:  64, steps per second:  55, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 5.216660, mae: 12.196815, mean_q: 23.629621, mean_eps: 0.500000\n",
            " 5682/8000: episode: 195, duration: 1.586s, episode steps:  87, steps per second:  55, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 5.312971, mae: 12.257192, mean_q: 23.763390, mean_eps: 0.500000\n",
            " 5715/8000: episode: 196, duration: 0.567s, episode steps:  33, steps per second:  58, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 5.998732, mae: 12.367105, mean_q: 23.914718, mean_eps: 0.500000\n",
            " 5737/8000: episode: 197, duration: 0.414s, episode steps:  22, steps per second:  53, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.881535, mae: 12.382953, mean_q: 24.119547, mean_eps: 0.500000\n",
            " 5804/8000: episode: 198, duration: 1.233s, episode steps:  67, steps per second:  54, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 5.298170, mae: 12.467213, mean_q: 24.213039, mean_eps: 0.500000\n",
            " 5923/8000: episode: 199, duration: 2.181s, episode steps: 119, steps per second:  55, episode reward: 119.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 5.099801, mae: 12.545635, mean_q: 24.399172, mean_eps: 0.500000\n",
            " 6009/8000: episode: 200, duration: 1.481s, episode steps:  86, steps per second:  58, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 4.434106, mae: 12.707400, mean_q: 24.784412, mean_eps: 0.500000\n",
            " 6115/8000: episode: 201, duration: 1.970s, episode steps: 106, steps per second:  54, episode reward: 106.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.685949, mae: 12.839780, mean_q: 25.088235, mean_eps: 0.500000\n",
            " 6161/8000: episode: 202, duration: 0.880s, episode steps:  46, steps per second:  52, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.211545, mae: 13.067796, mean_q: 25.356269, mean_eps: 0.500000\n",
            " 6303/8000: episode: 203, duration: 3.220s, episode steps: 142, steps per second:  44, episode reward: 142.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 4.461032, mae: 13.135213, mean_q: 25.691972, mean_eps: 0.500000\n",
            " 6378/8000: episode: 204, duration: 1.418s, episode steps:  75, steps per second:  53, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 5.834136, mae: 13.353434, mean_q: 26.077096, mean_eps: 0.500000\n",
            " 6473/8000: episode: 205, duration: 1.968s, episode steps:  95, steps per second:  48, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 4.243758, mae: 13.493950, mean_q: 26.552997, mean_eps: 0.500000\n",
            " 6503/8000: episode: 206, duration: 0.749s, episode steps:  30, steps per second:  40, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.264159, mae: 13.700891, mean_q: 26.794668, mean_eps: 0.500000\n",
            " 6546/8000: episode: 207, duration: 1.163s, episode steps:  43, steps per second:  37, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 5.164788, mae: 13.666961, mean_q: 26.793316, mean_eps: 0.500000\n",
            " 6746/8000: episode: 208, duration: 3.963s, episode steps: 200, steps per second:  50, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 5.005911, mae: 13.887393, mean_q: 27.276258, mean_eps: 0.500000\n",
            " 6919/8000: episode: 209, duration: 4.154s, episode steps: 173, steps per second:  42, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 5.492770, mae: 14.197156, mean_q: 27.860359, mean_eps: 0.500000\n",
            " 7013/8000: episode: 210, duration: 1.834s, episode steps:  94, steps per second:  51, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 5.408937, mae: 14.445958, mean_q: 28.380178, mean_eps: 0.500000\n",
            " 7056/8000: episode: 211, duration: 0.743s, episode steps:  43, steps per second:  58, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 4.758886, mae: 14.401746, mean_q: 28.384230, mean_eps: 0.500000\n",
            " 7088/8000: episode: 212, duration: 0.618s, episode steps:  32, steps per second:  52, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 4.713669, mae: 14.466461, mean_q: 28.629237, mean_eps: 0.500000\n",
            " 7210/8000: episode: 213, duration: 1.848s, episode steps: 122, steps per second:  66, episode reward: 122.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 5.845465, mae: 14.588993, mean_q: 28.726400, mean_eps: 0.500000\n",
            " 7303/8000: episode: 214, duration: 1.348s, episode steps:  93, steps per second:  69, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 5.995838, mae: 14.784550, mean_q: 29.158185, mean_eps: 0.500000\n",
            " 7373/8000: episode: 215, duration: 1.093s, episode steps:  70, steps per second:  64, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 4.965897, mae: 14.911380, mean_q: 29.459919, mean_eps: 0.500000\n",
            " 7459/8000: episode: 216, duration: 1.670s, episode steps:  86, steps per second:  52, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 5.860899, mae: 15.023179, mean_q: 29.606452, mean_eps: 0.500000\n",
            " 7595/8000: episode: 217, duration: 2.322s, episode steps: 136, steps per second:  59, episode reward: 136.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 6.279101, mae: 15.228073, mean_q: 29.991436, mean_eps: 0.500000\n",
            " 7620/8000: episode: 218, duration: 0.686s, episode steps:  25, steps per second:  36, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 6.616205, mae: 15.140521, mean_q: 29.761098, mean_eps: 0.500000\n",
            " 7753/8000: episode: 219, duration: 2.789s, episode steps: 133, steps per second:  48, episode reward: 133.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 6.325343, mae: 15.400117, mean_q: 30.327569, mean_eps: 0.500000\n",
            " 7867/8000: episode: 220, duration: 1.890s, episode steps: 114, steps per second:  60, episode reward: 114.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 4.774402, mae: 15.589575, mean_q: 30.825020, mean_eps: 0.500000\n",
            "done, took 167.824 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 187.000, steps: 187\n",
            "Episode 2: reward: 172.000, steps: 172\n",
            "Episode 3: reward: 154.000, steps: 154\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 200.000, steps: 200\n",
            "Episode 6: reward: 178.000, steps: 178\n",
            "Episode 7: reward: 200.000, steps: 200\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 144.000, steps: 144\n",
            "Episode 10: reward: 197.000, steps: 197\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 174.000, steps: 174\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 167.000, steps: 167\n",
            "Episode 15: reward: 162.000, steps: 162\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 170.000, steps: 170\n",
            "Episode 18: reward: 200.000, steps: 200\n",
            "Episode 19: reward: 194.000, steps: 194\n",
            "Episode 20: reward: 164.000, steps: 164\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb170066910>"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    }
  ]
}