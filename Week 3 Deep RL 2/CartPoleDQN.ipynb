{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPoleDQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Megacity1/CartpoleDQN/blob/main/Week%203%20Deep%20RL%202/CartPoleDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Deep Q Network (DQN) for CartPole Using Boltzmann Q Policy\n",
        "This exercise implements a DQN for CartPole using a Boltzmann Q policy for selecting the actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGsC7cJ5jNcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda73cbf-3020-40f4-e45e-7b87053448f1"
      },
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.24.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (13.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.44.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.10.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMIHLgQ3Z-lF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5a8608-456f-4127-ce8a-f551e5f4e4f3"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0AMLzq08ap0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b380e1-6c3d-4bce-8233-a8ac01deaf26"
      },
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "!pip install Adam\n",
        "#from keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Adam in /usr/local/lib/python3.7/dist-packages (0.0.0.dev0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll6bNdUm54WS"
      },
      "source": [
        "Implementation of DQN for CartPole, applying policy BoltzmannQPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCrPKNy40PC"
      },
      "source": [
        "##Implement DQN with BoltzmannGumbelQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efM9jkXr5A3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b3295534-2b41-402d-ff9c-e839205417c9"
      },
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import BoltzmannGumbelQPolicy\n",
        "from rl.policy import LinearAnnealedPolicy\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n",
        "# setup experience replay buffer\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
        "                               attr='eps',            \n",
        "                               value_max=8.,\n",
        "                               value_min=.8, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=20)\n",
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "# add extra layers here\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=20,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy) \n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=8000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_44\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_42 (Flatten)        (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_85 (Dense)            (None, 16)                80        \n",
            "                                                                 \n",
            " dense_86 (Dense)            (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 8000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   31/8000: episode: 1, duration: 12.823s, episode steps:  31, steps per second:   2, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.581 [0.000, 1.000],  loss: 0.523987, mae: 0.558366, mean_q: 0.126187, mean_eps: 0.800000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   50/8000: episode: 2, duration: 0.437s, episode steps:  19, steps per second:  43, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.459496, mae: 0.556377, mean_q: 0.250469, mean_eps: 0.800000\n",
            "   76/8000: episode: 3, duration: 0.504s, episode steps:  26, steps per second:  52, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.395295, mae: 0.582930, mean_q: 0.393101, mean_eps: 0.800000\n",
            "   93/8000: episode: 4, duration: 0.280s, episode steps:  17, steps per second:  61, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.328316, mae: 0.595084, mean_q: 0.522511, mean_eps: 0.800000\n",
            "  116/8000: episode: 5, duration: 0.335s, episode steps:  23, steps per second:  69, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 0.300602, mae: 0.646137, mean_q: 0.669113, mean_eps: 0.800000\n",
            "  126/8000: episode: 6, duration: 0.164s, episode steps:  10, steps per second:  61, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.262705, mae: 0.659551, mean_q: 0.783970, mean_eps: 0.800000\n",
            "  155/8000: episode: 7, duration: 0.457s, episode steps:  29, steps per second:  63, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 0.233207, mae: 0.715008, mean_q: 0.956576, mean_eps: 0.800000\n",
            "  183/8000: episode: 8, duration: 0.479s, episode steps:  28, steps per second:  58, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.191722, mae: 0.800864, mean_q: 1.183644, mean_eps: 0.800000\n",
            "  197/8000: episode: 9, duration: 0.368s, episode steps:  14, steps per second:  38, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.214 [0.000, 1.000],  loss: 0.179293, mae: 0.852693, mean_q: 1.311673, mean_eps: 0.800000\n",
            "  207/8000: episode: 10, duration: 0.274s, episode steps:  10, steps per second:  37, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.175913, mae: 0.895287, mean_q: 1.409191, mean_eps: 0.800000\n",
            "  247/8000: episode: 11, duration: 0.911s, episode steps:  40, steps per second:  44, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.160586, mae: 1.000067, mean_q: 1.682605, mean_eps: 0.800000\n",
            "  258/8000: episode: 12, duration: 0.219s, episode steps:  11, steps per second:  50, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.200834, mae: 1.120008, mean_q: 1.945140, mean_eps: 0.800000\n",
            "  287/8000: episode: 13, duration: 0.555s, episode steps:  29, steps per second:  52, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 0.220407, mae: 1.186637, mean_q: 2.008418, mean_eps: 0.800000\n",
            "  300/8000: episode: 14, duration: 0.230s, episode steps:  13, steps per second:  56, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.168694, mae: 1.249298, mean_q: 2.161563, mean_eps: 0.800000\n",
            "  313/8000: episode: 15, duration: 0.241s, episode steps:  13, steps per second:  54, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.224253, mae: 1.322602, mean_q: 2.318540, mean_eps: 0.800000\n",
            "  327/8000: episode: 16, duration: 0.277s, episode steps:  14, steps per second:  51, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.259050, mae: 1.391420, mean_q: 2.430237, mean_eps: 0.800000\n",
            "  351/8000: episode: 17, duration: 0.589s, episode steps:  24, steps per second:  41, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.192614, mae: 1.432824, mean_q: 2.575119, mean_eps: 0.800000\n",
            "  368/8000: episode: 18, duration: 0.343s, episode steps:  17, steps per second:  50, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.265853, mae: 1.547629, mean_q: 2.759677, mean_eps: 0.800000\n",
            "  385/8000: episode: 19, duration: 0.449s, episode steps:  17, steps per second:  38, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.238226, mae: 1.586642, mean_q: 2.868711, mean_eps: 0.800000\n",
            "  406/8000: episode: 20, duration: 0.462s, episode steps:  21, steps per second:  45, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.187650, mae: 1.645535, mean_q: 3.018634, mean_eps: 0.800000\n",
            "  420/8000: episode: 21, duration: 0.314s, episode steps:  14, steps per second:  45, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.274071, mae: 1.740215, mean_q: 3.162909, mean_eps: 0.800000\n",
            "  437/8000: episode: 22, duration: 0.348s, episode steps:  17, steps per second:  49, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.290268, mae: 1.806337, mean_q: 3.269764, mean_eps: 0.800000\n",
            "  450/8000: episode: 23, duration: 0.286s, episode steps:  13, steps per second:  46, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.180089, mae: 1.800603, mean_q: 3.324217, mean_eps: 0.800000\n",
            "  469/8000: episode: 24, duration: 0.364s, episode steps:  19, steps per second:  52, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 0.231715, mae: 1.890323, mean_q: 3.526143, mean_eps: 0.800000\n",
            "  481/8000: episode: 25, duration: 0.217s, episode steps:  12, steps per second:  55, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.297231, mae: 1.954854, mean_q: 3.631336, mean_eps: 0.800000\n",
            "  498/8000: episode: 26, duration: 0.281s, episode steps:  17, steps per second:  61, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.324962, mae: 2.036086, mean_q: 3.754867, mean_eps: 0.800000\n",
            "  518/8000: episode: 27, duration: 0.338s, episode steps:  20, steps per second:  59, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.650 [0.000, 1.000],  loss: 0.337645, mae: 2.112376, mean_q: 3.901256, mean_eps: 0.800000\n",
            "  544/8000: episode: 28, duration: 0.435s, episode steps:  26, steps per second:  60, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.302758, mae: 2.175589, mean_q: 4.044043, mean_eps: 0.800000\n",
            "  556/8000: episode: 29, duration: 0.191s, episode steps:  12, steps per second:  63, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.378565, mae: 2.273663, mean_q: 4.174251, mean_eps: 0.800000\n",
            "  578/8000: episode: 30, duration: 0.321s, episode steps:  22, steps per second:  68, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  loss: 0.497586, mae: 2.363072, mean_q: 4.254726, mean_eps: 0.800000\n",
            "  593/8000: episode: 31, duration: 0.261s, episode steps:  15, steps per second:  58, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.296138, mae: 2.365965, mean_q: 4.343341, mean_eps: 0.800000\n",
            "  604/8000: episode: 32, duration: 0.175s, episode steps:  11, steps per second:  63, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.257578, mae: 2.428091, mean_q: 4.529131, mean_eps: 0.800000\n",
            "  618/8000: episode: 33, duration: 0.248s, episode steps:  14, steps per second:  57, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 0.458717, mae: 2.521660, mean_q: 4.686470, mean_eps: 0.800000\n",
            "  633/8000: episode: 34, duration: 0.247s, episode steps:  15, steps per second:  61, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.643975, mae: 2.607857, mean_q: 4.687402, mean_eps: 0.800000\n",
            "  644/8000: episode: 35, duration: 0.177s, episode steps:  11, steps per second:  62, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.536991, mae: 2.639685, mean_q: 4.702510, mean_eps: 0.800000\n",
            "  654/8000: episode: 36, duration: 0.173s, episode steps:  10, steps per second:  58, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.400503, mae: 2.629759, mean_q: 4.825996, mean_eps: 0.800000\n",
            "  663/8000: episode: 37, duration: 0.145s, episode steps:   9, steps per second:  62, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.560536, mae: 2.694955, mean_q: 4.932899, mean_eps: 0.800000\n",
            "  675/8000: episode: 38, duration: 0.196s, episode steps:  12, steps per second:  61, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.533223, mae: 2.724644, mean_q: 5.047160, mean_eps: 0.800000\n",
            "  697/8000: episode: 39, duration: 0.360s, episode steps:  22, steps per second:  61, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.318 [0.000, 1.000],  loss: 0.580946, mae: 2.793305, mean_q: 5.056751, mean_eps: 0.800000\n",
            "  710/8000: episode: 40, duration: 0.222s, episode steps:  13, steps per second:  59, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.539421, mae: 2.854266, mean_q: 5.238202, mean_eps: 0.800000\n",
            "  724/8000: episode: 41, duration: 0.226s, episode steps:  14, steps per second:  62, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.524815, mae: 2.874809, mean_q: 5.272818, mean_eps: 0.800000\n",
            "  737/8000: episode: 42, duration: 0.202s, episode steps:  13, steps per second:  64, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.154 [0.000, 1.000],  loss: 0.660240, mae: 2.946149, mean_q: 5.327279, mean_eps: 0.800000\n",
            "  754/8000: episode: 43, duration: 0.287s, episode steps:  17, steps per second:  59, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 0.782762, mae: 3.016723, mean_q: 5.399878, mean_eps: 0.800000\n",
            "  771/8000: episode: 44, duration: 0.267s, episode steps:  17, steps per second:  64, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 0.624009, mae: 3.026059, mean_q: 5.522506, mean_eps: 0.800000\n",
            "  786/8000: episode: 45, duration: 0.289s, episode steps:  15, steps per second:  52, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.683069, mae: 3.096533, mean_q: 5.642619, mean_eps: 0.800000\n",
            "  809/8000: episode: 46, duration: 0.363s, episode steps:  23, steps per second:  63, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 0.697272, mae: 3.157242, mean_q: 5.766518, mean_eps: 0.800000\n",
            "  835/8000: episode: 47, duration: 0.417s, episode steps:  26, steps per second:  62, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.701529, mae: 3.224172, mean_q: 5.892767, mean_eps: 0.800000\n",
            "  847/8000: episode: 48, duration: 0.179s, episode steps:  12, steps per second:  67, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.772927, mae: 3.289303, mean_q: 5.989070, mean_eps: 0.800000\n",
            "  859/8000: episode: 49, duration: 0.187s, episode steps:  12, steps per second:  64, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.645550, mae: 3.310516, mean_q: 6.088200, mean_eps: 0.800000\n",
            "  877/8000: episode: 50, duration: 0.305s, episode steps:  18, steps per second:  59, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.640512, mae: 3.325548, mean_q: 6.146787, mean_eps: 0.800000\n",
            "  887/8000: episode: 51, duration: 0.165s, episode steps:  10, steps per second:  61, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.939621, mae: 3.436716, mean_q: 6.261590, mean_eps: 0.800000\n",
            "  906/8000: episode: 52, duration: 0.285s, episode steps:  19, steps per second:  67, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.263 [0.000, 1.000],  loss: 1.059806, mae: 3.499499, mean_q: 6.314957, mean_eps: 0.800000\n",
            "  928/8000: episode: 53, duration: 0.310s, episode steps:  22, steps per second:  71, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.946957, mae: 3.534667, mean_q: 6.382007, mean_eps: 0.800000\n",
            "  984/8000: episode: 54, duration: 0.799s, episode steps:  56, steps per second:  70, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.848319, mae: 3.626913, mean_q: 6.689435, mean_eps: 0.800000\n",
            "  998/8000: episode: 55, duration: 0.223s, episode steps:  14, steps per second:  63, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.687924, mae: 3.689107, mean_q: 6.983990, mean_eps: 0.800000\n",
            " 1033/8000: episode: 56, duration: 0.592s, episode steps:  35, steps per second:  59, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 0.906409, mae: 3.827701, mean_q: 7.169865, mean_eps: 0.800000\n",
            " 1045/8000: episode: 57, duration: 0.279s, episode steps:  12, steps per second:  43, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.952372, mae: 3.917750, mean_q: 7.365949, mean_eps: 0.800000\n",
            " 1056/8000: episode: 58, duration: 0.256s, episode steps:  11, steps per second:  43, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.670629, mae: 3.919689, mean_q: 7.397698, mean_eps: 0.800000\n",
            " 1082/8000: episode: 59, duration: 0.619s, episode steps:  26, steps per second:  42, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.957027, mae: 4.023358, mean_q: 7.510796, mean_eps: 0.800000\n",
            " 1107/8000: episode: 60, duration: 0.593s, episode steps:  25, steps per second:  42, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 1.197911, mae: 4.127187, mean_q: 7.594075, mean_eps: 0.800000\n",
            " 1126/8000: episode: 61, duration: 0.436s, episode steps:  19, steps per second:  44, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 1.153010, mae: 4.148157, mean_q: 7.630801, mean_eps: 0.800000\n",
            " 1142/8000: episode: 62, duration: 0.325s, episode steps:  16, steps per second:  49, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.699529, mae: 4.145259, mean_q: 7.785219, mean_eps: 0.800000\n",
            " 1156/8000: episode: 63, duration: 0.294s, episode steps:  14, steps per second:  48, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.803845, mae: 4.224643, mean_q: 8.011863, mean_eps: 0.800000\n",
            " 1187/8000: episode: 64, duration: 0.689s, episode steps:  31, steps per second:  45, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 1.004246, mae: 4.294825, mean_q: 8.081508, mean_eps: 0.800000\n",
            " 1233/8000: episode: 65, duration: 1.049s, episode steps:  46, steps per second:  44, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 1.089177, mae: 4.398651, mean_q: 8.206398, mean_eps: 0.800000\n",
            " 1250/8000: episode: 66, duration: 0.433s, episode steps:  17, steps per second:  39, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 1.146323, mae: 4.479420, mean_q: 8.352239, mean_eps: 0.800000\n",
            " 1271/8000: episode: 67, duration: 0.498s, episode steps:  21, steps per second:  42, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.381 [0.000, 1.000],  loss: 1.139237, mae: 4.528332, mean_q: 8.431339, mean_eps: 0.800000\n",
            " 1313/8000: episode: 68, duration: 0.930s, episode steps:  42, steps per second:  45, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.365404, mae: 4.669105, mean_q: 8.697768, mean_eps: 0.800000\n",
            " 1325/8000: episode: 69, duration: 0.285s, episode steps:  12, steps per second:  42, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.288027, mae: 4.741417, mean_q: 8.774549, mean_eps: 0.800000\n",
            " 1351/8000: episode: 70, duration: 0.562s, episode steps:  26, steps per second:  46, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.577 [0.000, 1.000],  loss: 1.083233, mae: 4.731388, mean_q: 8.945231, mean_eps: 0.800000\n",
            " 1366/8000: episode: 71, duration: 0.290s, episode steps:  15, steps per second:  52, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.419985, mae: 4.864070, mean_q: 9.045392, mean_eps: 0.800000\n",
            " 1379/8000: episode: 72, duration: 0.268s, episode steps:  13, steps per second:  49, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 1.513381, mae: 4.925768, mean_q: 9.176696, mean_eps: 0.800000\n",
            " 1427/8000: episode: 73, duration: 0.820s, episode steps:  48, steps per second:  59, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 1.432773, mae: 4.949646, mean_q: 9.244235, mean_eps: 0.800000\n",
            " 1439/8000: episode: 74, duration: 0.183s, episode steps:  12, steps per second:  66, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 1.219819, mae: 4.997858, mean_q: 9.405598, mean_eps: 0.800000\n",
            " 1480/8000: episode: 75, duration: 0.595s, episode steps:  41, steps per second:  69, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 1.208418, mae: 5.081356, mean_q: 9.608306, mean_eps: 0.800000\n",
            " 1493/8000: episode: 76, duration: 0.220s, episode steps:  13, steps per second:  59, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 1.627513, mae: 5.195188, mean_q: 9.730379, mean_eps: 0.800000\n",
            " 1503/8000: episode: 77, duration: 0.170s, episode steps:  10, steps per second:  59, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 1.732512, mae: 5.229267, mean_q: 9.704626, mean_eps: 0.800000\n",
            " 1539/8000: episode: 78, duration: 0.544s, episode steps:  36, steps per second:  66, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 1.427563, mae: 5.242143, mean_q: 9.847575, mean_eps: 0.800000\n",
            " 1552/8000: episode: 79, duration: 0.196s, episode steps:  13, steps per second:  66, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 1.316199, mae: 5.327072, mean_q: 10.080076, mean_eps: 0.800000\n",
            " 1566/8000: episode: 80, duration: 0.225s, episode steps:  14, steps per second:  62, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 2.081265, mae: 5.404476, mean_q: 10.024101, mean_eps: 0.800000\n",
            " 1620/8000: episode: 81, duration: 0.912s, episode steps:  54, steps per second:  59, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 1.778750, mae: 5.464730, mean_q: 10.156034, mean_eps: 0.800000\n",
            " 1634/8000: episode: 82, duration: 0.273s, episode steps:  14, steps per second:  51, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 1.943025, mae: 5.526611, mean_q: 10.233267, mean_eps: 0.800000\n",
            " 1693/8000: episode: 83, duration: 1.098s, episode steps:  59, steps per second:  54, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 1.469602, mae: 5.559955, mean_q: 10.463067, mean_eps: 0.800000\n",
            " 1709/8000: episode: 84, duration: 0.316s, episode steps:  16, steps per second:  51, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 1.499006, mae: 5.680116, mean_q: 10.784716, mean_eps: 0.800000\n",
            " 1744/8000: episode: 85, duration: 0.657s, episode steps:  35, steps per second:  53, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 2.106338, mae: 5.771370, mean_q: 10.833166, mean_eps: 0.800000\n",
            " 1784/8000: episode: 86, duration: 0.865s, episode steps:  40, steps per second:  46, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 1.720856, mae: 5.813405, mean_q: 10.932428, mean_eps: 0.800000\n",
            " 1800/8000: episode: 87, duration: 0.324s, episode steps:  16, steps per second:  49, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 1.756655, mae: 5.902653, mean_q: 11.121899, mean_eps: 0.800000\n",
            " 1816/8000: episode: 88, duration: 0.293s, episode steps:  16, steps per second:  55, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.696772, mae: 5.902587, mean_q: 11.204238, mean_eps: 0.800000\n",
            " 1840/8000: episode: 89, duration: 0.444s, episode steps:  24, steps per second:  54, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 1.566874, mae: 5.950385, mean_q: 11.400566, mean_eps: 0.800000\n",
            " 1861/8000: episode: 90, duration: 0.385s, episode steps:  21, steps per second:  55, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.560439, mae: 6.070434, mean_q: 11.628429, mean_eps: 0.800000\n",
            " 1877/8000: episode: 91, duration: 0.340s, episode steps:  16, steps per second:  47, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 2.418486, mae: 6.144115, mean_q: 11.515044, mean_eps: 0.800000\n",
            " 1913/8000: episode: 92, duration: 0.747s, episode steps:  36, steps per second:  48, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 2.263981, mae: 6.167967, mean_q: 11.568276, mean_eps: 0.800000\n",
            " 1953/8000: episode: 93, duration: 0.602s, episode steps:  40, steps per second:  66, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.847252, mae: 6.259912, mean_q: 11.885445, mean_eps: 0.800000\n",
            " 1974/8000: episode: 94, duration: 0.362s, episode steps:  21, steps per second:  58, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 1.626116, mae: 6.325886, mean_q: 12.067677, mean_eps: 0.800000\n",
            " 1985/8000: episode: 95, duration: 0.265s, episode steps:  11, steps per second:  41, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 1.671970, mae: 6.336914, mean_q: 12.122630, mean_eps: 0.800000\n",
            " 2018/8000: episode: 96, duration: 0.682s, episode steps:  33, steps per second:  48, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 2.028571, mae: 6.425238, mean_q: 12.217568, mean_eps: 0.800000\n",
            " 2033/8000: episode: 97, duration: 0.304s, episode steps:  15, steps per second:  49, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 2.466697, mae: 6.507632, mean_q: 12.224610, mean_eps: 0.800000\n",
            " 2049/8000: episode: 98, duration: 0.348s, episode steps:  16, steps per second:  46, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 1.521927, mae: 6.518109, mean_q: 12.421535, mean_eps: 0.800000\n",
            " 2066/8000: episode: 99, duration: 0.372s, episode steps:  17, steps per second:  46, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 1.595336, mae: 6.474207, mean_q: 12.440303, mean_eps: 0.800000\n",
            " 2077/8000: episode: 100, duration: 0.238s, episode steps:  11, steps per second:  46, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 1.754840, mae: 6.593878, mean_q: 12.656594, mean_eps: 0.800000\n",
            " 2126/8000: episode: 101, duration: 0.913s, episode steps:  49, steps per second:  54, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 1.577690, mae: 6.660222, mean_q: 12.835262, mean_eps: 0.800000\n",
            " 2146/8000: episode: 102, duration: 0.335s, episode steps:  20, steps per second:  60, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.018708, mae: 6.821930, mean_q: 13.002437, mean_eps: 0.800000\n",
            " 2166/8000: episode: 103, duration: 0.291s, episode steps:  20, steps per second:  69, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.549732, mae: 6.831882, mean_q: 12.859784, mean_eps: 0.800000\n",
            " 2193/8000: episode: 104, duration: 0.396s, episode steps:  27, steps per second:  68, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.563854, mae: 6.869722, mean_q: 13.259417, mean_eps: 0.800000\n",
            " 2230/8000: episode: 105, duration: 0.495s, episode steps:  37, steps per second:  75, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 1.967590, mae: 6.943632, mean_q: 13.345591, mean_eps: 0.800000\n",
            " 2254/8000: episode: 106, duration: 0.368s, episode steps:  24, steps per second:  65, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 1.616617, mae: 6.997453, mean_q: 13.546531, mean_eps: 0.800000\n",
            " 2277/8000: episode: 107, duration: 0.542s, episode steps:  23, steps per second:  42, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 2.564975, mae: 7.123796, mean_q: 13.612963, mean_eps: 0.800000\n",
            " 2321/8000: episode: 108, duration: 0.831s, episode steps:  44, steps per second:  53, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 2.509284, mae: 7.206506, mean_q: 13.685603, mean_eps: 0.800000\n",
            " 2347/8000: episode: 109, duration: 0.414s, episode steps:  26, steps per second:  63, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.258509, mae: 7.324706, mean_q: 13.790360, mean_eps: 0.800000\n",
            " 2361/8000: episode: 110, duration: 0.234s, episode steps:  14, steps per second:  60, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 2.589205, mae: 7.324278, mean_q: 13.912110, mean_eps: 0.800000\n",
            " 2404/8000: episode: 111, duration: 0.948s, episode steps:  43, steps per second:  45, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 1.915379, mae: 7.341542, mean_q: 14.121649, mean_eps: 0.800000\n",
            " 2416/8000: episode: 112, duration: 0.325s, episode steps:  12, steps per second:  37, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 2.778563, mae: 7.472036, mean_q: 14.362001, mean_eps: 0.800000\n",
            " 2453/8000: episode: 113, duration: 0.935s, episode steps:  37, steps per second:  40, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 1.995492, mae: 7.474840, mean_q: 14.377394, mean_eps: 0.800000\n",
            " 2478/8000: episode: 114, duration: 0.511s, episode steps:  25, steps per second:  49, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 3.134400, mae: 7.678640, mean_q: 14.589818, mean_eps: 0.800000\n",
            " 2516/8000: episode: 115, duration: 0.846s, episode steps:  38, steps per second:  45, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.966054, mae: 7.680521, mean_q: 14.621793, mean_eps: 0.800000\n",
            " 2544/8000: episode: 116, duration: 0.612s, episode steps:  28, steps per second:  46, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 2.512732, mae: 7.710852, mean_q: 14.675878, mean_eps: 0.800000\n",
            " 2555/8000: episode: 117, duration: 0.280s, episode steps:  11, steps per second:  39, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 1.985187, mae: 7.794662, mean_q: 15.088576, mean_eps: 0.800000\n",
            " 2579/8000: episode: 118, duration: 0.517s, episode steps:  24, steps per second:  46, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 2.422527, mae: 7.812952, mean_q: 15.137730, mean_eps: 0.800000\n",
            " 2603/8000: episode: 119, duration: 0.498s, episode steps:  24, steps per second:  48, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 3.061330, mae: 7.917502, mean_q: 15.044937, mean_eps: 0.800000\n",
            " 2620/8000: episode: 120, duration: 0.360s, episode steps:  17, steps per second:  47, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 3.439037, mae: 7.963132, mean_q: 15.041873, mean_eps: 0.800000\n",
            " 2638/8000: episode: 121, duration: 0.403s, episode steps:  18, steps per second:  45, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 2.507312, mae: 7.925286, mean_q: 15.176916, mean_eps: 0.800000\n",
            " 2689/8000: episode: 122, duration: 0.804s, episode steps:  51, steps per second:  63, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 3.106132, mae: 8.058702, mean_q: 15.422889, mean_eps: 0.800000\n",
            " 2706/8000: episode: 123, duration: 0.307s, episode steps:  17, steps per second:  55, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 2.297583, mae: 8.089441, mean_q: 15.530417, mean_eps: 0.800000\n",
            " 2718/8000: episode: 124, duration: 0.248s, episode steps:  12, steps per second:  48, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 3.811474, mae: 8.163224, mean_q: 15.538850, mean_eps: 0.800000\n",
            " 2743/8000: episode: 125, duration: 0.492s, episode steps:  25, steps per second:  51, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 2.146731, mae: 8.140775, mean_q: 15.699194, mean_eps: 0.800000\n",
            " 2791/8000: episode: 126, duration: 0.847s, episode steps:  48, steps per second:  57, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 2.772683, mae: 8.251624, mean_q: 15.868549, mean_eps: 0.800000\n",
            " 2807/8000: episode: 127, duration: 0.249s, episode steps:  16, steps per second:  64, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 3.753370, mae: 8.374277, mean_q: 15.956880, mean_eps: 0.800000\n",
            " 2854/8000: episode: 128, duration: 0.787s, episode steps:  47, steps per second:  60, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 2.906499, mae: 8.372619, mean_q: 15.977760, mean_eps: 0.800000\n",
            " 2866/8000: episode: 129, duration: 0.218s, episode steps:  12, steps per second:  55, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 2.890059, mae: 8.420281, mean_q: 16.185673, mean_eps: 0.800000\n",
            " 2887/8000: episode: 130, duration: 0.425s, episode steps:  21, steps per second:  49, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 2.573155, mae: 8.486244, mean_q: 16.380768, mean_eps: 0.800000\n",
            " 2939/8000: episode: 131, duration: 1.048s, episode steps:  52, steps per second:  50, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.442 [0.000, 1.000],  loss: 2.930151, mae: 8.559771, mean_q: 16.396774, mean_eps: 0.800000\n",
            " 3006/8000: episode: 132, duration: 1.140s, episode steps:  67, steps per second:  59, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 3.118570, mae: 8.658007, mean_q: 16.593988, mean_eps: 0.800000\n",
            " 3018/8000: episode: 133, duration: 0.250s, episode steps:  12, steps per second:  48, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 2.064348, mae: 8.618036, mean_q: 16.752186, mean_eps: 0.800000\n",
            " 3030/8000: episode: 134, duration: 0.244s, episode steps:  12, steps per second:  49, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 2.773450, mae: 8.737585, mean_q: 16.872243, mean_eps: 0.800000\n",
            " 3056/8000: episode: 135, duration: 0.633s, episode steps:  26, steps per second:  41, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.484157, mae: 8.738311, mean_q: 16.887055, mean_eps: 0.800000\n",
            " 3072/8000: episode: 136, duration: 0.336s, episode steps:  16, steps per second:  48, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 2.743103, mae: 8.866725, mean_q: 17.144328, mean_eps: 0.800000\n",
            " 3093/8000: episode: 137, duration: 0.517s, episode steps:  21, steps per second:  41, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 2.862425, mae: 8.885942, mean_q: 17.183015, mean_eps: 0.800000\n",
            " 3116/8000: episode: 138, duration: 0.515s, episode steps:  23, steps per second:  45, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 2.746357, mae: 8.860980, mean_q: 17.120940, mean_eps: 0.800000\n",
            " 3140/8000: episode: 139, duration: 0.428s, episode steps:  24, steps per second:  56, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 3.324683, mae: 8.974710, mean_q: 17.326325, mean_eps: 0.800000\n",
            " 3184/8000: episode: 140, duration: 0.651s, episode steps:  44, steps per second:  68, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 2.917282, mae: 9.038075, mean_q: 17.478706, mean_eps: 0.800000\n",
            " 3205/8000: episode: 141, duration: 0.312s, episode steps:  21, steps per second:  67, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 2.720520, mae: 9.173215, mean_q: 17.775635, mean_eps: 0.800000\n",
            " 3232/8000: episode: 142, duration: 0.421s, episode steps:  27, steps per second:  64, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 4.074281, mae: 9.263795, mean_q: 17.678350, mean_eps: 0.800000\n",
            " 3259/8000: episode: 143, duration: 0.373s, episode steps:  27, steps per second:  72, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 3.513181, mae: 9.186682, mean_q: 17.634999, mean_eps: 0.800000\n",
            " 3291/8000: episode: 144, duration: 0.624s, episode steps:  32, steps per second:  51, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 3.989248, mae: 9.347946, mean_q: 17.909139, mean_eps: 0.800000\n",
            " 3308/8000: episode: 145, duration: 0.381s, episode steps:  17, steps per second:  45, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 3.717212, mae: 9.353842, mean_q: 17.973063, mean_eps: 0.800000\n",
            " 3320/8000: episode: 146, duration: 0.294s, episode steps:  12, steps per second:  41, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 3.521971, mae: 9.418617, mean_q: 18.113049, mean_eps: 0.800000\n",
            " 3358/8000: episode: 147, duration: 0.773s, episode steps:  38, steps per second:  49, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  loss: 3.589953, mae: 9.383747, mean_q: 17.957637, mean_eps: 0.800000\n",
            " 3368/8000: episode: 148, duration: 0.198s, episode steps:  10, steps per second:  51, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 4.756456, mae: 9.462630, mean_q: 18.063403, mean_eps: 0.800000\n",
            " 3395/8000: episode: 149, duration: 0.545s, episode steps:  27, steps per second:  50, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 3.201565, mae: 9.429804, mean_q: 18.244960, mean_eps: 0.800000\n",
            " 3434/8000: episode: 150, duration: 0.726s, episode steps:  39, steps per second:  54, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 3.323851, mae: 9.539567, mean_q: 18.425517, mean_eps: 0.800000\n",
            " 3454/8000: episode: 151, duration: 0.384s, episode steps:  20, steps per second:  52, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 4.198442, mae: 9.619276, mean_q: 18.452834, mean_eps: 0.800000\n",
            " 3470/8000: episode: 152, duration: 0.295s, episode steps:  16, steps per second:  54, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 3.609437, mae: 9.678946, mean_q: 18.601880, mean_eps: 0.800000\n",
            " 3483/8000: episode: 153, duration: 0.204s, episode steps:  13, steps per second:  64, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 3.187862, mae: 9.666864, mean_q: 18.686861, mean_eps: 0.800000\n",
            " 3498/8000: episode: 154, duration: 0.260s, episode steps:  15, steps per second:  58, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 4.073886, mae: 9.678941, mean_q: 18.710784, mean_eps: 0.800000\n",
            " 3533/8000: episode: 155, duration: 0.616s, episode steps:  35, steps per second:  57, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 4.133818, mae: 9.761904, mean_q: 18.690508, mean_eps: 0.800000\n",
            " 3562/8000: episode: 156, duration: 0.421s, episode steps:  29, steps per second:  69, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 4.540307, mae: 9.716193, mean_q: 18.595763, mean_eps: 0.800000\n",
            " 3573/8000: episode: 157, duration: 0.174s, episode steps:  11, steps per second:  63, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 3.691927, mae: 9.725926, mean_q: 18.703441, mean_eps: 0.800000\n",
            " 3591/8000: episode: 158, duration: 0.395s, episode steps:  18, steps per second:  46, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 3.395936, mae: 9.716718, mean_q: 18.713908, mean_eps: 0.800000\n",
            " 3661/8000: episode: 159, duration: 1.452s, episode steps:  70, steps per second:  48, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.266650, mae: 9.897264, mean_q: 19.018494, mean_eps: 0.800000\n",
            " 3672/8000: episode: 160, duration: 0.238s, episode steps:  11, steps per second:  46, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 4.075290, mae: 9.915868, mean_q: 19.069379, mean_eps: 0.800000\n",
            " 3696/8000: episode: 161, duration: 0.536s, episode steps:  24, steps per second:  45, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.284338, mae: 9.865574, mean_q: 19.079798, mean_eps: 0.800000\n",
            " 3745/8000: episode: 162, duration: 0.878s, episode steps:  49, steps per second:  56, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  loss: 4.797595, mae: 10.104659, mean_q: 19.373365, mean_eps: 0.800000\n",
            " 3762/8000: episode: 163, duration: 0.262s, episode steps:  17, steps per second:  65, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 4.669030, mae: 10.089251, mean_q: 19.249117, mean_eps: 0.800000\n",
            " 3802/8000: episode: 164, duration: 0.604s, episode steps:  40, steps per second:  66, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.280323, mae: 10.048660, mean_q: 19.298109, mean_eps: 0.800000\n",
            " 3832/8000: episode: 165, duration: 0.450s, episode steps:  30, steps per second:  67, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.567 [0.000, 1.000],  loss: 3.778248, mae: 9.966579, mean_q: 19.170272, mean_eps: 0.800000\n",
            " 3855/8000: episode: 166, duration: 0.441s, episode steps:  23, steps per second:  52, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 3.855804, mae: 10.186190, mean_q: 19.699350, mean_eps: 0.800000\n",
            " 3923/8000: episode: 167, duration: 1.380s, episode steps:  68, steps per second:  49, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.431410, mae: 10.211737, mean_q: 19.805042, mean_eps: 0.800000\n",
            " 3957/8000: episode: 168, duration: 0.543s, episode steps:  34, steps per second:  63, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.670631, mae: 10.347715, mean_q: 20.000498, mean_eps: 0.800000\n",
            " 3974/8000: episode: 169, duration: 0.305s, episode steps:  17, steps per second:  56, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 3.515747, mae: 10.455332, mean_q: 20.361585, mean_eps: 0.800000\n",
            " 4034/8000: episode: 170, duration: 1.048s, episode steps:  60, steps per second:  57, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 5.108926, mae: 10.493680, mean_q: 20.164764, mean_eps: 0.800000\n",
            " 4062/8000: episode: 171, duration: 0.632s, episode steps:  28, steps per second:  44, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.770980, mae: 10.539187, mean_q: 20.426507, mean_eps: 0.800000\n",
            " 4104/8000: episode: 172, duration: 0.799s, episode steps:  42, steps per second:  53, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 3.767426, mae: 10.596891, mean_q: 20.633039, mean_eps: 0.800000\n",
            " 4117/8000: episode: 173, duration: 0.264s, episode steps:  13, steps per second:  49, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 4.797746, mae: 10.717218, mean_q: 20.754178, mean_eps: 0.800000\n",
            " 4156/8000: episode: 174, duration: 0.745s, episode steps:  39, steps per second:  52, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 4.675498, mae: 10.739319, mean_q: 20.757577, mean_eps: 0.800000\n",
            " 4266/8000: episode: 175, duration: 1.739s, episode steps: 110, steps per second:  63, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 4.880254, mae: 10.886460, mean_q: 21.025979, mean_eps: 0.800000\n",
            " 4304/8000: episode: 176, duration: 0.507s, episode steps:  38, steps per second:  75, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 3.422827, mae: 11.040171, mean_q: 21.487303, mean_eps: 0.800000\n",
            " 4346/8000: episode: 177, duration: 0.587s, episode steps:  42, steps per second:  72, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 4.801226, mae: 11.131137, mean_q: 21.595619, mean_eps: 0.800000\n",
            " 4368/8000: episode: 178, duration: 0.308s, episode steps:  22, steps per second:  71, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.013154, mae: 11.243337, mean_q: 21.750236, mean_eps: 0.800000\n",
            " 4456/8000: episode: 179, duration: 1.244s, episode steps:  88, steps per second:  71, episode reward: 88.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 4.466301, mae: 11.206629, mean_q: 21.722148, mean_eps: 0.800000\n",
            " 4486/8000: episode: 180, duration: 0.483s, episode steps:  30, steps per second:  62, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.891487, mae: 11.420894, mean_q: 22.249014, mean_eps: 0.800000\n",
            " 4521/8000: episode: 181, duration: 0.516s, episode steps:  35, steps per second:  68, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 4.614054, mae: 11.481350, mean_q: 22.301952, mean_eps: 0.800000\n",
            " 4567/8000: episode: 182, duration: 0.646s, episode steps:  46, steps per second:  71, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 5.236130, mae: 11.517784, mean_q: 22.267150, mean_eps: 0.800000\n",
            " 4613/8000: episode: 183, duration: 0.665s, episode steps:  46, steps per second:  69, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 4.647730, mae: 11.597249, mean_q: 22.473841, mean_eps: 0.800000\n",
            " 4633/8000: episode: 184, duration: 0.332s, episode steps:  20, steps per second:  60, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 4.812837, mae: 11.618919, mean_q: 22.655111, mean_eps: 0.800000\n",
            " 4682/8000: episode: 185, duration: 0.697s, episode steps:  49, steps per second:  70, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 5.058128, mae: 11.727851, mean_q: 22.708897, mean_eps: 0.800000\n",
            " 4736/8000: episode: 186, duration: 0.839s, episode steps:  54, steps per second:  64, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.053143, mae: 11.824006, mean_q: 22.809096, mean_eps: 0.800000\n",
            " 4785/8000: episode: 187, duration: 0.736s, episode steps:  49, steps per second:  67, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 5.521138, mae: 11.935577, mean_q: 23.141774, mean_eps: 0.800000\n",
            " 4809/8000: episode: 188, duration: 0.353s, episode steps:  24, steps per second:  68, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 5.061442, mae: 11.898322, mean_q: 23.078901, mean_eps: 0.800000\n",
            " 4844/8000: episode: 189, duration: 0.660s, episode steps:  35, steps per second:  53, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 5.836591, mae: 12.044879, mean_q: 23.414572, mean_eps: 0.800000\n",
            " 4874/8000: episode: 190, duration: 0.609s, episode steps:  30, steps per second:  49, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 4.493722, mae: 12.059587, mean_q: 23.491328, mean_eps: 0.800000\n",
            " 4891/8000: episode: 191, duration: 0.329s, episode steps:  17, steps per second:  52, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 4.787993, mae: 11.932665, mean_q: 23.271685, mean_eps: 0.800000\n",
            " 4934/8000: episode: 192, duration: 0.720s, episode steps:  43, steps per second:  60, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 4.051630, mae: 12.163699, mean_q: 23.814411, mean_eps: 0.800000\n",
            " 4962/8000: episode: 193, duration: 0.392s, episode steps:  28, steps per second:  71, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 2.974556, mae: 12.186147, mean_q: 24.000473, mean_eps: 0.800000\n",
            " 4983/8000: episode: 194, duration: 0.294s, episode steps:  21, steps per second:  71, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 3.278088, mae: 12.383653, mean_q: 24.381925, mean_eps: 0.800000\n",
            " 5001/8000: episode: 195, duration: 0.260s, episode steps:  18, steps per second:  69, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 5.361120, mae: 12.428335, mean_q: 24.297279, mean_eps: 0.800000\n",
            " 5102/8000: episode: 196, duration: 1.527s, episode steps: 101, steps per second:  66, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 6.372730, mae: 12.388374, mean_q: 24.022728, mean_eps: 0.800000\n",
            " 5128/8000: episode: 197, duration: 0.551s, episode steps:  26, steps per second:  47, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 4.861136, mae: 12.608125, mean_q: 24.656974, mean_eps: 0.800000\n",
            " 5152/8000: episode: 198, duration: 0.485s, episode steps:  24, steps per second:  49, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.289044, mae: 12.549612, mean_q: 24.728024, mean_eps: 0.800000\n",
            " 5255/8000: episode: 199, duration: 1.797s, episode steps: 103, steps per second:  57, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 5.382436, mae: 12.718705, mean_q: 24.872693, mean_eps: 0.800000\n",
            " 5302/8000: episode: 200, duration: 0.801s, episode steps:  47, steps per second:  59, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 6.356974, mae: 12.839409, mean_q: 24.952971, mean_eps: 0.800000\n",
            " 5366/8000: episode: 201, duration: 1.033s, episode steps:  64, steps per second:  62, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.767648, mae: 12.930898, mean_q: 25.209020, mean_eps: 0.800000\n",
            " 5386/8000: episode: 202, duration: 0.328s, episode steps:  20, steps per second:  61, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 4.972770, mae: 12.930592, mean_q: 25.290466, mean_eps: 0.800000\n",
            " 5417/8000: episode: 203, duration: 0.467s, episode steps:  31, steps per second:  66, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.581 [0.000, 1.000],  loss: 3.752111, mae: 13.057252, mean_q: 25.665201, mean_eps: 0.800000\n",
            " 5452/8000: episode: 204, duration: 0.510s, episode steps:  35, steps per second:  69, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 7.114473, mae: 13.169847, mean_q: 25.619768, mean_eps: 0.800000\n",
            " 5512/8000: episode: 205, duration: 0.876s, episode steps:  60, steps per second:  68, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 6.155353, mae: 13.263883, mean_q: 25.866811, mean_eps: 0.800000\n",
            " 5581/8000: episode: 206, duration: 1.151s, episode steps:  69, steps per second:  60, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 5.890638, mae: 13.302716, mean_q: 26.003823, mean_eps: 0.800000\n",
            " 5617/8000: episode: 207, duration: 0.753s, episode steps:  36, steps per second:  48, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 7.173792, mae: 13.328606, mean_q: 25.915838, mean_eps: 0.800000\n",
            " 5640/8000: episode: 208, duration: 0.447s, episode steps:  23, steps per second:  51, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  loss: 7.116800, mae: 13.507364, mean_q: 26.297838, mean_eps: 0.800000\n",
            " 5686/8000: episode: 209, duration: 0.785s, episode steps:  46, steps per second:  59, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.290969, mae: 13.606932, mean_q: 26.562153, mean_eps: 0.800000\n",
            " 5724/8000: episode: 210, duration: 0.538s, episode steps:  38, steps per second:  71, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.572806, mae: 13.573108, mean_q: 26.338277, mean_eps: 0.800000\n",
            " 5789/8000: episode: 211, duration: 0.923s, episode steps:  65, steps per second:  70, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.446 [0.000, 1.000],  loss: 5.779755, mae: 13.564579, mean_q: 26.471738, mean_eps: 0.800000\n",
            " 5808/8000: episode: 212, duration: 0.327s, episode steps:  19, steps per second:  58, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 8.236993, mae: 13.733078, mean_q: 26.555624, mean_eps: 0.800000\n",
            " 5826/8000: episode: 213, duration: 0.289s, episode steps:  18, steps per second:  62, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 7.146782, mae: 13.564397, mean_q: 26.324872, mean_eps: 0.800000\n",
            " 5839/8000: episode: 214, duration: 0.194s, episode steps:  13, steps per second:  67, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 3.865407, mae: 13.617626, mean_q: 26.790285, mean_eps: 0.800000\n",
            " 5869/8000: episode: 215, duration: 0.541s, episode steps:  30, steps per second:  55, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.596225, mae: 13.784876, mean_q: 27.028703, mean_eps: 0.800000\n",
            " 5917/8000: episode: 216, duration: 1.027s, episode steps:  48, steps per second:  47, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.671287, mae: 13.890424, mean_q: 27.158831, mean_eps: 0.800000\n",
            " 5986/8000: episode: 217, duration: 1.568s, episode steps:  69, steps per second:  44, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 5.213082, mae: 13.954833, mean_q: 27.428434, mean_eps: 0.800000\n",
            " 6054/8000: episode: 218, duration: 1.470s, episode steps:  68, steps per second:  46, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 5.571859, mae: 14.118262, mean_q: 27.739358, mean_eps: 0.800000\n",
            " 6082/8000: episode: 219, duration: 0.456s, episode steps:  28, steps per second:  61, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 6.043750, mae: 14.203352, mean_q: 27.867600, mean_eps: 0.800000\n",
            " 6100/8000: episode: 220, duration: 0.283s, episode steps:  18, steps per second:  64, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 5.066327, mae: 14.314560, mean_q: 28.193972, mean_eps: 0.800000\n",
            " 6122/8000: episode: 221, duration: 0.404s, episode steps:  22, steps per second:  54, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 7.593840, mae: 14.303064, mean_q: 27.949640, mean_eps: 0.800000\n",
            " 6155/8000: episode: 222, duration: 0.693s, episode steps:  33, steps per second:  48, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 5.334793, mae: 14.308424, mean_q: 28.186795, mean_eps: 0.800000\n",
            " 6177/8000: episode: 223, duration: 0.387s, episode steps:  22, steps per second:  57, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.318 [0.000, 1.000],  loss: 6.777543, mae: 14.376730, mean_q: 28.298262, mean_eps: 0.800000\n",
            " 6206/8000: episode: 224, duration: 0.450s, episode steps:  29, steps per second:  64, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 8.279867, mae: 14.528276, mean_q: 28.316425, mean_eps: 0.800000\n",
            " 6265/8000: episode: 225, duration: 0.897s, episode steps:  59, steps per second:  66, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.424 [0.000, 1.000],  loss: 8.223267, mae: 14.596447, mean_q: 28.519314, mean_eps: 0.800000\n",
            " 6291/8000: episode: 226, duration: 0.395s, episode steps:  26, steps per second:  66, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 5.794789, mae: 14.448459, mean_q: 28.394206, mean_eps: 0.800000\n",
            " 6327/8000: episode: 227, duration: 0.543s, episode steps:  36, steps per second:  66, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.604242, mae: 14.471567, mean_q: 28.501880, mean_eps: 0.800000\n",
            " 6382/8000: episode: 228, duration: 0.968s, episode steps:  55, steps per second:  57, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 7.176984, mae: 14.584087, mean_q: 28.588498, mean_eps: 0.800000\n",
            " 6413/8000: episode: 229, duration: 0.546s, episode steps:  31, steps per second:  57, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 9.131684, mae: 14.799977, mean_q: 28.824218, mean_eps: 0.800000\n",
            " 6425/8000: episode: 230, duration: 0.194s, episode steps:  12, steps per second:  62, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 8.962793, mae: 14.816017, mean_q: 28.777968, mean_eps: 0.800000\n",
            " 6445/8000: episode: 231, duration: 0.297s, episode steps:  20, steps per second:  67, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 10.439879, mae: 14.822319, mean_q: 28.705555, mean_eps: 0.800000\n",
            " 6457/8000: episode: 232, duration: 0.192s, episode steps:  12, steps per second:  63, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 5.196937, mae: 14.590077, mean_q: 28.551645, mean_eps: 0.800000\n",
            " 6503/8000: episode: 233, duration: 0.787s, episode steps:  46, steps per second:  58, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 7.172343, mae: 14.908259, mean_q: 29.215259, mean_eps: 0.800000\n",
            " 6524/8000: episode: 234, duration: 0.415s, episode steps:  21, steps per second:  51, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 7.440193, mae: 14.891278, mean_q: 29.140114, mean_eps: 0.800000\n",
            " 6558/8000: episode: 235, duration: 0.613s, episode steps:  34, steps per second:  55, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 9.606467, mae: 14.944151, mean_q: 29.018190, mean_eps: 0.800000\n",
            " 6569/8000: episode: 236, duration: 0.184s, episode steps:  11, steps per second:  60, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 3.606825, mae: 14.700599, mean_q: 29.060021, mean_eps: 0.800000\n",
            " 6632/8000: episode: 237, duration: 1.257s, episode steps:  63, steps per second:  50, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 7.065637, mae: 15.001601, mean_q: 29.518403, mean_eps: 0.800000\n",
            " 6647/8000: episode: 238, duration: 0.314s, episode steps:  15, steps per second:  48, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 6.176960, mae: 15.128567, mean_q: 29.724833, mean_eps: 0.800000\n",
            " 6680/8000: episode: 239, duration: 0.734s, episode steps:  33, steps per second:  45, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 8.347383, mae: 15.261223, mean_q: 29.867007, mean_eps: 0.800000\n",
            " 6719/8000: episode: 240, duration: 0.626s, episode steps:  39, steps per second:  62, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 6.317310, mae: 15.083200, mean_q: 29.586717, mean_eps: 0.800000\n",
            " 6745/8000: episode: 241, duration: 0.378s, episode steps:  26, steps per second:  69, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 8.671484, mae: 15.268756, mean_q: 29.844762, mean_eps: 0.800000\n",
            " 6777/8000: episode: 242, duration: 0.440s, episode steps:  32, steps per second:  73, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.594 [0.000, 1.000],  loss: 6.055477, mae: 15.095929, mean_q: 29.715820, mean_eps: 0.800000\n",
            " 6819/8000: episode: 243, duration: 0.624s, episode steps:  42, steps per second:  67, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.405 [0.000, 1.000],  loss: 8.054995, mae: 15.324463, mean_q: 29.998040, mean_eps: 0.800000\n",
            " 6845/8000: episode: 244, duration: 0.386s, episode steps:  26, steps per second:  67, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.577 [0.000, 1.000],  loss: 4.571598, mae: 15.376318, mean_q: 30.410563, mean_eps: 0.800000\n",
            " 6883/8000: episode: 245, duration: 0.623s, episode steps:  38, steps per second:  61, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 6.966120, mae: 15.524558, mean_q: 30.533044, mean_eps: 0.800000\n",
            " 6902/8000: episode: 246, duration: 0.301s, episode steps:  19, steps per second:  63, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 10.112827, mae: 15.575564, mean_q: 30.455594, mean_eps: 0.800000\n",
            " 6955/8000: episode: 247, duration: 0.757s, episode steps:  53, steps per second:  70, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 6.981492, mae: 15.545566, mean_q: 30.533425, mean_eps: 0.800000\n",
            " 7019/8000: episode: 248, duration: 0.904s, episode steps:  64, steps per second:  71, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 8.147654, mae: 15.747492, mean_q: 30.863294, mean_eps: 0.800000\n",
            " 7052/8000: episode: 249, duration: 0.509s, episode steps:  33, steps per second:  65, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 4.840598, mae: 15.538013, mean_q: 30.692492, mean_eps: 0.800000\n",
            " 7067/8000: episode: 250, duration: 0.255s, episode steps:  15, steps per second:  59, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 8.554000, mae: 15.831160, mean_q: 31.123306, mean_eps: 0.800000\n",
            " 7082/8000: episode: 251, duration: 0.255s, episode steps:  15, steps per second:  59, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 5.864606, mae: 15.886973, mean_q: 31.385844, mean_eps: 0.800000\n",
            " 7101/8000: episode: 252, duration: 0.377s, episode steps:  19, steps per second:  50, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 5.943447, mae: 15.665821, mean_q: 30.958710, mean_eps: 0.800000\n",
            " 7121/8000: episode: 253, duration: 0.517s, episode steps:  20, steps per second:  39, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 7.229428, mae: 15.717268, mean_q: 30.985886, mean_eps: 0.800000\n",
            " 7276/8000: episode: 254, duration: 2.950s, episode steps: 155, steps per second:  53, episode reward: 155.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 6.977846, mae: 16.044704, mean_q: 31.656605, mean_eps: 0.800000\n",
            " 7308/8000: episode: 255, duration: 0.554s, episode steps:  32, steps per second:  58, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 6.195669, mae: 16.229807, mean_q: 32.149325, mean_eps: 0.800000\n",
            " 7327/8000: episode: 256, duration: 0.394s, episode steps:  19, steps per second:  48, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 11.289082, mae: 16.400177, mean_q: 32.111504, mean_eps: 0.800000\n",
            " 7348/8000: episode: 257, duration: 0.428s, episode steps:  21, steps per second:  49, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 8.935666, mae: 16.315185, mean_q: 32.085874, mean_eps: 0.800000\n",
            " 7375/8000: episode: 258, duration: 0.559s, episode steps:  27, steps per second:  48, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.630 [0.000, 1.000],  loss: 13.829491, mae: 16.459414, mean_q: 32.071989, mean_eps: 0.800000\n",
            " 7405/8000: episode: 259, duration: 0.651s, episode steps:  30, steps per second:  46, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 8.030936, mae: 16.337768, mean_q: 32.121840, mean_eps: 0.800000\n",
            " 7425/8000: episode: 260, duration: 0.328s, episode steps:  20, steps per second:  61, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 8.125436, mae: 16.538669, mean_q: 32.612016, mean_eps: 0.800000\n",
            " 7491/8000: episode: 261, duration: 1.234s, episode steps:  66, steps per second:  53, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 7.357814, mae: 16.552521, mean_q: 32.666892, mean_eps: 0.800000\n",
            " 7505/8000: episode: 262, duration: 0.363s, episode steps:  14, steps per second:  39, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.162121, mae: 16.427737, mean_q: 32.534956, mean_eps: 0.800000\n",
            " 7531/8000: episode: 263, duration: 0.627s, episode steps:  26, steps per second:  41, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.577 [0.000, 1.000],  loss: 7.640666, mae: 16.704759, mean_q: 32.948952, mean_eps: 0.800000\n",
            " 7563/8000: episode: 264, duration: 0.821s, episode steps:  32, steps per second:  39, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 7.360357, mae: 16.691335, mean_q: 32.944942, mean_eps: 0.800000\n",
            " 7578/8000: episode: 265, duration: 0.388s, episode steps:  15, steps per second:  39, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 11.089035, mae: 16.731460, mean_q: 32.697058, mean_eps: 0.800000\n",
            " 7600/8000: episode: 266, duration: 0.496s, episode steps:  22, steps per second:  44, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.656954, mae: 16.769191, mean_q: 32.918489, mean_eps: 0.800000\n",
            " 7633/8000: episode: 267, duration: 0.881s, episode steps:  33, steps per second:  37, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 6.513874, mae: 16.727011, mean_q: 33.070302, mean_eps: 0.800000\n",
            " 7658/8000: episode: 268, duration: 0.591s, episode steps:  25, steps per second:  42, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 7.514330, mae: 17.026097, mean_q: 33.555165, mean_eps: 0.800000\n",
            " 7744/8000: episode: 269, duration: 1.872s, episode steps:  86, steps per second:  46, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 10.022751, mae: 17.004317, mean_q: 33.330266, mean_eps: 0.800000\n",
            " 7777/8000: episode: 270, duration: 0.709s, episode steps:  33, steps per second:  47, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 7.467073, mae: 16.933186, mean_q: 33.509416, mean_eps: 0.800000\n",
            " 7800/8000: episode: 271, duration: 0.530s, episode steps:  23, steps per second:  43, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 5.501773, mae: 17.104468, mean_q: 33.881994, mean_eps: 0.800000\n",
            " 7873/8000: episode: 272, duration: 1.862s, episode steps:  73, steps per second:  39, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 6.556987, mae: 17.160402, mean_q: 34.008183, mean_eps: 0.800000\n",
            " 7892/8000: episode: 273, duration: 0.462s, episode steps:  19, steps per second:  41, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 8.780866, mae: 17.365465, mean_q: 34.181686, mean_eps: 0.800000\n",
            " 7971/8000: episode: 274, duration: 1.517s, episode steps:  79, steps per second:  52, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 8.751159, mae: 17.389017, mean_q: 34.321108, mean_eps: 0.800000\n",
            "done, took 158.742 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 105.000, steps: 105\n",
            "Episode 4: reward: 192.000, steps: 192\n",
            "Episode 5: reward: 184.000, steps: 184\n",
            "Episode 6: reward: 84.000, steps: 84\n",
            "Episode 7: reward: 145.000, steps: 145\n",
            "Episode 8: reward: 104.000, steps: 104\n",
            "Episode 9: reward: 200.000, steps: 200\n",
            "Episode 10: reward: 134.000, steps: 134\n",
            "Episode 11: reward: 174.000, steps: 174\n",
            "Episode 12: reward: 102.000, steps: 102\n",
            "Episode 13: reward: 88.000, steps: 88\n",
            "Episode 14: reward: 92.000, steps: 92\n",
            "Episode 15: reward: 133.000, steps: 133\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 200.000, steps: 200\n",
            "Episode 18: reward: 200.000, steps: 200\n",
            "Episode 19: reward: 110.000, steps: 110\n",
            "Episode 20: reward: 200.000, steps: 200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb170a02a90>"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    }
  ]
}