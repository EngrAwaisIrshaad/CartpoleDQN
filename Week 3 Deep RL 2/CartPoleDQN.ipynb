{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPoleDQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Megacity1/CartpoleDQN/blob/main/Week%203%20Deep%20RL%202/CartPoleDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Deep Q Network (DQN) for CartPole Using Boltzmann Q Policy\n",
        "This exercise implements a DQN for CartPole using a Boltzmann Q policy for selecting the actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGsC7cJ5jNcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda73cbf-3020-40f4-e45e-7b87053448f1"
      },
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.24.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (13.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.44.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.10.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMIHLgQ3Z-lF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5a8608-456f-4127-ce8a-f551e5f4e4f3"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0AMLzq08ap0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b380e1-6c3d-4bce-8233-a8ac01deaf26"
      },
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "!pip install Adam\n",
        "#from keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Adam in /usr/local/lib/python3.7/dist-packages (0.0.0.dev0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll6bNdUm54WS"
      },
      "source": [
        "Implementation of DQN for CartPole, applying policy BoltzmannQPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCrPKNy40PC"
      },
      "source": [
        "##Implement DQN with BoltzmannGumbelQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efM9jkXr5A3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "112fe5be-22a4-4fd8-bd93-ddca1bd8d4f0"
      },
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import BoltzmannGumbelQPolicy\n",
        "from rl.policy import LinearAnnealedPolicy\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n",
        "# setup experience replay buffer\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
        "                               attr='eps',            \n",
        "                               value_max=10.,\n",
        "                               value_min=.10, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=20)\n",
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "# add extra layers here\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=20,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy) \n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=2000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_39\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_37 (Flatten)        (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_75 (Dense)            (None, 16)                80        \n",
            "                                                                 \n",
            " dense_76 (Dense)            (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 2000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   48/2000: episode: 1, duration: 8.949s, episode steps:  48, steps per second:   5, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.466658, mae: 0.546587, mean_q: 0.173075, mean_eps: 0.100000\n",
            "   95/2000: episode: 2, duration: 0.628s, episode steps:  47, steps per second:  75, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 0.334656, mae: 0.542567, mean_q: 0.376742, mean_eps: 0.100000\n",
            "  127/2000: episode: 3, duration: 0.443s, episode steps:  32, steps per second:  72, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.244245, mae: 0.584729, mean_q: 0.640156, mean_eps: 0.100000\n",
            "  145/2000: episode: 4, duration: 0.257s, episode steps:  18, steps per second:  70, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.183401, mae: 0.638729, mean_q: 0.878160, mean_eps: 0.100000\n",
            "  156/2000: episode: 5, duration: 0.144s, episode steps:  11, steps per second:  76, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.154622, mae: 0.696294, mean_q: 1.065467, mean_eps: 0.100000\n",
            "  168/2000: episode: 6, duration: 0.167s, episode steps:  12, steps per second:  72, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.128017, mae: 0.752916, mean_q: 1.257813, mean_eps: 0.100000\n",
            "  178/2000: episode: 7, duration: 0.146s, episode steps:  10, steps per second:  68, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.128022, mae: 0.820937, mean_q: 1.437806, mean_eps: 0.100000\n",
            "  186/2000: episode: 8, duration: 0.121s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.133223, mae: 0.893650, mean_q: 1.587230, mean_eps: 0.100000\n",
            "  198/2000: episode: 9, duration: 0.155s, episode steps:  12, steps per second:  77, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.083 [0.000, 1.000],  loss: 0.181810, mae: 0.932282, mean_q: 1.710349, mean_eps: 0.100000\n",
            "  209/2000: episode: 10, duration: 0.151s, episode steps:  11, steps per second:  73, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.214769, mae: 1.015315, mean_q: 1.872718, mean_eps: 0.100000\n",
            "  221/2000: episode: 11, duration: 0.156s, episode steps:  12, steps per second:  77, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.299520, mae: 1.077391, mean_q: 1.895495, mean_eps: 0.100000\n",
            "  231/2000: episode: 12, duration: 0.153s, episode steps:  10, steps per second:  66, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.348337, mae: 1.152155, mean_q: 1.970842, mean_eps: 0.100000\n",
            "  240/2000: episode: 13, duration: 0.144s, episode steps:   9, steps per second:  63, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.378871, mae: 1.250677, mean_q: 2.138860, mean_eps: 0.100000\n",
            "  250/2000: episode: 14, duration: 0.157s, episode steps:  10, steps per second:  64, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.349067, mae: 1.250965, mean_q: 2.232076, mean_eps: 0.100000\n",
            "  260/2000: episode: 15, duration: 0.140s, episode steps:  10, steps per second:  72, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.338478, mae: 1.240972, mean_q: 2.291043, mean_eps: 0.100000\n",
            "  269/2000: episode: 16, duration: 0.137s, episode steps:   9, steps per second:  66, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.413760, mae: 1.330037, mean_q: 2.477585, mean_eps: 0.100000\n",
            "  278/2000: episode: 17, duration: 0.133s, episode steps:   9, steps per second:  68, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.500713, mae: 1.411227, mean_q: 2.494412, mean_eps: 0.100000\n",
            "  288/2000: episode: 18, duration: 0.151s, episode steps:  10, steps per second:  66, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.459306, mae: 1.422350, mean_q: 2.553374, mean_eps: 0.100000\n",
            "  296/2000: episode: 19, duration: 0.119s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.560345, mae: 1.478118, mean_q: 2.605839, mean_eps: 0.100000\n",
            "  305/2000: episode: 20, duration: 0.135s, episode steps:   9, steps per second:  67, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.545587, mae: 1.533717, mean_q: 2.765992, mean_eps: 0.100000\n",
            "  316/2000: episode: 21, duration: 0.170s, episode steps:  11, steps per second:  65, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.493048, mae: 1.500229, mean_q: 2.763429, mean_eps: 0.100000\n",
            "  325/2000: episode: 22, duration: 0.167s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.464713, mae: 1.522877, mean_q: 2.900375, mean_eps: 0.100000\n",
            "  337/2000: episode: 23, duration: 0.174s, episode steps:  12, steps per second:  69, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.083 [0.000, 1.000],  loss: 0.496984, mae: 1.618175, mean_q: 3.032967, mean_eps: 0.100000\n",
            "  346/2000: episode: 24, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.440283, mae: 1.579808, mean_q: 3.073056, mean_eps: 0.100000\n",
            "  355/2000: episode: 25, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.696504, mae: 1.724377, mean_q: 3.186970, mean_eps: 0.100000\n",
            "  364/2000: episode: 26, duration: 0.138s, episode steps:   9, steps per second:  65, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.558760, mae: 1.721136, mean_q: 3.147230, mean_eps: 0.100000\n",
            "  374/2000: episode: 27, duration: 0.141s, episode steps:  10, steps per second:  71, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.641639, mae: 1.804300, mean_q: 3.207523, mean_eps: 0.100000\n",
            "  385/2000: episode: 28, duration: 0.174s, episode steps:  11, steps per second:  63, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.498613, mae: 1.768157, mean_q: 3.274879, mean_eps: 0.100000\n",
            "  396/2000: episode: 29, duration: 0.163s, episode steps:  11, steps per second:  68, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.478954, mae: 1.806012, mean_q: 3.443415, mean_eps: 0.100000\n",
            "  406/2000: episode: 30, duration: 0.152s, episode steps:  10, steps per second:  66, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.524311, mae: 1.869055, mean_q: 3.571516, mean_eps: 0.100000\n",
            "  416/2000: episode: 31, duration: 0.144s, episode steps:  10, steps per second:  69, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.460785, mae: 1.877466, mean_q: 3.596791, mean_eps: 0.100000\n",
            "  426/2000: episode: 32, duration: 0.145s, episode steps:  10, steps per second:  69, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.683933, mae: 1.972103, mean_q: 3.670512, mean_eps: 0.100000\n",
            "  434/2000: episode: 33, duration: 0.119s, episode steps:   8, steps per second:  67, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.483453, mae: 1.992661, mean_q: 3.657376, mean_eps: 0.100000\n",
            "  444/2000: episode: 34, duration: 0.150s, episode steps:  10, steps per second:  67, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.472271, mae: 1.970759, mean_q: 3.768956, mean_eps: 0.100000\n",
            "  455/2000: episode: 35, duration: 0.170s, episode steps:  11, steps per second:  65, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.562312, mae: 2.081610, mean_q: 3.963849, mean_eps: 0.100000\n",
            "  468/2000: episode: 36, duration: 0.178s, episode steps:  13, steps per second:  73, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.569356, mae: 2.109635, mean_q: 4.016166, mean_eps: 0.100000\n",
            "  478/2000: episode: 37, duration: 0.136s, episode steps:  10, steps per second:  73, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.687348, mae: 2.163076, mean_q: 4.030389, mean_eps: 0.100000\n",
            "  486/2000: episode: 38, duration: 0.130s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.699175, mae: 2.192857, mean_q: 4.042157, mean_eps: 0.100000\n",
            "  495/2000: episode: 39, duration: 0.121s, episode steps:   9, steps per second:  75, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.640987, mae: 2.174987, mean_q: 4.072565, mean_eps: 0.100000\n",
            "  504/2000: episode: 40, duration: 0.136s, episode steps:   9, steps per second:  66, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.727923, mae: 2.223903, mean_q: 4.179111, mean_eps: 0.100000\n",
            "  513/2000: episode: 41, duration: 0.136s, episode steps:   9, steps per second:  66, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.570673, mae: 2.141059, mean_q: 4.213298, mean_eps: 0.100000\n",
            "  524/2000: episode: 42, duration: 0.187s, episode steps:  11, steps per second:  59, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.598861, mae: 2.180894, mean_q: 4.278303, mean_eps: 0.100000\n",
            "  536/2000: episode: 43, duration: 0.184s, episode steps:  12, steps per second:  65, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.593190, mae: 2.217656, mean_q: 4.391745, mean_eps: 0.100000\n",
            "  545/2000: episode: 44, duration: 0.149s, episode steps:   9, steps per second:  60, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.536129, mae: 2.244547, mean_q: 4.425789, mean_eps: 0.100000\n",
            "  556/2000: episode: 45, duration: 0.150s, episode steps:  11, steps per second:  73, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.566220, mae: 2.315117, mean_q: 4.496886, mean_eps: 0.100000\n",
            "  567/2000: episode: 46, duration: 0.173s, episode steps:  11, steps per second:  63, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.721569, mae: 2.382383, mean_q: 4.554364, mean_eps: 0.100000\n",
            "  576/2000: episode: 47, duration: 0.131s, episode steps:   9, steps per second:  69, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.753694, mae: 2.421718, mean_q: 4.524067, mean_eps: 0.100000\n",
            "  585/2000: episode: 48, duration: 0.154s, episode steps:   9, steps per second:  58, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.571029, mae: 2.405567, mean_q: 4.534546, mean_eps: 0.100000\n",
            "  595/2000: episode: 49, duration: 0.143s, episode steps:  10, steps per second:  70, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.561792, mae: 2.443552, mean_q: 4.719811, mean_eps: 0.100000\n",
            "  604/2000: episode: 50, duration: 0.131s, episode steps:   9, steps per second:  69, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.441861, mae: 2.443787, mean_q: 4.819913, mean_eps: 0.100000\n",
            "  612/2000: episode: 51, duration: 0.121s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.700816, mae: 2.557928, mean_q: 4.836290, mean_eps: 0.100000\n",
            "  624/2000: episode: 52, duration: 0.197s, episode steps:  12, steps per second:  61, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.600348, mae: 2.525372, mean_q: 4.829978, mean_eps: 0.100000\n",
            "  633/2000: episode: 53, duration: 0.136s, episode steps:   9, steps per second:  66, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.564956, mae: 2.513592, mean_q: 4.897328, mean_eps: 0.100000\n",
            "  644/2000: episode: 54, duration: 0.179s, episode steps:  11, steps per second:  62, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.545041, mae: 2.546489, mean_q: 4.981211, mean_eps: 0.100000\n",
            "  654/2000: episode: 55, duration: 0.169s, episode steps:  10, steps per second:  59, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.722938, mae: 2.638156, mean_q: 5.107791, mean_eps: 0.100000\n",
            "  664/2000: episode: 56, duration: 0.161s, episode steps:  10, steps per second:  62, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.466972, mae: 2.577940, mean_q: 5.063887, mean_eps: 0.100000\n",
            "  675/2000: episode: 57, duration: 0.165s, episode steps:  11, steps per second:  67, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.480170, mae: 2.580442, mean_q: 5.179841, mean_eps: 0.100000\n",
            "  685/2000: episode: 58, duration: 0.139s, episode steps:  10, steps per second:  72, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.621300, mae: 2.684011, mean_q: 5.221117, mean_eps: 0.100000\n",
            "  695/2000: episode: 59, duration: 0.146s, episode steps:  10, steps per second:  69, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.582847, mae: 2.625404, mean_q: 5.145550, mean_eps: 0.100000\n",
            "  705/2000: episode: 60, duration: 0.150s, episode steps:  10, steps per second:  67, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.581964, mae: 2.662431, mean_q: 5.286119, mean_eps: 0.100000\n",
            "  717/2000: episode: 61, duration: 0.183s, episode steps:  12, steps per second:  66, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.083 [0.000, 1.000],  loss: 0.570461, mae: 2.684602, mean_q: 5.275499, mean_eps: 0.100000\n",
            "  726/2000: episode: 62, duration: 0.137s, episode steps:   9, steps per second:  66, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.589037, mae: 2.744427, mean_q: 5.444663, mean_eps: 0.100000\n",
            "  735/2000: episode: 63, duration: 0.135s, episode steps:   9, steps per second:  66, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.636206, mae: 2.731796, mean_q: 5.240244, mean_eps: 0.100000\n",
            "  745/2000: episode: 64, duration: 0.193s, episode steps:  10, steps per second:  52, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.619856, mae: 2.781063, mean_q: 5.329707, mean_eps: 0.100000\n",
            "  753/2000: episode: 65, duration: 0.154s, episode steps:   8, steps per second:  52, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.548901, mae: 2.813873, mean_q: 5.381912, mean_eps: 0.100000\n",
            "  763/2000: episode: 66, duration: 0.150s, episode steps:  10, steps per second:  67, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.519832, mae: 2.811290, mean_q: 5.448890, mean_eps: 0.100000\n",
            "  774/2000: episode: 67, duration: 0.172s, episode steps:  11, steps per second:  64, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.588818, mae: 2.828817, mean_q: 5.425726, mean_eps: 0.100000\n",
            "  783/2000: episode: 68, duration: 0.163s, episode steps:   9, steps per second:  55, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.369073, mae: 2.893008, mean_q: 5.687828, mean_eps: 0.100000\n",
            "  792/2000: episode: 69, duration: 0.145s, episode steps:   9, steps per second:  62, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.526229, mae: 2.897762, mean_q: 5.614366, mean_eps: 0.100000\n",
            "  803/2000: episode: 70, duration: 0.166s, episode steps:  11, steps per second:  66, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.457574, mae: 2.897415, mean_q: 5.530174, mean_eps: 0.100000\n",
            "  813/2000: episode: 71, duration: 0.146s, episode steps:  10, steps per second:  68, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.573800, mae: 2.940767, mean_q: 5.643946, mean_eps: 0.100000\n",
            "  822/2000: episode: 72, duration: 0.132s, episode steps:   9, steps per second:  68, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.303246, mae: 2.943090, mean_q: 5.792552, mean_eps: 0.100000\n",
            "  831/2000: episode: 73, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.382371, mae: 2.959638, mean_q: 5.735793, mean_eps: 0.100000\n",
            "  841/2000: episode: 74, duration: 0.147s, episode steps:  10, steps per second:  68, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.399061, mae: 2.969631, mean_q: 5.635936, mean_eps: 0.100000\n",
            "  852/2000: episode: 75, duration: 0.191s, episode steps:  11, steps per second:  58, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.455788, mae: 3.008034, mean_q: 5.669196, mean_eps: 0.100000\n",
            "  861/2000: episode: 76, duration: 0.139s, episode steps:   9, steps per second:  65, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.520580, mae: 3.068360, mean_q: 5.809720, mean_eps: 0.100000\n",
            "  869/2000: episode: 77, duration: 0.123s, episode steps:   8, steps per second:  65, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.555230, mae: 3.101094, mean_q: 5.761939, mean_eps: 0.100000\n",
            "  879/2000: episode: 78, duration: 0.157s, episode steps:  10, steps per second:  64, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.457245, mae: 3.067708, mean_q: 5.746173, mean_eps: 0.100000\n",
            "  889/2000: episode: 79, duration: 0.153s, episode steps:  10, steps per second:  65, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.400273, mae: 3.116837, mean_q: 5.807257, mean_eps: 0.100000\n",
            "  908/2000: episode: 80, duration: 0.284s, episode steps:  19, steps per second:  67, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.365545, mae: 3.163425, mean_q: 5.940548, mean_eps: 0.100000\n",
            "  919/2000: episode: 81, duration: 0.201s, episode steps:  11, steps per second:  55, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.277251, mae: 3.171878, mean_q: 5.984904, mean_eps: 0.100000\n",
            "  948/2000: episode: 82, duration: 0.406s, episode steps:  29, steps per second:  71, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 0.400871, mae: 3.242917, mean_q: 6.026046, mean_eps: 0.100000\n",
            "  984/2000: episode: 83, duration: 0.484s, episode steps:  36, steps per second:  74, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.291869, mae: 3.292764, mean_q: 6.192208, mean_eps: 0.100000\n",
            " 1095/2000: episode: 84, duration: 1.444s, episode steps: 111, steps per second:  77, episode reward: 111.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 0.359522, mae: 3.513270, mean_q: 6.633947, mean_eps: 0.100000\n",
            " 1118/2000: episode: 85, duration: 0.326s, episode steps:  23, steps per second:  71, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 0.519072, mae: 3.745672, mean_q: 7.080048, mean_eps: 0.100000\n",
            " 1137/2000: episode: 86, duration: 0.264s, episode steps:  19, steps per second:  72, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 0.348279, mae: 3.902394, mean_q: 7.512417, mean_eps: 0.100000\n",
            " 1147/2000: episode: 87, duration: 0.150s, episode steps:  10, steps per second:  67, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.661879, mae: 3.960896, mean_q: 7.554282, mean_eps: 0.100000\n",
            " 1167/2000: episode: 88, duration: 0.294s, episode steps:  20, steps per second:  68, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.650 [0.000, 1.000],  loss: 0.554359, mae: 3.949213, mean_q: 7.512174, mean_eps: 0.100000\n",
            " 1194/2000: episode: 89, duration: 0.416s, episode steps:  27, steps per second:  65, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 0.563745, mae: 4.084503, mean_q: 7.834625, mean_eps: 0.100000\n",
            " 1217/2000: episode: 90, duration: 0.388s, episode steps:  23, steps per second:  59, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.747614, mae: 4.199018, mean_q: 7.944950, mean_eps: 0.100000\n",
            " 1237/2000: episode: 91, duration: 0.274s, episode steps:  20, steps per second:  73, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 0.733071, mae: 4.094929, mean_q: 7.747654, mean_eps: 0.100000\n",
            " 1271/2000: episode: 92, duration: 0.461s, episode steps:  34, steps per second:  74, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.884104, mae: 4.369415, mean_q: 8.301295, mean_eps: 0.100000\n",
            " 1313/2000: episode: 93, duration: 0.556s, episode steps:  42, steps per second:  76, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.942593, mae: 4.468481, mean_q: 8.428184, mean_eps: 0.100000\n",
            " 1344/2000: episode: 94, duration: 0.685s, episode steps:  31, steps per second:  45, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 0.796313, mae: 4.565730, mean_q: 8.721881, mean_eps: 0.100000\n",
            " 1361/2000: episode: 95, duration: 0.291s, episode steps:  17, steps per second:  58, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.843574, mae: 4.702426, mean_q: 8.992022, mean_eps: 0.100000\n",
            " 1383/2000: episode: 96, duration: 0.410s, episode steps:  22, steps per second:  54, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.068870, mae: 4.813253, mean_q: 9.232490, mean_eps: 0.100000\n",
            " 1418/2000: episode: 97, duration: 0.682s, episode steps:  35, steps per second:  51, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 0.887775, mae: 4.870129, mean_q: 9.315424, mean_eps: 0.100000\n",
            " 1438/2000: episode: 98, duration: 0.281s, episode steps:  20, steps per second:  71, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 1.307562, mae: 5.129961, mean_q: 9.799140, mean_eps: 0.100000\n",
            " 1462/2000: episode: 99, duration: 0.335s, episode steps:  24, steps per second:  72, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.083456, mae: 5.098528, mean_q: 9.715966, mean_eps: 0.100000\n",
            " 1481/2000: episode: 100, duration: 0.247s, episode steps:  19, steps per second:  77, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.104536, mae: 5.185027, mean_q: 9.875766, mean_eps: 0.100000\n",
            " 1498/2000: episode: 101, duration: 0.223s, episode steps:  17, steps per second:  76, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1.397692, mae: 5.277435, mean_q: 10.071762, mean_eps: 0.100000\n",
            " 1542/2000: episode: 102, duration: 0.617s, episode steps:  44, steps per second:  71, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.120748, mae: 5.305248, mean_q: 10.102767, mean_eps: 0.100000\n",
            " 1575/2000: episode: 103, duration: 0.514s, episode steps:  33, steps per second:  64, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 1.660499, mae: 5.517441, mean_q: 10.423583, mean_eps: 0.100000\n",
            " 1597/2000: episode: 104, duration: 0.426s, episode steps:  22, steps per second:  52, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.265702, mae: 5.565431, mean_q: 10.531181, mean_eps: 0.100000\n",
            " 1622/2000: episode: 105, duration: 0.414s, episode steps:  25, steps per second:  60, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 1.327913, mae: 5.562599, mean_q: 10.584523, mean_eps: 0.100000\n",
            " 1637/2000: episode: 106, duration: 0.250s, episode steps:  15, steps per second:  60, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.942598, mae: 5.653044, mean_q: 10.673645, mean_eps: 0.100000\n",
            " 1648/2000: episode: 107, duration: 0.238s, episode steps:  11, steps per second:  46, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 1.713578, mae: 5.665353, mean_q: 10.660369, mean_eps: 0.100000\n",
            " 1660/2000: episode: 108, duration: 0.226s, episode steps:  12, steps per second:  53, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 2.123448, mae: 5.777443, mean_q: 10.814982, mean_eps: 0.100000\n",
            " 1674/2000: episode: 109, duration: 0.277s, episode steps:  14, steps per second:  51, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 1.630633, mae: 5.771771, mean_q: 10.903269, mean_eps: 0.100000\n",
            " 1709/2000: episode: 110, duration: 0.705s, episode steps:  35, steps per second:  50, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 1.900861, mae: 5.891729, mean_q: 11.121767, mean_eps: 0.100000\n",
            " 1774/2000: episode: 111, duration: 0.942s, episode steps:  65, steps per second:  69, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 1.741681, mae: 6.024680, mean_q: 11.419935, mean_eps: 0.100000\n",
            " 1802/2000: episode: 112, duration: 0.404s, episode steps:  28, steps per second:  69, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 1.824017, mae: 6.119829, mean_q: 11.594747, mean_eps: 0.100000\n",
            " 1818/2000: episode: 113, duration: 0.369s, episode steps:  16, steps per second:  43, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 1.961758, mae: 6.167982, mean_q: 11.679620, mean_eps: 0.100000\n",
            " 1849/2000: episode: 114, duration: 0.550s, episode steps:  31, steps per second:  56, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 2.467593, mae: 6.259003, mean_q: 11.709202, mean_eps: 0.100000\n",
            " 1875/2000: episode: 115, duration: 0.394s, episode steps:  26, steps per second:  66, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 2.627922, mae: 6.259855, mean_q: 11.614833, mean_eps: 0.100000\n",
            " 1896/2000: episode: 116, duration: 0.318s, episode steps:  21, steps per second:  66, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.663207, mae: 6.288255, mean_q: 11.899210, mean_eps: 0.100000\n",
            " 1946/2000: episode: 117, duration: 0.699s, episode steps:  50, steps per second:  71, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.632553, mae: 6.427944, mean_q: 12.301655, mean_eps: 0.100000\n",
            " 1985/2000: episode: 118, duration: 0.515s, episode steps:  39, steps per second:  76, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 1.978310, mae: 6.525517, mean_q: 12.411912, mean_eps: 0.100000\n",
            "done, took 38.858 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 21.000, steps: 21\n",
            "Episode 2: reward: 37.000, steps: 37\n",
            "Episode 3: reward: 114.000, steps: 114\n",
            "Episode 4: reward: 78.000, steps: 78\n",
            "Episode 5: reward: 21.000, steps: 21\n",
            "Episode 6: reward: 22.000, steps: 22\n",
            "Episode 7: reward: 36.000, steps: 36\n",
            "Episode 8: reward: 80.000, steps: 80\n",
            "Episode 9: reward: 53.000, steps: 53\n",
            "Episode 10: reward: 26.000, steps: 26\n",
            "Episode 11: reward: 45.000, steps: 45\n",
            "Episode 12: reward: 27.000, steps: 27\n",
            "Episode 13: reward: 26.000, steps: 26\n",
            "Episode 14: reward: 60.000, steps: 60\n",
            "Episode 15: reward: 25.000, steps: 25\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 35.000, steps: 35\n",
            "Episode 18: reward: 54.000, steps: 54\n",
            "Episode 19: reward: 25.000, steps: 25\n",
            "Episode 20: reward: 36.000, steps: 36\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb171aac990>"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    }
  ]
}